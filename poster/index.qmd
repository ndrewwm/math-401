---
title: Vector-valued Gaussian Processes
knitr:
  opts_chunk:
    echo: false
    warning: false
    message: false
format:
  poster-typst:
    size: "36x24"
    poster-authors: "Andrew Moore, Grady Wright"
    departments: "Department of Mathematics"
    institution-logo: "./images/boisestate-laligned-logo.png"
    footer-text: "Spring 2025, Senior Showcase"
    footer-url: "https://ndrewwm.github.io/math-401"
    footer-emails: "andrewmoore1@u.boisestate.edu"
    footer-color: "0033A0" #"D64309"
    keywords: ["Gaussian Processes", "Statistics", "Velocity Fields"]
---

# Overview of Gaussian Processes

Gaussian processes are generalizations of the multivariate Gaussian distribution. Rather than characterizing a probability distributions of vectors, Gaussian processes can be used to describe a probability distribution over families of functions.

## Multivariate Gaussian (Multivariate Normal) Distribution

```{=typst}

The multivariate normal distribution is used to model _random vectors_ (vectors whose elements are jointly distributed random variables). This distribution is parameterized by a _mean vector_ $bold(mu)$ and _covariance matrix_ $bold(Sigma)$. Suppose $bold(x) in RR^N$ is drawn from a multivariate Gaussian distribution. Then, we can write the following:

$ bold(x) in RR^N &tilde.op cal(N)_N (bold(mu), bold(Sigma)) \ 
  bold(mu) in RR^N &= (mu_1, mu_2, ..., mu_N)^top = (EE(x_1), EE(x_2), ..., EE(x_N))^top \ 
  bold(Sigma) in RR^(N times N) &= EE((bold(x) - mu)(bold(x) - mu)^top) = [op("cov")(x_i, x_j)]_(i j)^N \
  x_i &tilde.op cal(N)(mu_i, bold(Sigma)_(i i)) $
```

## Gaussian Processes (GPs)

```{=typst}

Formally, a _Gaussian process_ (GP) is an uncountably infinite collection of random variables, with any finite sample from the process sharing a joint multivariate Gaussian distribution. GPs are fully specified by a _mean function_ $m$ and _covariance (kernel) function_ $k$. For notational convenience, it's often assumed that the mean function is $bold(0)$, but this is not required. The kernel function must produce a positive semi-definite matrix when evaluated on a set of input points (or vectors).
```

::: {.block fill="luma(230)" inset="8pt" radius="4pt"}

```{=typst}
*Positive semi-definite matrix.*

Let $M in RR^(N times N)$ be a symmetric matrix. We say that $M$ is positive semi-definite if, for all vectors $bold(x) in RR^N$ not equal to $bold(0)$, the following holds: $bold(x)^top M bold(x) >= 0.$
```

:::


```{=typst}
We will focus primarily on the _squared exponential kernel_ $k: RR^p times RR^p -> RR$, defined as:

$ k(x, x') = alpha^2 op("exp")(-1/(2rho^2) ||x - x'||^2), $

where $|| dot ||$ is the Euclidean norm. The squared exponential kernel has two _hyperparameters_, $alpha$ and $rho$, which control the variance scale and length scale of functions drawn from the GP. Typically, $alpha$ is set to 1, and $rho$ is specified based on domain knowledge or is estimated from observed data.
```

 This choice of kernel function reflects the assumption that the covariance between two points (or vectors) $x$ and $x'$ decays exponentially based on the distance between them.

::: {.block fill="luma(230)" inset="8pt" radius="4pt"}

```{=typst}
*Euclidean (L2) Norm.*

Let $bold(x) in RR^N$. The Euclidean Norm of $bold(x)$, written $||bold(x)||$, is defined as
$ ||bold(x)|| = sqrt(sum_(i = 1)^N x_i) = sqrt(x_1 + x_2 + dots.h.c + x_N). $
```

:::

# Gaussian Process Regression -- Univariate $\mathbf{y}$

```{=typst}

In practice, Gaussian Processes are often brought to bear on _regression problems_, in which an analyst has collected a dataset $S = (bold(x), bold(y)) = { (x_i, y_i) : x_i in RR^p, y_i in RR^d, i in 1, 2, ..., N }$ with the goal of learning the relationship $f$ between $bold(x)$ and $bold(y)$:

$ bold(y) &= f(bold(x)) " or " \
  bold(y) &= f(bold(x)) + epsilon " (with additive noise)". $

Once $f$ has been estimated, it can then be used to predict the values of future or test points. Gaussian Process Regression can be considered a Bayesian method for learning $f$. As an overview, we'll consider an example using simulated data where $x_i, y_i in RR$. \ \
```

```{=typst}
Let $f(x) = sin(2x) + sin(4x)$ be an unknown function that an analyst is attempting to model using sampled data. Let $N = 20$ and $M = 400 - N$. We wish to use our sample $S$ to predict the values $bold(y)_* = f(bold(x)_*)$ for test data $bold(x)_* in RR^M$.
```

```{r}
#| fig.height: 4.5
#| fig.width: 10

library(tidyverse)
library(MASS, include.only = "mvrnorm")
set.seed(123)
theme_set(theme_minimal(base_size = 18) + theme(panel.grid.minor = element_blank()))

f <- function(x) sin(2*x) + sin(4*x)
k <- function(x_i, x_j, alpha, rho) alpha^2 * exp(-(x_i - x_j)^2 / (2 * rho^2))

k_xX <- function(x, X, alpha = 1, rho = 0.4) {
  N <- NROW(x)
  M <- NROW(X)
  K <- matrix(0, N, M)
  for (n in 1:N) {
    for (m in 1:M) {
      K[n, m] <- k(x[n], X[m], alpha, rho)
    }
  }
  return(K)
}

N <- 400
n <- 20
x <- seq(-5, 5, length.out = N)
f_x <- f(x)

D <- tibble(x, y = f_x)
S <- slice_sample(D, n = n)
Z <- anti_join(D, S, by = "x")

K_xx <- k_xX(x, x)

leg <- c("Training Set" = "black", "Target" = "orange", "Prior" = "grey", "Posterior Mean" = "blue")
p0 <- ggplot() +
  geom_line(data = D, aes(x, y, color = "Target")) +
  geom_point(data = S, aes(x, y, color = "Training Set")) +
  scale_color_manual(name = "", values = leg) +
  labs(y = "f(x)") +
  theme(
    legend.position = c(0.13, 0.9)
  )

# p0
```

```{=typst}
We can draw samples from the prior distribution:

$ \ f " " &~ " " cal(N)_M (bold(0), k(bold(x)_*, bold(x)_*)) \
 k(bold(x)_*, bold(x)_*) in RR^(M times M) &= [k(bold(x)_(*i), bold(x)_(*j))]_(i,j)^M = mat( 
  k(x_(*1), x_(*1)), k(x_(*1), x_(*2)), ..., k(x_(*1), x_(*M));
  k(x_(*2), x_(*1)), k(x_(*2), x_(*2)), ..., k(x_(*2), x_(*M));
  dots.v, dots.v, dots.down, dots.v;
  k(x_(*M), x_(*1)), k(x_(*M), x_(*2)), ..., k(x_(*M), x_(*M));
). $
```

Draws from the prior distribution (shown in grey) don't necessarily agree with our data points. They represent our state of knowledge before observing the training data. In this instance, our kernel function ensures that draws are smooth curves-- matching what we might believe is a property of the target function.

```{r}
#| fig.height: 4.5
#| fig.width: 10

prior <- mvrnorm(10, rep_len(0, nrow(K_xx)), K_xx) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x)

p0 + geom_line(data = prior, aes(x, y = value, group = name, color = "Prior"), alpha = 0.7)
```

Our prior model for $f$ and our observed data $S$ can be combined to form a _posterior_ distribution. This distribution has an analytical form:

```{=typst}
$ bold(y)_* | bold(x), bold(y), bold(x)_* " " &~ " " cal(N)_M (hat(mu), hat(Sigma)) \
hat(mu) in RR^M &= k(bold(x)_*, bold(x)) (k(bold(x), bold(x)))^(-1) bold(y) \
hat(Sigma) in RR^(M times M) &= k(bold(x)_*, bold(x)_*) - k(bold(x)_*, bold(x))(k(bold(x), bold(x)))^(-1)k(bold(x)_*, bold(x))^top \
k(bold(x), bold(x)) in RR^(N times N) &= [k(x_i, x_j)]_(i,j)^N \
k(bold(x)_*, bold(x)) in RR^(M times N) &= [k(x_(*i), x_j)]_(i,j)^(M,N). $
```

```{r}
#| fig.height: 4.5
#| fig.width: 10

eps <- 0.009
K_XX <- k_xX(S$x, S$x)
K_xX <- k_xX(D$x, S$x)

# Adding a small amount of noise to the diagonal for numerical stability
mu <- K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% S$y
sigma <- K_xx - K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% t(K_xX)

post <- mvrnorm(100, mu, sigma) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x) |>
  group_by(x) |>
  summarise(y = mean(value), s = sd(value))

p0 +
  geom_ribbon(data = post, aes(x, ymin = y - 2*s, ymax = y + 2*s), alpha = 0.2) +
  geom_line(data = post, aes(x, y, color = "Posterior Mean")) +
  geom_point(data = S, aes(x, y, color = "Training Set"))
```

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut.

# Multioutput GPR -- Vector-valued $\mathbf{y}$

1. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.
1. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.
1. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut.

| Lorem ipsum dolor sit. | Lorem ipsum. | Lorem ipsum. |
|------------------------|--------------|--------------|
| Lorem ipsum dolor.     | Lorem ipsum. | $\alpha$     |
| Lorem ipsum.           | Lorem ipsum. | $\beta$      |
| Lorem.                 | Lorem.       | $\gamma$     |
| Lorem ipsum dolor.     | Lorem.       | $\theta$     |

: Lorem ipsum dolor sit amet {#tbl-1}

Lorem @tbl-1 ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut postea variari voluptas distinguique possit, augeri amplificarique non possit. At etiam Athenis, ut e patre audiebam facete et urbane Stoicos irridente, statua est in quo a nobis philosophia defensa et.

## Lorem ipsum dolor sit amet

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut postea variari voluptas distinguique possit, augeri amplificarique non possit. At etiam Athenis, ut e patre.

![Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.](./images/Standard_lettering.png){width="100%"}

---
title: Vector-valued Gaussian Processes
execute: 
  freeze: true
knitr:
  opts_chunk:
    echo: false
    warning: false
    message: false
format:
  poster-typst:
    size: "48x36"
    poster-authors: "Andrew Moore, Grady Wright"
    departments: "Department of Mathematics"
    institution-logo: "./images/boisestate-laligned-logo.png"
    footer-text: "Spring 2025, Senior Showcase"
    footer-url: "https://ndrewwm.github.io/math-401"
    footer-emails: "andrewmoore1@u.boisestate.edu"
    footer-color: "0033A0" #"D64309"
    keywords: ["Gaussian Processes", "Statistics", "Velocity Fields"]
---

# Overview of Gaussian Processes

Gaussian processes generalize the multivariate Gaussian distribution and can be used to describe a probability distribution over families of functions.

## Multivariate Gaussian Distribution

```{=typst}
The multivariate Gaussian distribution is used to model _random vectors_ (vectors of jointly distributed random variables).

$ bold(x) in RR^N &tilde.op cal(N)_N (bold(mu), bold(Sigma)) \ 
  bold(mu) in RR^N &= (mu_1, mu_2, ..., mu_N)^top = (EE(x_1), EE(x_2), ..., EE(x_N))^top \ 
  bold(Sigma) in RR^(N times N) &= EE((bold(x) - mu)(bold(x) - mu)^top) = [op("cov")(x_i, x_j)]_(i,j = 1)^N \
  x_i &tilde.op cal(N)(mu_i, bold(Sigma)_(i i)) $
```

## Gaussian Processes (GPs)

```{=typst}
- _Gaussian process_ (GP): an uncountably infinite collection of random variables; any finite sample is a draw from a MV Gaussian distribution.
- GPs are fully specified by a _mean function_ $m$ and _covariance (kernel) function_ $k$.
- The kernel function must produce a positive semi-definite matrix when evaluated on a set of input points (or vectors).
```


```{=typst}
We focus on the _squared exponential kernel_ $k: RR^p times RR^p -> RR$, defined as:

$ k(x, x') = alpha^2 op("exp")(-1/(2rho^2) ||x - x'||^2). $

- $||dot||$ is the Euclidean Norm: $||bold(x)|| = sqrt(x_1 + x_2 + dots.h.c + x_N)$
- $alpha$ and $rho$ are _hyperparameters_ (chosen, or estimated from data)
```

# Gaussian Process Regression -- Univariate $\mathbf{y}$

```{=typst}
Let $S = (bold(x), bold(y)) = { (x_i, y_i) : x_i in RR^p, y_i in RR^d, i in 1, 2, ..., N }$ be a researcher's dataset, and let $N = 20$ and $M = 400 - N$. We wish to use $S$ to find an unknown function $f$ that satisfies $bold(y) = f(bold(x))$, possibly subject to additive noise $epsilon$.
```

```{r}
#| fig.height: 7
#| fig.width: 15

ORANGE <- "#D64309"
BLUE <- "#0033A0"

library(tidyverse)
library(MASS, include.only = "mvrnorm")
set.seed(123)
theme_set(theme_minimal(base_size = 32) + theme(panel.grid.minor = element_blank()))

f <- function(x) sin(2*x) + sin(4*x)
k <- function(x_i, x_j, alpha, rho) alpha^2 * exp(-(x_i - x_j)^2 / (2 * rho^2))

k_xX <- function(x, X, alpha = 1, rho = 0.4) {
  N <- NROW(x)
  M <- NROW(X)
  K <- matrix(0, N, M)
  for (n in 1:N) {
    for (m in 1:M) {
      K[n, m] <- k(x[n], X[m], alpha, rho)
    }
  }
  return(K)
}

N <- 400
n <- 20
x <- seq(-5, 5, length.out = N)
f_x <- f(x)

D <- tibble(x, y = f_x)
S <- slice_sample(D, n = n)
Z <- anti_join(D, S, by = "x")

K_xx <- k_xX(x, x)

leg <- c("Training Set" = "black", "Target" = ORANGE, "Draws from the Prior" = "grey", "Posterior Mean" = BLUE)
p0 <- ggplot() +
  geom_line(data = D, aes(x, y, color = "Target")) +
  geom_point(data = S, aes(x, y, color = "Training Set")) +
  scale_color_manual(name = "", values = leg) +
  labs(y = "f(x)") +
  theme(
    legend.position = c(0.13, 0.9)
  )

# p0
```

```{=typst}
We can draw samples from the prior distribution:

$
f " " &~ " " cal("GP")(bold(0), k) \
bold(y)_* " " &~ " " cal(N)_M (bold(0), k(bold(x)_*, bold(x)_*)) \
k(bold(x)_*, bold(x)_*) in RR^(M times M) &= [k(bold(x)_(*i), bold(x)_(*j))]_(i,j = 1)^M. $


- Draws from the prior distribution (shown in grey) don't necessarily agree with the data points.
- Kernel choice determines properties of $f$ (e.g., smoothness)
```

```{r}
#| fig.height: 7
#| fig.width: 15

prior <- mvrnorm(10, rep_len(0, nrow(K_xx)), K_xx) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x)

p0 + geom_line(data = prior, aes(x, y = value, group = name, color = "Draws from the Prior"), alpha = 0.7)
```

Our prior model for $f$ and the observed data $S$ can be combined to form a _posterior_ distribution:

```{=typst}
$ bold(y)_* | bold(x), bold(y), bold(x)_* " " &~ " " cal(N)_M (hat(mu), hat(Sigma)) \
hat(mu) in RR^M &= k(bold(x)_*, bold(x)) (k(bold(x), bold(x)))^(-1) bold(y) \
hat(Sigma) in RR^(M times M) &= k(bold(x)_*, bold(x)_*) - k(bold(x)_*, bold(x))(k(bold(x), bold(x)))^(-1)k(bold(x)_*, bold(x))^top \
k(bold(x), bold(x)) in RR^(N times N) &= [k(x_i, x_j)]_(i,j = 1)^N \
k(bold(x)_*, bold(x)) in RR^(M times N) &= [k(x_(*i), x_j)]_(i,j = 1)^(M,N). $
```

```{r}
#| fig.height: 7
#| fig.width: 15

eps <- 0.009
K_XX <- k_xX(S$x, S$x)
K_xX <- k_xX(D$x, S$x)

# Adding a small amount of noise to the diagonal for numerical stability
mu <- K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% S$y
sigma <- K_xx - K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% t(K_xX)

post <- mvrnorm(100, mu, sigma) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x) |>
  group_by(x) |>
  summarise(y = mean(value), s = sd(value))

p0 +
  geom_ribbon(data = post, aes(x, ymin = y - 2*s, ymax = y + 2*s), alpha = 0.2) +
  geom_line(data = post, aes(x, y, color = "Posterior Mean")) +
  geom_point(data = S, aes(x, y, color = "Training Set"))
```

# Multioutput GPR -- Vector-valued $\mathbf{y}$
```{=typst}
- GPR can be extended to targets with >1 dimensions.
- Velocity fields: $bold(X), bold(Y) in RR^(N times 2)$
- Intrinsic Coregionalization Model (ICM): combines the kernel matrix with a similarity matrix $B$
```

```{=typst}
$ bold(X), bold(Y) in RR^(N times 2), " " bold(y) &in RR^(2N) = "vec"(Y), " " bold(X)_* in RR^(M times 2) \
  bold(y)_* | bold(X), bold(y), bold(X)_* " " &~ " " cal(N)_(2M) (hat(mu), hat(Sigma)) \
  hat(mu) in RR^(2M) &= K_(bold(X)_* bold(X)) K_(bold(X) bold(X))^(-1) bold(y) \
  hat(Sigma) in RR^(2M times 2M) &= K_(bold(X)_* bold(X)_*) - K_(bold(X)_* bold(X)) K_(bold(X) bold(X))^(-1) K_(bold(X)_* bold(X))^top \
  K_(bold(X) bold(X)) in RR^(2N times 2N) &= B times.circle k(bold(X), bold(X)) \
  K_(bold(X)_* bold(X)) in RR^(2M times 2N) &= B times.circle k(bold(X)_*, bold(X)) \
  K_(bold(X)_* bold(X)_*) in RR^(2M times 2M) &= B times.circle k(bold(X)_*, bold(X)_*) \
  B in RR^(2 times 2) &= "corr"(bold(Y)) = (
  (angle.l bold(y)_i - overline(bold(y))_i, bold(y)_j - overline(bold(y))_j angle.r) / 
    (||bold(y)_i - overline(bold(y))_i|| ||bold(y)_j - overline(bold(y))_j||)
)_(i,j = 1)^2 
$
```

```{r}
#| fig.height: 10
#| fig.width: 15

library(gcookbook)
library(scico)
library(grid)
set.seed(123)

# Keep a subset in the middle of the the z-axis
d_isabel <- isabel |>
  filter(z == 10.035) |>
  as_tibble()

# Keep 1 out of every 'by' values in vector x
every_n <- function(x, by = 4) {
  x <- sort(x)
  x[seq(1, length(x), by = by)]
}

# Keep 1 of every 4 values in x and y
keep_x <- every_n(unique(isabel$x), 4)
keep_y <- every_n(unique(isabel$y), 4)

# Keep only those rows where x value is in keepx and y value is in keepy
d <- d_isabel |>
  filter(x %in% keep_x, y %in% keep_y) |>
  mutate(index = 1:n())

# Need to load grid for arrow() function
library(grid)

# Make a plot with the subset, and use an arrowhead 0.1 cm long
p1 <- ggplot(d, aes(x, y)) +
  geom_segment(
    aes(xend = x + vx/60, yend = y + vy/60),
    arrow = arrow(length = unit(0.1, "cm")),
    linewidth = 0.25
  ) +
  labs(x = "Longitude", y = "Latitude")

# p1
```

```{r}
#| cache: true

# Set up the training data
S <- d |>
  slice_sample(n = floor(0.3 * nrow(d))) |>
  select(x, y, vx, vy, index)

X <- S |> select(x, y) |> as.matrix()
Y <- S |> select(vx, vy) |> as.matrix()
y <- as.vector(Y)
B <- cor(Y)
N <- nrow(X)

# Pull out test cases
Z <- d |>
  anti_join(S, by = "index") |>
  select(x, y, vx, vy, index)

X_star <- Z |> select(x, y) |> as.matrix()

k <- function(xi, xj, alpha = 1, rho = 1) {
  alpha^2 * exp(-norm(xi - xj, type = "2")^2 / (2 * rho^2))
}

k_xX <- function(x, X, alpha = 1, rho = 1) {
  N <- nrow(x)
  M <- nrow(X)

  K <- matrix(0, N, M)
  for (i in 1:N) {
    for (j in 1:M) {
      K[i, j] <- k(x[i, ], X[j, ], alpha, rho)
    }
  }

  K
}

get_posterior <- function(X, y, B, Z, alpha = 1, rho = 1, sigma = c(1e-9, 1e-9)) {
  N <- nrow(X)
  M <- nrow(Z)

  V <- diag(sigma^2, 2)
  K_XX <- kronecker(B, k_xX(X, X, alpha, rho)) + kronecker(V, diag(N))
  K_ZX <- kronecker(B, k_xX(Z, X, alpha, rho))
  K_ZZ <- kronecker(B, k_xX(Z, Z, alpha, rho))

  mu <- K_ZX %*% solve(K_XX) %*% y
  mu <- matrix(mu, M, 2)
  Sigma <- K_ZZ - K_ZX %*% solve(K_XX) %*% t(K_ZX)

  return(lst(mu, Sigma))
}

rmse <- function(y, yhat) {
  sqrt(1/length(y) * sum((yhat - y)^2))
}

naive <- get_posterior(X, y, B, as.matrix(select(Z, x, y)))
```

```{r}
#| fig.height: 9
#| fig.width: 9
#| fig.align: center

get_evaluation <- function(mu, Z, S, dd = 50) {
  out <- Z |>
    bind_cols(as_tibble(mu) |> set_names(c("vx_h", "vy_h"))) |>
    select(x, y, vx, vy, vx_h, vy_h, index) |>
    bind_rows(S) |>
    mutate(
      col = factor(
        as.numeric(index %in% S$index),
        levels = 0:1,
        labels = c("Test", "Train")
      )
    )

  col <- c("Training Points" = ORANGE, "Test Points" = BLUE)

  fig <- ggplot() +
    geom_segment(
      data = out,
      aes(x, y, xend = x + vx/dd, yend = y + vy/dd),
      arrow = arrow(length = unit(0.1, "cm")),
      linewidth = 0.25,
      alpha = 0.3
    ) +
    geom_segment(
      data = filter(out, col == "Train"),
      aes(x, y, xend = x + vx/dd, yend = y + vy/dd, color = "Training Points"),
      arrow = arrow(length = unit(0.1, "cm")),
      linewidth = 0.25
    ) +
    geom_segment(
      data = filter(out, col == "Test"),
      aes(x, y, xend = x + vx_h/dd, yend = y + vy_h/dd, color = "Test Points"),
      arrow = arrow(length = unit(0.1, "cm")),
      linewidth = 0.25
    ) +
    scale_color_manual(name = "", values = col) +
    labs(x = "Longitude", y = "Latitude") +
    theme(legend.position = "none")

  perf <- out |>
    filter(col == "Test") |>
    summarize(
      rmse_vx = rmse(vx, vx_h),
      rmse_vy = rmse(vy, vy_h)
    )

  return(lst(fig, perf))
}

res <- get_evaluation(naive$mu, Z, S, 60)
res$fig
```


<!-- PIV DATA -->

```{r}
#| fig.height: 10
#| fig.width: 10
#| fig.align: center

library(R.matlab)

piv <- readMat("PIV_Data_Tank_Lvl100.mat")

D <- tibble(
  x = as.vector(piv$x),
  y = as.vector(piv$y),
  vx = as.vector(piv$u),
  vy = as.vector(piv$v)
)

p2 <- ggplot(D, aes(x, y)) +
  geom_segment(
    aes(xend = x + vx, yend = y + vy),
    arrow = arrow(length = unit(0.1, "cm")),
    linewidth = 0.25
  )

p2
```

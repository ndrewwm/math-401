---
title: Vector-valued Gaussian Processes
knitr:
  opts_chunk:
    echo: false
    warning: false
    message: false
format:
  poster-typst:
    size: "48x36"
    poster-authors: "Andrew Moore, Grady Wright"
    departments: "Department of Mathematics"
    institution-logo: "./images/boisestate-laligned-logo.png"
    footer-text: "Spring 2025, Senior Showcase"
    footer-url: "https://ndrewwm.github.io/math-401"
    footer-emails: "andrewmoore1@u.boisestate.edu"
    footer-color: "0033A0" #"D64309"
    keywords: ["Gaussian Processes", "Statistics", "Velocity Fields"]
---

# Overview of Gaussian Processes

Gaussian processes generalize the multivariate Gaussian distribution and can be used to describe a probability distribution over families of functions.

## Multivariate Gaussian Distribution

```{=typst}
The multivariate normal distribution is used to model _random vectors_ (vectors of jointly distributed random variables).

$ bold(x) in RR^N &tilde.op cal(N)_N (bold(mu), bold(Sigma)) \ 
  bold(mu) in RR^N &= (mu_1, mu_2, ..., mu_N)^top = (EE(x_1), EE(x_2), ..., EE(x_N))^top \ 
  bold(Sigma) in RR^(N times N) &= EE((bold(x) - mu)(bold(x) - mu)^top) = [op("cov")(x_i, x_j)]_(i,j)^N \
  x_i &tilde.op cal(N)(mu_i, bold(Sigma)_(i i)) $
```

## Gaussian Processes (GPs)

```{=typst}
- _Gaussian process_ (GP): an uncountably infinite collection of random variables; any finite sample is a draw from a MV Gaussian distribution.
- GPs are fully specified by a _mean function_ $m$ and _covariance (kernel) function_ $k$.
- The kernel function must produce a positive semi-definite matrix when evaluated on a set of input points (or vectors).
```


```{=typst}
We focus on the _squared exponential kernel_ $k: RR^p times RR^p -> RR$, defined as:

$ k(x, x') = alpha^2 op("exp")(-1/(2rho^2) ||x - x'||^2). $

- $||dot||$ is the Euclidean Norm: $||bold(x)|| = sqrt(x_1 + x_2 + dots.h.c + x_N)$
- $alpha$ and $rho$ are _hyperparameters_ (chosen, or estimated from data)
```

# Gaussian Process Regression -- Univariate $\mathbf{y}$

```{=typst}

In practice, Gaussian Processes are often brought to bear on _regression problems_, in which an analyst has collected a dataset $S = (bold(x), bold(y)) = { (x_i, y_i) : x_i in RR^p, y_i in RR^d, i in 1, 2, ..., N }$ with the goal of learning the relationship $f$ between $bold(x)$ and $bold(y)$. 

\
Let $f(x) = sin(2x) + sin(4x)$ be an unknown function that an analyst is attempting to model using sampled data. Let $N = 20$ and $M = 400 - N$. We wish to use our sample $S$ to predict the values $bold(y)_* = f(bold(x)_*)$ for test data $bold(x)_* in RR^M$.
```

```{r}
#| fig.height: 7
#| fig.width: 15

library(tidyverse)
library(MASS, include.only = "mvrnorm")
set.seed(123)
theme_set(theme_minimal(base_size = 32) + theme(panel.grid.minor = element_blank()))

f <- function(x) sin(2*x) + sin(4*x)
k <- function(x_i, x_j, alpha, rho) alpha^2 * exp(-(x_i - x_j)^2 / (2 * rho^2))

k_xX <- function(x, X, alpha = 1, rho = 0.4) {
  N <- NROW(x)
  M <- NROW(X)
  K <- matrix(0, N, M)
  for (n in 1:N) {
    for (m in 1:M) {
      K[n, m] <- k(x[n], X[m], alpha, rho)
    }
  }
  return(K)
}

N <- 400
n <- 20
x <- seq(-5, 5, length.out = N)
f_x <- f(x)

D <- tibble(x, y = f_x)
S <- slice_sample(D, n = n)
Z <- anti_join(D, S, by = "x")

K_xx <- k_xX(x, x)

leg <- c("Training Set" = "black", "Target" = "orange", "Prior" = "grey", "Posterior Mean" = "blue")
p0 <- ggplot() +
  geom_line(data = D, aes(x, y, color = "Target")) +
  geom_point(data = S, aes(x, y, color = "Training Set")) +
  scale_color_manual(name = "", values = leg) +
  labs(y = "f(x)") +
  theme(
    legend.position = c(0.13, 0.9)
  )

# p0
```

```{=typst}
\ \
We can draw samples from the prior distribution:

$
f " " &~ " " cal("GP")(bold(0), k) \
bold(y)_* " " &~ " " cal(N)_M (bold(0), k(bold(x)_*, bold(x)_*)) \
k(bold(x)_*, bold(x)_*) in RR^(M times M) &= [k(bold(x)_(*i), bold(x)_(*j))]_(i,j)^M. $


- Draws from the prior distribution (shown in grey) don't necessarily agree with our data points.
- Kernel choice determines properties of $f$ (e.g., smoothness)
```

```{r}
#| fig.height: 7
#| fig.width: 15

prior <- mvrnorm(10, rep_len(0, nrow(K_xx)), K_xx) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x)

p0 + geom_line(data = prior, aes(x, y = value, group = name, color = "Prior"), alpha = 0.7)
```

Our prior model for $f$ and our observed data $S$ can be combined to form a _posterior_ distribution:

```{=typst}
$ bold(y)_* | bold(x), bold(y), bold(x)_* " " &~ " " cal(N)_M (hat(mu), hat(Sigma)) \
hat(mu) in RR^M &= k(bold(x)_*, bold(x)) (k(bold(x), bold(x)))^(-1) bold(y) \
hat(Sigma) in RR^(M times M) &= k(bold(x)_*, bold(x)_*) - k(bold(x)_*, bold(x))(k(bold(x), bold(x)))^(-1)k(bold(x)_*, bold(x))^top \
k(bold(x), bold(x)) in RR^(N times N) &= [k(x_i, x_j)]_(i,j)^N \
k(bold(x)_*, bold(x)) in RR^(M times N) &= [k(x_(*i), x_j)]_(i,j)^(M,N). $
```

```{r}
#| fig.height: 7
#| fig.width: 15

eps <- 0.009
K_XX <- k_xX(S$x, S$x)
K_xX <- k_xX(D$x, S$x)

# Adding a small amount of noise to the diagonal for numerical stability
mu <- K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% S$y
sigma <- K_xx - K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% t(K_xX)

post <- mvrnorm(100, mu, sigma) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x) |>
  group_by(x) |>
  summarise(y = mean(value), s = sd(value))

p0 +
  geom_ribbon(data = post, aes(x, ymin = y - 2*s, ymax = y + 2*s), alpha = 0.2) +
  geom_line(data = post, aes(x, y, color = "Posterior Mean")) +
  geom_point(data = S, aes(x, y, color = "Training Set"))
```

# Multioutput GPR -- Vector-valued $\mathbf{y}$

1. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.
1. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.
1. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut.

| Lorem ipsum dolor sit. | Lorem ipsum. | Lorem ipsum. |
|------------------------|--------------|--------------|
| Lorem ipsum dolor.     | Lorem ipsum. | $\alpha$     |
| Lorem ipsum.           | Lorem ipsum. | $\beta$      |
| Lorem.                 | Lorem.       | $\gamma$     |
| Lorem ipsum dolor.     | Lorem.       | $\theta$     |

: Lorem ipsum dolor sit amet {#tbl-1}

Lorem @tbl-1 ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut postea variari voluptas distinguique possit, augeri amplificarique non possit. At etiam Athenis, ut e patre audiebam facete et urbane Stoicos irridente, statua est in quo a nobis philosophia defensa et.

## Lorem ipsum dolor sit amet

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. Quod idem licet transferre in voluptatem, ut postea variari voluptas distinguique possit, augeri amplificarique non possit. At etiam Athenis, ut e patre.

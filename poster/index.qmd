---
title: Vector-valued Gaussian Processes
execute: 
  freeze: true
bibliography: poster.bib
nocite: |
  @harlander2012reconstruction, @alvarez2012kernels, @williams2006gaussian, @chang2018r
knitr:
  opts_chunk:
    echo: false
    warning: false
    message: false
format:
  poster-typst:
    size: "48x36"
    poster-authors: "Andrew Moore, Grady Wright (Advisor)"
    departments: "Department of Mathematics"
    institution-logo: "./images/boisestate-leftalignedlogo-2color-rgb.png"
    footer-text: "Spring 2025, Senior Showcase"
    footer-url: "https://ndrewwm.github.io/math-401"
    footer-emails: "andrewmoore1@u.boisestate.edu"
    footer-color: "0033A0" #"D64309"
    keywords: ["Gaussian Processes", "Statistics", "Velocity Fields"]
---

# Overview of Gaussian Processes

Gaussian processes generalize the multivariate Gaussian dist. and can describe probability distributions over functions. [@williams2006gaussian]

## Multivariate Gaussian Distribution

```{=typst}
$ bold(z) in RR^N &tilde.op cal(N)_N (bold(mu), bold(Sigma)) " " z_i tilde.op cal(N)(mu_i, bold(Sigma)_(i i)) \ 
  bold(mu) in RR^N &= (mu_1, mu_2, ..., mu_N)^top = (EE(z_1), EE(z_2), ..., EE(z_N))^top \ 
  bold(Sigma) in RR^(N times N) &= EE((bold(z) - mu)(bold(z) - mu)^top) = [op("cov")(z_i, z_j)]_(i,j = 1)^N $
```

## Gaussian Processes (GPs)

```{=typst}
- GP: an uncountably infinite collection of random variables; any finite sample is a draw from a MV Gaussian distribution.
- GPs fully specified by _mean_ and _covariance (kernel)_ functions.
- The covariance function $k$ must produce a positive semi-definite matrix.
- Squared exponential kernel, $k: RR^p times RR^p -> RR$: $ k(x, x') = alpha^2 op("exp")(-1/(2rho^2) ||x - x'||^2). $
// - $||dot||$ is the Euclidean Norm: $||bold(x)|| = sqrt(x_1 + x_2 + dots.h.c + x_N)$
- $alpha$ and $rho$ are _hyperparameters_ (chosen, or estimated from data)
```

# Gaussian Process Regression -- Univariate $\mathbf{y}$

```{=typst}
```

```{r}
#| fig.height: 7
#| fig.width: 15

ORANGE <- "#D64309"
BLUE <- "#0033A0"

library(tidyverse)
library(MASS, include.only = "mvrnorm")
set.seed(123)
theme_set(theme_minimal(base_size = 32) + theme(panel.grid.minor = element_blank()))

f <- function(x) sin(2*x) + sin(4*x)
k <- function(x_i, x_j, alpha, rho) alpha^2 * exp(-(x_i - x_j)^2 / (2 * rho^2))

k_xX <- function(x, X, alpha = 1, rho = 0.4) {
  N <- NROW(x)
  M <- NROW(X)
  K <- matrix(0, N, M)
  for (n in 1:N) {
    for (m in 1:M) {
      K[n, m] <- k(x[n], X[m], alpha, rho)
    }
  }
  return(K)
}

N <- 400
n <- 20
x <- seq(-5, 5, length.out = N)
f_x <- f(x)

D <- tibble(x, y = f_x)
S <- slice_sample(D, n = n)
Z <- anti_join(D, S, by = "x")

K_xx <- k_xX(x, x)

leg <- c("Training Set" = "black", "Target" = ORANGE, "Draws from the Prior" = "grey", "Posterior Mean" = BLUE)
p0 <- ggplot() +
  geom_line(data = D, aes(x, y, color = "Target")) +
  geom_point(data = S, aes(x, y, color = "Training Set")) +
  scale_color_manual(name = "", values = leg) +
  labs(y = "f(x)") +
  theme(
    legend.position = c(0.13, 0.9)
  )

# p0
```

```{=typst}
$ S = (bold(x), bold(y)) &= { (x_i, y_i) : x_i, y_i in RR, i in 1, 2, ..., N } \
  f " " ~ " " cal("GP")(bold(0), k) " " & " "
  bold(y)_* " " ~ " " cal(N)_M (bold(0), k(bold(x)_*, bold(x)_*)) \
  k(bold(x)_*, bold(x)_*) in RR^(M times M) &= [k(x_(*i), x_(*j))]_(i,j = 1)^M. $

// - Draws from the prior distribution (shown in grey) don't necessarily agree with the data points.
// - Kernel choice determines properties of $f$ (e.g., smoothness)
```

```{r}
#| fig.height: 6.2
#| fig.width: 15

prior <- mvrnorm(10, rep_len(0, nrow(K_xx)), K_xx) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x)

p0 + geom_line(data = prior, aes(x, y = value, group = name, color = "Draws from the Prior"), alpha = 0.7)
```

```{=typst}
$ bold(y)_* | bold(x), bold(y), bold(x)_* " " &~ " " cal(N)_M (hat(mu), hat(Sigma)) \
hat(mu) in RR^M &= k(bold(x)_*, bold(x)) k(bold(x), bold(x))^(-1) bold(y) \
hat(Sigma) in RR^(M times M) &= k(bold(x)_*, bold(x)_*) - k(bold(x)_*, bold(x))k(bold(x), bold(x))^(-1)k(bold(x)_*, bold(x))^top $
```

```{=typst}
$ k(bold(x), bold(x)) in RR^(N times N) &= [k(x_i, x_j)]_(i,j = 1)^N \
k(bold(x)_*, bold(x)) in RR^(M times N) &= [k(x_(*i), x_j)]_(i,j = 1)^(M,N). $
```

```{r}
#| fig.height: 6.2
#| fig.width: 15

eps <- 0.009
K_XX <- k_xX(S$x, S$x)
K_xX <- k_xX(D$x, S$x)

# Adding a small amount of noise to the diagonal for numerical stability
mu <- K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% S$y
sigma <- K_xx - K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% t(K_xX)

post <- mvrnorm(100, mu, sigma) |>
  t() |>
  as_tibble() |>
  mutate(x = x) |>
  pivot_longer(-x) |>
  group_by(x) |>
  summarise(y = mean(value), s = sd(value))

p0 +
  geom_ribbon(data = post, aes(x, ymin = y - 2*s, ymax = y + 2*s), alpha = 0.2) +
  geom_line(data = post, aes(x, y, color = "Posterior Mean")) +
  geom_point(data = S, aes(x, y, color = "Training Set"))
```

# Multioutput GPR -- Vector-valued $\mathbf{y}$

```{=typst}
- Velocity fields: $bold(X), bold(Y) in RR^(N times 2), " " bold(y) &in RR^(2N) = "vec"(Y), " " bold(X)_* in RR^(M times 2)$
```

<!-- // - Idea: columns of $bold(Y)$ might not be independent -->
- Intrinsic Coregionalization Model (ICM) [@alvarez2012kernels] [@bonilla2007multi]

```{=typst}
$ bold(y)_* | bold(X), bold(y), bold(X)_* " " &~ " " cal(N)_(2M) (hat(mu), hat(Sigma)) \
  hat(mu) in RR^(2M) &= K_(bold(X)_* bold(X)) K_(bold(X) bold(X))^(-1) bold(y) \
  hat(Sigma) in RR^(2M times 2M) &= K_(bold(X)_* bold(X)_*) - K_(bold(X)_* bold(X)) K_(bold(X) bold(X))^(-1) K_(bold(X)_* bold(X))^top \
  K_(bold(X) bold(X)) in RR^(2N times 2N) &= B times.circle k(bold(X), bold(X)) \
  K_(bold(X)_* bold(X)) in RR^(2M times 2N) &= B times.circle k(bold(X)_*, bold(X)) \ 
  K_(bold(X)_* bold(X)_*) in RR^(2M times 2M) &= B times.circle k(bold(X)_*, bold(X)_*) \
  B in RR^(2 times 2) &= "corr"(bold(Y)) = (
  (angle.l bold(y)_i - overline(bold(y))_i, bold(y)_j - overline(bold(y))_j angle.r) / 
    (||bold(y)_i - overline(bold(y))_i|| ||bold(y)_j - overline(bold(y))_j||)
)_(i,j = 1)^2 $
```

**Case study:** Hurricane Isabel Simulation [@chang2018r]

```{r}
#| fig.height: 10
#| fig.width: 15

library(gcookbook)
library(scico)
library(grid)
set.seed(123)

# Keep a subset in the middle of the the z-axis
d_isabel <- isabel |>
  filter(z == 10.035) |>
  as_tibble()

# Keep 1 out of every 'by' values in vector x
every_n <- function(x, by = 4) {
  x <- sort(x)
  x[seq(1, length(x), by = by)]
}

# Keep 1 of every 4 values in x and y
keep_x <- every_n(unique(isabel$x), 4)
keep_y <- every_n(unique(isabel$y), 4)

# Keep only those rows where x value is in keepx and y value is in keepy
d <- d_isabel |>
  filter(x %in% keep_x, y %in% keep_y) |>
  mutate(index = 1:n())

# Need to load grid for arrow() function
library(grid)

# Make a plot with the subset, and use an arrowhead 0.1 cm long
p1 <- ggplot(d, aes(x, y)) +
  geom_segment(
    aes(xend = x + vx/60, yend = y + vy/60),
    arrow = arrow(length = unit(0.1, "cm")),
    linewidth = 0.25
  ) +
  labs(x = "Longitude", y = "Latitude")

# p1
```

```{r}
#| cache: true

# Set up the training data
S <- d |>
  slice_sample(n = floor(0.3 * nrow(d))) |>
  select(x, y, vx, vy, index)

X <- S |> select(x, y) |> as.matrix()
Y <- S |> select(vx, vy) |> as.matrix()
y <- as.vector(Y)
B <- cor(Y)
N <- nrow(X)

# Pull out test cases
Z <- d |>
  anti_join(S, by = "index") |>
  select(x, y, vx, vy, index)

X_star <- Z |> select(x, y) |> as.matrix()

k <- function(xi, xj, alpha = 1, rho = 1) {
  alpha^2 * exp(-norm(xi - xj, type = "2")^2 / (2 * rho^2))
}

k_xX <- function(x, X, alpha = 1, rho = 1) {
  N <- nrow(x)
  M <- nrow(X)

  K <- matrix(0, N, M)
  for (i in 1:N) {
    for (j in 1:M) {
      K[i, j] <- k(x[i, ], X[j, ], alpha, rho)
    }
  }

  K
}

get_posterior <- function(X, y, B, Z, alpha = 1, rho = 1, sigma = c(1e-9, 1e-9)) {
  N <- nrow(X)
  M <- nrow(Z)

  V <- diag(sigma^2, 2)
  K_XX <- kronecker(B, k_xX(X, X, alpha, rho)) + kronecker(V, diag(N))
  K_ZX <- kronecker(B, k_xX(Z, X, alpha, rho))
  K_ZZ <- kronecker(B, k_xX(Z, Z, alpha, rho))

  mu <- K_ZX %*% solve(K_XX) %*% y
  mu <- matrix(mu, M, 2)
  Sigma <- K_ZZ - K_ZX %*% solve(K_XX) %*% t(K_ZX)

  return(lst(mu, Sigma))
}

rmse <- function(y, yhat) {
  sqrt(1/length(y) * sum((yhat - y)^2))
}

naive <- get_posterior(X, y, B, as.matrix(select(Z, x, y)))
```

```{r}
library(ggvfields)

vfield <- function(dat, L = 0.07, xlab = "x", ylab = "y") {
  ggplot() +
    geom_vector(
      data = dat,
      aes(x = x, y = y, fx = vx, fy = vy),
      arrow = arrow(length = unit(0.1, "cm")),
      show.legend = FALSE,
      color = "grey",
      L = L
    ) +
    geom_vector(
      data = dat |> filter(col == "Train"),
      aes(x = x, y = y, fx = vx, fy = vy),
      arrow = arrow(length = unit(0.1, "cm")),
      show.legend = FALSE,
      color = ORANGE,
      L = L
    ) +
    geom_vector(
      data = dat |> filter(col == "Test"),
      aes(x = x, y = y, fx = vx_h, fy = vy_h),
      arrow = arrow(length = unit(0.1, "cm")),
      show.legend = FALSE,
      color = BLUE,
      L = L
    ) +
    labs(x = xlab, y = ylab)
}
```

```{r}
#| fig.height: 8.5
#| fig.width: 8.5
#| fig.align: center

get_evaluation <- function(mu, Z, S, dd = 50, xlab = "Longitude", ylab = "Latitude") {
  out <- Z |>
    bind_cols(as_tibble(mu) |> set_names(c("vx_h", "vy_h"))) |>
    select(x, y, vx, vy, vx_h, vy_h, index) |>
    bind_rows(S) |>
    mutate(
      col = factor(
        as.numeric(index %in% S$index),
        levels = 0:1,
        labels = c("Test", "Train")
      )
    )

  perf <- out |>
    filter(col == "Test") |>
    summarize(
      rmse_vx = rmse(vx, vx_h),
      rmse_vy = rmse(vy, vy_h)
    )

  return(lst(perf, out))
}

res <- get_evaluation(naive$mu, Z, S, 60)
vfield(res$out, L = 0.6, xlab = "Longitude", ylab = "Latitude")
```


<!-- PIV DATA -->

**Case study:** Particle Image Velocimetry [@harlander2012reconstruction]

```{r}
library(R.matlab)

piv <- readMat("PIV_Data_Tank_Lvl100.mat")

D <- tibble(
  x = as.vector(piv$x),
  y = as.vector(piv$y),
  vx = as.vector(piv$u),
  vy = as.vector(piv$v),
  index = 1:length(piv$x)
)
```

```{r}
#| cache: true

set.seed(123)

# Set up the training data
S <- D |>
  slice_sample(n = floor(0.3 * nrow(D))) |>
  select(x, y, vx, vy, index)

X <- S |> select(x, y) |> as.matrix()
Y <- S |> select(vx, vy) |> as.matrix()
y <- as.vector(Y)
B <- cor(Y)
N <- nrow(X)

# Pull out test cases
Z <- D |>
  anti_join(S, by = "index") |>
  select(x, y, vx, vy, index)

X_star <- Z |> select(x, y) |> as.matrix()

naive <- get_posterior(X, y, B, X_star, sigma = c(1e-06, 1e-06))
```

```{r}
#| fig.height: 10
#| fig.width: 10
#| fig.align: center

res <- get_evaluation(naive$mu, Z, S, 15, "x", "y")
vfield(res$out)
```

*Note:* colors reflect <span style="color: #D64309">sample data</span>, <span style="color: #0033A0">posterior mean</span>, and <span style="color: grey;">test points.</span>

```{=typst}
\
```

[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 401",
    "section": "",
    "text": "Hello! This website contains the work I completed during the course of Math-401, my senior project and final course to complete my BS in Applied Mathematics. I completed this course during Spring 2025; Dr. Grady Wright (Professor, Boise State Department of Mathematics), served as my project advisor. During the course, I focused on the topic of Gaussian Processes (GPs), a generalization of the multivariate Gaussian distribution, and how GPs can be useful in the context of various regression problems.\nYou can browse my writings on different subtopics from the sidebar, or by searching using the magnifying glass in the upper-right corner of the page. This website was built using Quarto, with data analysis, inference, and visualization being conducted using the R and Stan programming languages. My poster’s layout was built from an excellent typst template, with a few formatting tweaks."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "MATH 401",
    "section": "4/25/25",
    "text": "4/25/25\n\nWrap-up. Working on completing incomplete reflections."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "MATH 401",
    "section": "4/14/25",
    "text": "4/14/25\n\nReview of current draft\n\nShould tighten up which variables are used in Section I– would be better to use \\(\\mathbf{z}\\), rather than \\(\\mathbf{x}\\) (which is what we use to refer to training data).\n\nScaling of the PIV vector field is off (all the magnitudes are too small)– and appears to be scrambled compared to what we expect. Bug in my code somewhere."
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "MATH 401",
    "section": "4/4/25",
    "text": "4/4/25\n\nSummary of my progress re: poster drafting"
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "MATH 401",
    "section": "3/28/25",
    "text": "3/28/25\n\nReading about the ICM and LCM\nUpdating Stan code used for inference of hyperparameters."
  },
  {
    "objectID": "index.html#section-4",
    "href": "index.html#section-4",
    "title": "MATH 401",
    "section": "3/14/25",
    "text": "3/14/25\n\nWriteup of multioutput GPR is close to finished. Used data from a simulation of hurricane Isabel to test out the approach.\nI’ve been working on parameter estimation for multioutput GPR.\n\nI’ve been using R.M.S.E. to measure performance on unseen test data.\nMy attempts to optimize the log-likelihood function were not giving better performance over naive defaults of \\(\\alpha = 1\\) and \\(\\rho = 1\\).\nDue to this, I began testing out the Stan programming language for estimation, which appears to work a bit better (but not improving over the defaults).\n\nMultioutput GPR appears to relate to the idea of the Matrix Normal distribution. In some circumstances, it may be equivalent?"
  },
  {
    "objectID": "index.html#section-5",
    "href": "index.html#section-5",
    "title": "MATH 401",
    "section": "3/10/25",
    "text": "3/10/25\n\nI needed to cancel due to a conflict. We’ll postpone until 3/14."
  },
  {
    "objectID": "index.html#section-6",
    "href": "index.html#section-6",
    "title": "MATH 401",
    "section": "3/3/25",
    "text": "3/3/25\n\nStill working on code for multioutput GPR.\nReviewed my CV and discussed topics related to Reflection 2.\n\nLearned that Tukey was involved in a landmark implementation of the Fast Fourier Transform (FFT)."
  },
  {
    "objectID": "index.html#section-7",
    "href": "index.html#section-7",
    "title": "MATH 401",
    "section": "2/24/25",
    "text": "2/24/25\n\nReading Bonilla, Chai, & Williams (2007)\nWorking towards implementing a simulation of multi-task GPR\nLast week, I completed a basic simulation of bivariate GPR, which can be viewed here. It’s light on theory/notation, but I think it follows fairly straightforwardly from the univariate case.\n\nI need to actually start doing more formal parameter estimation for the kernel/covariance functions. Currently, I’ve just been using arbitrary values for \\(\\alpha\\), \\(\\rho\\) and \\(\\sigma\\) in the RBF kernel. These are fine for demonstration purposes, but aren’t representative of what a fully application would entail."
  },
  {
    "objectID": "index.html#section-8",
    "href": "index.html#section-8",
    "title": "MATH 401",
    "section": "2/14/25",
    "text": "2/14/25\n\nReading and searching for more background information about multi-output Gaussian Processes\n\ne.g., vector-valued GPs (blog post)\nCompleted review of the multivariate Gaussian distribution (link)\nStill working on modeling bivariate data, but not (yet) fruitfully\n\n\nIdeally, we could model position and velocity together as a vector\n\\[\n\\begin{bmatrix} u \\\\ v \\end{bmatrix} = f(x, y) + \\epsilon\n\\]\nrather than something like \\[\n\\begin{align*}\nu &= f_1(x) + \\epsilon_1 \\\\\nv &= f_2(y) + \\epsilon_2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "index.html#section-9",
    "href": "index.html#section-9",
    "title": "MATH 401",
    "section": "2/7/25",
    "text": "2/7/25\n\nDiscussed the PIV laboratory data that Dr. Wright worked on in 2012.\n\\(\\partial_x u + \\partial_y v + \\partial_z w = 0\\)\n\nrecall: \\(u\\) position, \\(v\\) velocity\n\nGoal is to induce Rossby waves within a rotating cylinder.\n\nPreprocessing of data may be subject to artifacts– opportunity to use statistical methods to fill in, or correct, artifacts."
  },
  {
    "objectID": "index.html#section-10",
    "href": "index.html#section-10",
    "title": "MATH 401",
    "section": "1/31/25",
    "text": "1/31/25\n\nWorking on an overview of Gaussian processes\n\nI have a more formal writeup (in the sense of notation) of GPR here\n\nWorking on an overview of KRR\n\ntodos:\n\nupdate citations"
  },
  {
    "objectID": "index.html#section-11",
    "href": "index.html#section-11",
    "title": "MATH 401",
    "section": "1/23/25",
    "text": "1/23/25\n\nReviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing."
  },
  {
    "objectID": "index.html#section-12",
    "href": "index.html#section-12",
    "title": "MATH 401",
    "section": "1/16/25",
    "text": "1/16/25\n\nDiscussion, admin work\nDecided to cover first 3 sections of Kanagawa et al. (2018)\nMentioned __ textbook."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "MATH 401",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor GPs, uncertainty is represented through the posterior variance, and can be estimated/visualized via draws from the posterior distribution. With KRR, Kanagawa et al. (2018) suggest that the posterior variance can be interpreted as a “worst case error” in a Reproducing Kernel Hilbert Space (RKHS).↩︎\nFor KRR, it is assumed that the target function belongs to a RKHS (or can be well approximated by function in a RKHS). However, the support for GPs is not identical to that of a RKHS (Kanagawa et al. 2018, 3).↩︎"
  },
  {
    "objectID": "reflections/cv.html",
    "href": "reflections/cv.html",
    "title": "cv",
    "section": "",
    "text": "Technical Skills\n\nPython (pandas, prefect, flask, fastapi)\nSQL (dbt, Oracle, Snowflake, MS SQL-Server)\nR (dplyr/tidyverse, devtools)\nJS (Svelte, SvelteKit)\nStatistical analysis (R, SAS, SPSS)\nData visualization (Tableau, ggplot2/shiny, Observable)\nGit/GitHub, Google Apps Suite, Qualtrics\n\n\n\nProfessional Experience\n\nSenior Data Engineer\nUniversity of Colorado-Boulder, Office of Information Technology\nApril 2024 to present\n\n\nData Engineer\nUniversity of Colorado-Boulder, Office of Information Technology\nJanuary 2023 to April 2024\n\n\nData Warehouse Developer\nBoise State University, Office of Information Technology\nJuly 2021 to December 2022\n\n\nInstitutional Research Analyst\nBoise State University, Office of Institutional Research\nJanuary 2020 to July 2021\n\n\nData Analyst\nUniversity of Michigan, School of Social Work\nNovember 2015 to December 2019\n\n\nLab Manager & Research Technician\nUniversity of Michigan, Department of Psychology\nJune 2014 to November 2015\n\n\nSurvey Technician\nUniversity of Michigan, Survey Research Center\nMay 2013 to June 2014\n\n\n\nEducation\n\n\nB.S. Applied Mathematics, Statistics Emphasis  Boise State University, 2021 - 2025 (anticipated)\n\n\n\nB.A. Psychology  The College of Idaho, 2009 - 2013, cum laude\n\n\n\n\nSelected Publications\n\n\nPerron, B.E., Victor, B.G., Bushman, G., Moore, A., Ryan, J.P., Lu, A.J., & Piellusch, E.K. (2019). Detecting substance-related problems in narrative investigation summaries of child abuse and neglect using text mining and machine learning. Child Abuse & Neglect, 98, 104180. doi:10.1016/j.chiabu.2019.104180\n\n\n\nRyan J.P., Jacob B.A., Gross M., Perron B.E., Moore A., & Ferguson S.M. (2018). Early exposure to child maltreatment and academic outcomes. Child Maltreatment. Advance online publication. doi:10.1177/1077559518786815\n\n\n\n\nCommunity Engagement\n\n\nCo-Organizer, Ann Arbor R-Users Group  2017-2019\n\n\n\nWorkshop Instructor, Ann Arbor R-Users Group  2016",
    "crumbs": [
      "Home",
      "Reflections",
      "cv"
    ]
  },
  {
    "objectID": "reflections/reflection_1.html",
    "href": "reflections/reflection_1.html",
    "title": "Reflection 1",
    "section": "",
    "text": "A key aspect of my experience at Boise state is that I am a second-degree student. My first B.A. degree was in the field of psychology. After graduating in 2013, I worked as a research technician in various settings and as a data analyst. Coursework and independent research experience from my first degree prepared me for the work I would perform, but I was constantly drawn to the study of mathematics and statistical science. Modern software makes computing and visualizing statistical data quick and often easy to perform. To me, this convenience highlights the importance of understanding the fundamentals beneath a software implementation. I was computing summaries and building/visualizing small statistical models. I wanted to make sure that I wasn’t mishandling these “golems”, as Richard McElreath (McElreath 2018, chap. 1) describes them.\nWhen I began my program at Boise State, I had already completed Calculus I, Calculus II, Calculus III, and an introduction to Linear Algebra. I believe there are two themes visible in my coursework, and will elaborate on them in the following paragraphs. Below is a list of courses I completed during my second degree, with courses as part of the statistics emphasis in italics and courses I want to highlight in bold.\n\n\n\nTerm\nCourse\nTitle\n\n\n\n\n2021 Spring\nMATH 361\nProbability & Statistics I\n\n\n2021 Summer\nMATH 189\nDiscrete Mathematics\n\n\n2021 Fall\nMATH 287\nMathematical Proofs & Methods\n\n\n\nMATH 471\nData Analysis\n\n\n2022 Spring\nMATH 305\nAbstract Algebra/Number Theory\n\n\n\nMATH 314\nFoundations of Analysis\n\n\n2022 Fall\nMATH 333\nDifferential Equations w/ Matrix Theory\n\n\n\nMATH 462\nProbability & Statistics II\n\n\n2023 Spring\nMATH 472\nComputational Statistics\n\n\n2023 Fall\nMATH 365\nIntroduction to Computational Mathematics\n\n\n2024 Spring\nMATH 308\nIntroduction to Algebraic Cryptology\n\n\n2024 Fall\nMATH 403\nLinear Algebra\n\n\n\nMATH 465\nIntroduction to Numerical Methods\n\n\n\nThe first theme is the development of literacy for mathematical writing. I greatly appreciated the first few classes I took that emphasized the examination and writing of mathematical proofs. I remember MATH 287, MATH 305, and MATH 314 being very challenging, but deeply rewarding in terms of strengthening my ability to read and decompose theorems that I’d encounter in other classes. I credit MATH 287 in particular for helping me learn about the structure of proofs, and the kinds of logical arguments used to integrate and extend mathematical facts/topics.\nThe second theme is an appreciation for the “species” of mathematical objects and spaces that one might encounter. I think MATH-314 and MATH-403 are my favorite examples of this, but I should also mention MATH-305 for similar reasons. MATH-314 was interesting in that it cracks open several things we almost take for granted, such as calculus and the real number line. Calculus is incredibly far-reaching in its impact on technology and science, but we don’t really get a sense for the mathematical results that provide its foundation until studying analysis. Similarly, linear algebra is deeply influential for modern computing, but I’ve grown to appreciate how useful it is for translating mathematical concepts into multiple dimensions. Being aware of the space(s) or set(s) of numbers you’re reasoning about is crucial to framing the problems you’re working on.\nUnifying these two themes is an interest in computation and statistical inference. This stems from my motivations to pursue the degree, but my learning in MATH-462 and MATH-472 would have been much more shallow if I hadn’t taken MATH-287 and MATH-314. Most importantly, courses like MATH-365, MATH-465, and MATH-472 emphasized the implementation of mathematical methods via programming and simulation. I’ve found that defining and conducting a simulation and implementing algorithms to be the best way I learn when exploring a mathematical property or topics within a course. Looking back, I think the absence of an application area was a reason I struggled with MATH-189 and MATH-305.\n\n\n\n\nReferences\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Home",
      "Reflections",
      "Reflection 1"
    ]
  },
  {
    "objectID": "reflections/reflection_3.html",
    "href": "reflections/reflection_3.html",
    "title": "Reflection 3",
    "section": "",
    "text": "For my project, I hope to communicate the following key ideas:\n\n1. Gaussian processes (GPs) generalize the multivariate Gaussian distribution to “families” of functions.\n\nMost people have encountered the Gaussian distribution, written \\(Z \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). When the probability (density) function is graphed, it forms a bell-shaped curve. Recall that \\(\\mu\\) determines where the curve is centered, and \\(\\sigma\\) determines how closely the mass of the distribution falls around \\(\\mu\\).\n\n\n\n\n\n\n\n\n\n\n\nWe have a related probability distribution, called the multivariate Gaussian, which we use to model vectors whose entries come from a Gaussian distribution. The multivariate Gaussian distribution is parameterized by a mean vector, and covariance matrix. We write this as\n\n\\[\n\\begin{align*}\n\\mathbf{z} &= (z_1, z_2, \\dots, z_N)^\\top \\\\\n\\mathbf{z} &\\sim \\mathcal{N}_N (\\mu, \\mathbf{\\Sigma}) \\\\\n       z_i &\\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i,i}) \\\\\n\\mu &= (\\mathbb{E}(z_1), \\mathbb{E}(z_2), \\dots, \\mathbb{E}(z_N))^\\top \\\\\n\\mathbf{\\Sigma} &= cov(\\mathbf{z}) = \\mathbb{E}[(\\mathbf{z} - \\mu)(\\mathbf{z} - \\mu)^\\top]\n\\end{align*}\n\\]\n\nMultivariate Gaussian distributions are used to model vectors of a fixed size (dimension). However, we can consider vectors of any size, by introducing the idea of a Gaussian Process (GP).\nFormally, a Gaussian Process is defined as an uncountably infinite collection of random variables, of which any finite sample from the process is described by a multivariate Gaussian distribution. In this way, you could liken GPs as a way to describe probability distributions over functions (where functions are “infinitely tall” vectors) (Williams and Rasmussen 2006). We write this as \\(f \\sim \\mathcal{GP}(m, k)\\) where \\(m\\) and \\(k\\) are mean and covariance (kernel) functions.\n\n\n\n2. Gaussian processes are useful in the context of regression problems.\n\nThis perspective is helpful in the context of regression problems, in which a researcher is wants to model the relationship \\(f\\) between inputs and outputs, typically written \\(\\mathbf{y} = f(\\mathbf{x})\\). Here, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors representing data collected by the researcher.\nIf we assume \\(f\\) comes from a Gaussian process, we can choose an appropriate covariance function \\(k\\) that induces characteristics we assume \\(f\\) has. For example, we might want a kernel that results in smooth functions. Other kernels can be chosen to induce periodic behavior (think sine/cosine functions), or linearity.\nThis is attractive because we frequently encounter problems where the relationship \\(f\\) is complicated and nonlinear.\n\n\n\n\n\n\n\n\n\n\n\n\n3. We can use GPs to model functions that have more than one dimension.\n\nIt’s common for researchers and scientists to be interested in a statistical outcome that’s univariate. That is, we might want to predict the value of \\(y_*\\) (a scalar), given that we’ve collected \\((x_1, y_1), \\dots, (x_N, y_N)\\).\nHowever, not all quantities that interest us are scalars. We might be interested in modeling phenomena that are best described by vectors. An example of this is wind velocity, which has two components: direction and magnitude at a measured point in space. This means rather than a single \\(y_*\\), we might have a vector: \\(\\mathbf{y}_* = (y_1, y_2)^\\top\\).\nRather than building a model for each component, it’s likely that our analysis will benefit from considering both components together. If the components of our target vectors aren’t independent, we could leverage that information to improve our predictions.\nThis idea is translated smoothly within the framework of Gaussian Process regression. A simple approach (referred to as the Intrinsic Corregionalization Model, ICM) (Alvarez et al. 2012) is to construct a similarity matrix that summarizes the relationships between our outcome components, and combine it with our inputs.\n\n\n\n\n\n\n\n\n\n\nData from the R Graphics Cookbook (Chang 2018).\n\n\nQuestion – Think about yourself as a freshman: what would you’ve found interesting or impressive about your project?\nIn writing up this outline, I imagined persons similar to myself when I started my math program. When I began my second degree, I had disciplinary knowledge from the social sciences and some training in statistics, but wanted greater confidence in my understanding of the “nuts and bolts”. In the overview, I wanted to connect existing knowledge from initial statistics coursework to the much broader context of multivariate statistics.\nAdditionally, I wanted to illustrate how Gaussian process regression (GPR) can be used to model complex phenomena. My example in the first figure hopefully helps demonstrate this. Even with just a few points, we’re able to get a decent fit to the target function. Lastly, I think the case of a 2-dimensional outcome variable is very interesting. Due to their simplicity, most class examples teaching regression focus on scalar outcomes, and this is appropriate for learning. However, many things we would want to learn about in the world aren’t neatly quantified as scalars. GPR is very flexible, and it can be useful for all kinds of modeling problems.\n\n\n\n\n\nReferences\n\nAlvarez, Mauricio A, Lorenzo Rosasco, Neil D Lawrence, et al. 2012. “Kernels for Vector-Valued Functions: A Review.” Foundations and Trends in Machine Learning 4 (3): 195–266.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for Visualizing Data. O’Reilly Media.\n\n\nWilliams, Christopher KI, and Carl Edward Rasmussen. 2006. Gaussian Processes for Machine Learning. Vol. 2. 3. MIT press Cambridge, MA.",
    "crumbs": [
      "Home",
      "Reflections",
      "Reflection 3"
    ]
  },
  {
    "objectID": "gaussian_processes/bivariate_case/index.html",
    "href": "gaussian_processes/bivariate_case/index.html",
    "title": "Modelling two variables using GPR",
    "section": "",
    "text": "Dr. Gramacy’s textbook, Surrogates (Gramacy 2020), particularly chapter 5, was very helpful in putting together the code for this simulation.\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lhs)\nlibrary(plotly)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\n\n# Our target function\n# x is an N x 2 matrix\nf &lt;- function(x) {\n  x[, 1] * exp(-x[, 1]^2 - x[, 2]^2)\n}\n\n# x_i and x_j can be vectors or scalars \nrbf &lt;- function(xi, xj, alpha = 1, rho = 1) {\n  alpha^2 * exp(-norm(xi - xj, type = \"2\") / (2 * rho^2))\n}\n\nk_XX &lt;- function(X, err = eps) {\n  N &lt;- nrow(X)\n  K &lt;- matrix(0, N, N)\n  for (i in 1:N) {\n    for (j in 1:N) {\n      K[i, j] &lt;- rbf(X[i, ], X[j, ])\n    }\n  }\n\n  if (!is.null(err)) {\n    K &lt;- K + diag(err, ncol(K))\n  }\n\n  K\n}\n\nk_xX &lt;- function(x, X) {\n  N &lt;- nrow(x)\n  M &lt;- nrow(X)\n\n  K &lt;- matrix(0, N, M)\n  for (i in 1:N) {\n    for (j in 1:M) {\n      K[i, j] &lt;- rbf(x[i, ], X[j, ])\n    }\n  }\n\n  K\n}\n\n# Training dataset: D = (X, Y)\nX &lt;- randomLHS(100, 2)\nX[, 1] &lt;- -4 + 8*X[, 1]\nX[, 2] &lt;- -4 + 8*X[, 2]\nY &lt;- f(X)\n\n# Test points: Z\nx &lt;- seq(-4, 4, length.out = 20)\nZ &lt;- expand.grid(x, x) |&gt; as.matrix()\n\nK_XX &lt;- k_XX(X) + diag(eps, nrow(X))\nK_ZX &lt;- k_xX(Z, X)\nK_ZZ &lt;- k_XX(Z)\n\nm_post &lt;- K_ZX %*% solve(K_XX) %*% Y\ns_post &lt;- K_ZZ - K_ZX %*% solve(K_XX) %*% t(K_ZX)\n\n\nresults &lt;- as_tibble(Z) |&gt;\n  rename(x = Var1, y = Var2) |&gt;\n  mutate(z_m = as.vector(m_post), z_sd = sqrt(diag(s_post)))\n\np1 &lt;- ggplot(results, aes(x, y, fill = z_m)) +\n  geom_tile() +\n  scale_fill_viridis_c(name = expression(mu)) +\n  labs(title = \"Posterior mean\")\n\np2 &lt;- ggplot(results, aes(x, y, fill = z_sd)) +\n  geom_tile() +\n  scale_fill_viridis_c(name = expression(sigma)) +\n  labs(title = \"Posterior standard deviation\")\n\np1 | p2\n\n\n\n\n\n\n\n\n\nplot_ly(z = ~matrix(f(Z), ncol = 20)) |&gt; add_surface()\n\n\n\n\nplot_ly(z = matrix(results$z_m, ncol = 20)) |&gt; add_surface()\n\n\n\n\n\n\n\n\n\nReferences\n\nGramacy, Robert B. 2020. Surrogates: Gaussian Process Modeling, Design and  Optimization for the Applied Sciences. Boca Raton, Florida: Chapman Hall/CRC. https://bookdown.org/rbg/surrogates/.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Modelling two variables using GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html",
    "href": "gaussian_processes/multioutput/index.html",
    "title": "Multioutput Gaussian Process Regression (GPR)",
    "section": "",
    "text": "Elsewhere, I’ve discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times P}\\) is used to model \\(\\mathbf{y} \\in \\mathbb{R}^N\\): \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{X}) + \\mathbf{\\epsilon} \\text{ where } \\\\\nf(\\mathbf{X}) &\\sim \\mathcal{N}_N(\\mathbf{0}, \\mathbf{K}), \\\\\n\\mathbf{\\epsilon} &\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_y), \\text{ and } \\\\\n\\mathbf{K} &= [k(x_i, x_j)]_{ij}^N + \\sigma^2_yI_N.\n\\end{align*}\n\\]\n\n\nThroughout this page, I’ll be using the subscript after \\(\\mathcal{N}\\) to indicate we’re looking at a multivariate normal distribution, and to show the dimension of the vectors that the distribution produces.\nWhile interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we’ll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the gcookbook package:\n\n\nThis example and dataset is drawn from The R Graphics Cookbook (Chang 2018), specifically chapter 13.12, on creating vector fields.\n\nThis data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.\n\nBelow I’ve plotted the x and y components of the storm’s velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm’s winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector: \\[\n\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}.\n\\]\nVelocity captures both the direction of the wind, and its magnitude (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we’re looking at, such as the storm’s spiral shape.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\n\n# Keep a subset in the middle of the the z-axis\nd_isabel &lt;- isabel |&gt;\n  filter(z == 10.035) |&gt;\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n &lt;- function(x, by = 4) {\n  x &lt;- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x &lt;- every_n(unique(isabel$x))\nkeep_y &lt;- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd &lt;- d_isabel |&gt;\n  filter(x %in% keep_x, y %in% keep_y) |&gt;\n  mutate(index = 1:n())\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 &lt;- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0\n\n\n\n\n\n\n\n\n\nA similar consideration applies to the task of predicting velocity using statistical models. A “naive” approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, \\(f_1: \\mathbb{R}^2 \\to \\mathbb{R}\\) and \\(f_2: \\mathbb{R}^2 \\to \\mathbb{R}\\). This amounts to treating \\(v_x\\) and \\(v_y\\) as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.\nAn improved approach would allow us to incorporate natural information on how our observed target values (each \\(v_x\\) and \\(v_y\\)) might interrelate. The approach should also (ideally) account for the fact that we’re trying to predict a vector at a specific point. In other words, we’d like a single function, whose outputs are vector-valued:\n\\[\n\\underbrace{\\begin{aligned}[c]\nv_x = f_1\\Bigl((x, y)^\\top \\Bigr) + \\epsilon \\\\\nv_y = f_2\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Naive approach}}\n\\qquad\\longrightarrow\\qquad\n\\underbrace{\\begin{aligned}[c]\n\\begin{bmatrix}v_x \\\\ v_y\\end{bmatrix} = f\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Vector-valued approach}}\n\\]\nThese characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as “multioutput”, “multitask”, “cokriging”, or “vector-valued” GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics (Alvarez et al. 2012). In the following section, we’ll cover the theory behind the approach as it applies to our example data.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput Gaussian Process Regression (GPR)"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#motivation-visualizing-vector-fields",
    "href": "gaussian_processes/multioutput/index.html#motivation-visualizing-vector-fields",
    "title": "Multioutput Gaussian Process Regression (GPR)",
    "section": "",
    "text": "Elsewhere, I’ve discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times P}\\) is used to model \\(\\mathbf{y} \\in \\mathbb{R}^N\\): \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{X}) + \\mathbf{\\epsilon} \\text{ where } \\\\\nf(\\mathbf{X}) &\\sim \\mathcal{N}_N(\\mathbf{0}, \\mathbf{K}), \\\\\n\\mathbf{\\epsilon} &\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_y), \\text{ and } \\\\\n\\mathbf{K} &= [k(x_i, x_j)]_{ij}^N + \\sigma^2_yI_N.\n\\end{align*}\n\\]\n\n\nThroughout this page, I’ll be using the subscript after \\(\\mathcal{N}\\) to indicate we’re looking at a multivariate normal distribution, and to show the dimension of the vectors that the distribution produces.\nWhile interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we’ll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the gcookbook package:\n\n\nThis example and dataset is drawn from The R Graphics Cookbook (Chang 2018), specifically chapter 13.12, on creating vector fields.\n\nThis data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.\n\nBelow I’ve plotted the x and y components of the storm’s velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm’s winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector: \\[\n\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}.\n\\]\nVelocity captures both the direction of the wind, and its magnitude (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we’re looking at, such as the storm’s spiral shape.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\n\n# Keep a subset in the middle of the the z-axis\nd_isabel &lt;- isabel |&gt;\n  filter(z == 10.035) |&gt;\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n &lt;- function(x, by = 4) {\n  x &lt;- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x &lt;- every_n(unique(isabel$x))\nkeep_y &lt;- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd &lt;- d_isabel |&gt;\n  filter(x %in% keep_x, y %in% keep_y) |&gt;\n  mutate(index = 1:n())\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 &lt;- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0\n\n\n\n\n\n\n\n\n\nA similar consideration applies to the task of predicting velocity using statistical models. A “naive” approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, \\(f_1: \\mathbb{R}^2 \\to \\mathbb{R}\\) and \\(f_2: \\mathbb{R}^2 \\to \\mathbb{R}\\). This amounts to treating \\(v_x\\) and \\(v_y\\) as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.\nAn improved approach would allow us to incorporate natural information on how our observed target values (each \\(v_x\\) and \\(v_y\\)) might interrelate. The approach should also (ideally) account for the fact that we’re trying to predict a vector at a specific point. In other words, we’d like a single function, whose outputs are vector-valued:\n\\[\n\\underbrace{\\begin{aligned}[c]\nv_x = f_1\\Bigl((x, y)^\\top \\Bigr) + \\epsilon \\\\\nv_y = f_2\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Naive approach}}\n\\qquad\\longrightarrow\\qquad\n\\underbrace{\\begin{aligned}[c]\n\\begin{bmatrix}v_x \\\\ v_y\\end{bmatrix} = f\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Vector-valued approach}}\n\\]\nThese characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as “multioutput”, “multitask”, “cokriging”, or “vector-valued” GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics (Alvarez et al. 2012). In the following section, we’ll cover the theory behind the approach as it applies to our example data.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput Gaussian Process Regression (GPR)"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#theory-and-notation",
    "href": "gaussian_processes/multioutput/index.html#theory-and-notation",
    "title": "Multioutput Gaussian Process Regression (GPR)",
    "section": "Theory and notation",
    "text": "Theory and notation\nWe are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times 2}\\), and we may refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N}\\) and \\(\\mathbf{x} = \\text{vec}\\ X \\in \\mathbb{R}^{2N}\\). We will use \\(\\mathbf{X}_* \\in \\mathbb{R}^{M \\times 2}\\) to denote a set of test points, for which we want to generate predictions.\nWe are engaged in a regression task, i.e., we’re attempting to learn the functional relationship \\(f\\) between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), with an assumption that this relationship has been corrupted to some degree by noise. For the following, I’ve adapted notation from Chapter 2.2 in the well-known Williams and Rasmussen text to the context of our multidimensional \\(\\mathbf{Y}\\) (Williams and Rasmussen 2006). Setting up the problem, we have \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\\\\nf &\\sim \\mathcal{GP}(m, k) \\\\\nf(\\mathbf{x}) &\\sim \\mathcal{N}_{2N}(\\mathbf{0}, \\mathbf{K}_{XX})\n\\end{align*}\n\\tag{1}\\]\nTo parse these statements, we assume \\(f\\) is drawn from a Gaussian Process with mean function \\(m\\) and covariance (kernel) function \\(k\\). Thus, our observations \\(\\mathbf{y} = f(\\mathbf{x})\\) have a multivariate normal probability distribution, parameterized by a covariance matrix \\(\\mathbf{K}_{XX}\\). By convention, we assume the mean vector is \\(\\mathbf{0}\\) (implying that \\(m\\) is the zero function).\nThe joint distribution of \\(\\mathbf{y}\\) and our predictions for the test point(s) \\(\\mathbf{y}_*\\) is also described by a multivariate normal distribution, specified as: \\[\n\\begin{align*}\n\\begin{bmatrix}\n  \\mathbf{y} \\\\\n  \\mathbf{y_*}\n\\end{bmatrix} &\\sim \\mathcal{N}_{2N + 2M}\\Biggl( \\mathbf{0}, \\begin{bmatrix}\n  \\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N & \\mathbf{K}_{XX_*} \\\\\n  \\mathbf{K}_{X_*X} & \\mathbf{K}_{X_* X_*}\n\\end{bmatrix} \\Biggr) \\\\\n\\mathbf{V} &= \\begin{bmatrix} \\sigma^2_1 & 0 \\\\ 0 & \\sigma^2_2 \\end{bmatrix}\n\\end{align*}\n\\tag{2}\\]\nHere, \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the independent and additive noise associated with each of our outcome’s components. The \\(\\otimes\\) symbol denotes the Kronecker product. Each of the sub-matrices, such as \\(\\mathbf{K}_{XX_*}\\), are defined below in Equation 8. From this joint distribution, we can derive the conditional distribution for \\(\\mathbf{y}_*\\):\n\\[\n\\begin{align*}\n\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*} &\\sim \\mathcal{N}_{2M}(\\hat{\\mu},\\ \\hat{\\mathbf{\\Sigma}}) \\\\\n\\hat{\\mathbf{\\mu}} &= \\mathbb{E}[\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*}] = \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1} \\mathbf{y} \\\\\n\\hat{\\mathbf{\\Sigma}} &= \\mathbf{K}_{X_*X_*} - \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1}\\mathbf{K}_{XX_*} \\\\\n\\end{align*}\n\\tag{3}\\]\nWe will now work through how each piece of our distribution is defined. Let \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be a (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(-\\frac{1}{2\\rho^2} \\| x_i - x_j \\|^2 \\Bigl),\n\\tag{4}\\]\nwhere \\(\\alpha, \\rho &gt; 0\\), and \\(\\| \\cdot \\|\\) is the Euclidean (L2) norm. This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let \\(k(\\mathbf{X}, \\mathbf{X}) \\in \\mathbb{R}^{N \\times N}\\), the covariance matrix between all points in \\(\\mathbf{X}\\), be defined as \\[\nk(\\mathbf{X}, \\mathbf{X}) = (k(x_i, x_j))_{i,j}^N = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\tag{5}\\]\nLet \\(\\mathbf{B} \\in \\mathbb{R}^{2 \\times 2}\\), the matrix of similarities between the outputs \\(\\mathbf{Y}\\), be defined as \\[\n\\mathbf{B} = \\text{corr}(\\mathbf{Y}) = \\Biggl( \\frac{\\langle \\mathbf{y}_i - \\bar{\\mathbf{y}}_i,\\ \\mathbf{y}_j - \\bar{\\mathbf{y}}_j \\rangle}{\\| \\mathbf{y}_i - \\bar{\\mathbf{y}}_i \\| \\| \\mathbf{y}_j - \\bar{\\mathbf{y}}_j \\| } \\Biggr)^{2,2}_{i,j},\n\\tag{6}\\]\nwhere \\(\\mathbf{y}_i\\) is the i-th column of \\(\\mathbf{Y}\\) and \\(\\bar{\\mathbf{y}}_i\\) is the arithmetic mean of \\(\\mathbf{y}_i\\). The operation in the numerator, \\(\\langle \\cdot, \\cdot \\rangle\\), denotes the standard inner product. Each entry \\(b_{i,j}\\) of \\(\\mathbf{B}\\) is the Pearson correlation coefficient, as estimated from our training data.\nLet \\(\\mathbf{K}_{XX} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{K}_{XX} = \\mathbf{B} \\otimes k(\\mathbf{X}, \\mathbf{X}).\n\\tag{7}\\]\nAlvarez et al. identifies this approach of combining the kernel matrix with a similarity matrix via a Kronecker product as the “Intrinsic Coregionalization Model”; see equation (21) in section 4.2.2 (Alvarez et al. 2012). They describe this approach as being more restrictive, but simpler (to implement, I assume).\nWe can now define the other pieces needed to establish the covariance matrix used for the joint distribution of \\(\\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{y}_* \\end{bmatrix}\\):\n\\[\n\\begin{align*}\n  \\mathbf{K}_{X_*X_*} &= \\mathbf{B} \\otimes k(\\mathbf{X}_*, \\mathbf{X}_*) = \\mathbf{B} \\otimes (k(x_{*i}, x_{*j}))_{i,j=1}^{M} \\in \\mathbb{R}^{2M \\times 2M} \\\\\n  \\mathbf{K}_{X_*X} &= \\mathbf{B} \\otimes k(\\mathbf{X}_*, \\mathbf{X}) = \\mathbf{B} \\otimes (k(x_{*i}, x_j))_{i,j=1}^{M,N} \\in \\mathbb{R}^{2M \\times 2N} \\\\\n  \\mathbf{K}_{XX_*} &= \\mathbf{K}_{X_*X}^\\top \\in \\mathbb{R}^{2N \\times 2M}.\n\\end{align*}\n\\tag{8}\\]",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput Gaussian Process Regression (GPR)"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#training-and-test-data",
    "href": "gaussian_processes/multioutput/index.html#training-and-test-data",
    "title": "Multioutput Gaussian Process Regression (GPR)",
    "section": "Training and test data",
    "text": "Training and test data\nHere, using the Hurricane Isabel data visualized above, we generate \\(S\\), defined as a 30% random sample of observations. Here d is our subset of the isabel dataset, at z = 10.035.\n\n# Set up the training data\nS &lt;- d |&gt;\n  slice_sample(n = floor(0.3 * nrow(d))) |&gt;\n  select(x, y, vx, vy, index)\n\nX &lt;- S |&gt; select(x, y) |&gt; as.matrix()\nY &lt;- S |&gt; select(vx, vy) |&gt; as.matrix()\ny &lt;- as.vector(Y)\nB &lt;- cor(Y)\nN &lt;- nrow(X)\n\n# Pull out test cases\nZ &lt;- d |&gt;\n  anti_join(S, by = \"index\") |&gt;\n  select(x, y, vx, vy, index)\n\nX_star &lt;- Z |&gt; select(x, y) |&gt; as.matrix()",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput Gaussian Process Regression (GPR)"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#kernel-functions",
    "href": "gaussian_processes/multioutput/index.html#kernel-functions",
    "title": "Multioutput Gaussian Process Regression (GPR)",
    "section": "Kernel functions",
    "text": "Kernel functions\nThis section defines R functions used to compute \\(\\mathbf{K}_{XX}\\).\n\n\nCode\n#' Squared exponential (scalar) kernel function\n#' \n#' @param xi numeric vector\n#' @param xj numeric vector\n#' @param alpha scale, alpha &gt; 0\n#' @param rho length-scale, rho &gt; 0\n#' @return numeric\nk &lt;- function(xi, xj, alpha = 1, rho = 1) {\n  alpha^2 * exp(-norm(xi - xj, type = \"2\")^2 / (2 * rho^2))\n}\n\n#' Compute the kernel matrix, given a set of input vectors\n#' \n#' @param X numeric matrix of dimensions N x P, where N is observations and P is components\n#' @param alpha numeric, variance scale, alpha &gt; 0\n#' @param rho numeric, length-scale, rho &gt; 0\n#' @param err numeric, err &gt; 0\n#' @return N x N matrix\nk_XX &lt;- function(X, alpha = 1, rho = 1, err = eps) {\n  N &lt;- nrow(X)\n  K &lt;- matrix(0, N, N)\n  for (i in 1:N) {\n    for (j in 1:N) {\n      K[i, j] &lt;- k(X[i, ], X[j, ], alpha, rho)\n    }\n  }\n\n  if (!is.null(err)) {\n    K &lt;- K + diag(err, ncol(K))\n  }\n\n  K\n}\n\n#' Compute the covariance between two sets of vectors\n#'\n#' @param x numeric matrix, N x P\n#' @param X numeric matrix, M x P\n#' @param alpha numeric, variance scale, alpha &gt; 0\n#' @param rho numeric, length-scale, rho &gt; 0\n#' @return A numeric matrix, N x M\nk_xX &lt;- function(x, X, alpha = 1, rho = 1) {\n  N &lt;- nrow(x)\n  M &lt;- nrow(X)\n\n  K &lt;- matrix(0, N, M)\n  for (i in 1:N) {\n    for (j in 1:M) {\n      K[i, j] &lt;- k(x[i, ], X[j, ], alpha, rho)\n    }\n  }\n\n  K\n}",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput Gaussian Process Regression (GPR)"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#hyperparameter-inference",
    "href": "gaussian_processes/multioutput/index.html#hyperparameter-inference",
    "title": "Multioutput Gaussian Process Regression (GPR)",
    "section": "Hyperparameter Inference",
    "text": "Hyperparameter Inference\n\nEstimation via Stan\nThis section contains the rstan code used to estimate hyperparameter values using the probabilistic programming language, Stan. Within the scoped blocks of the Stan program we establish the model for \\(f(x)\\) discussed in Equation 1, and attempt to find plausible values for \\(\\rho\\) and \\(\\alpha\\) via Hamiltonian Monte-Carlo (HMC). We place prior distributions on each hyperparameter. Constraints are used within Stan to require these parameters be &gt; 0.\n\n\nCode\nlibrary(rstan)\n\nfit_stan &lt;- function(filepath, model_name = \"\", data, verbose = FALSE) {\n  stan(\n    file = filepath,\n    model_name = model_name,\n    data = data,\n    chains = 2,\n    iter = 800,\n    warmup = 400,\n    seed = 123,\n    cores = 2,\n    verbose = verbose\n  )\n}\n\n# ICM\n# fit_stan(\"./icm.stan\", \"Isabel GPR, ICM: rho\", lst(N, X, Y, B))\n# fit_stan(\"./icm_w_alpha.stan\", \"Isabel GPR, ICM: alpha & rho\", lst(N, X, Y, B))\n# fit_stan(\"./icm_w_alpha_and_sigma.stan\", \"Isabel GPR, ICM: alpha, rho, & sigma\", lst(N, X, Y, B))\n\n# Independent GPs\n# fit_stan(\"./icm.stan\", \"Isabel GPR, Indep: rho\", lst(N, X, Y, B = diag(1, 2)))\n# fit_stan(\"./icm_w_alpha.stan\", \"Isabel GPR, Indep: alpha & rho\", lst(N, X, Y, B = diag(1, 2)))\nfit_stan(\"./icm_w_alpha_and_sigma.stan\", \"Isabel GPR, Indep: alpha, rho, & sigma\", lst(N, X, Y, B = diag(1, 2)))\n\n\n\n\nEvaluation\nThis section defines R functions used to compute the analytical posterior distribution for each parameter specification, and summarize the results with a figure and performance with respect to RMSE.\n\n\nCode\nget_posterior &lt;- function(X, y, B, Z, alpha = 1, rho = 1, sigma = c(1e-9, 1e-9)) {\n  N &lt;- nrow(X)\n  M &lt;- nrow(Z)\n\n  V &lt;- diag(sigma^2, 2)\n  K_XX &lt;- kronecker(B, k_XX(X, alpha, rho, NULL)) + kronecker(V, diag(N))\n  K_ZX &lt;- kronecker(B, k_xX(Z, X, alpha, rho))\n  K_ZZ &lt;- kronecker(B, k_XX(Z, alpha, rho, NULL))\n\n  mu &lt;- K_ZX %*% solve(K_XX) %*% y\n  mu &lt;- matrix(mu, M, 2)\n  Sigma &lt;- K_ZZ - K_ZX %*% solve(K_XX) %*% t(K_ZX)\n\n  return(lst(mu, Sigma))\n}\n\nrmse &lt;- function(y, yhat) {\n  sqrt(1/length(y) * sum((yhat - y)^2))\n}\n\nget_evaluation &lt;- function(mu, Z, S) {\n  out &lt;- Z |&gt;\n    bind_cols(as_tibble(mu) |&gt; set_names(c(\"vx_h\", \"vy_h\"))) |&gt;\n    select(x, y, vx, vy, vx_h, vy_h, index) |&gt;\n    bind_rows(S) |&gt;\n    mutate(\n      col = factor(\n        as.numeric(index %in% S$index),\n        levels = 0:1,\n        labels = c(\"Test\", \"Train\")\n      )\n    )\n\n  fig &lt;- ggplot() +\n    geom_segment(\n      data = out,\n      aes(x, y, xend = x + vx/50, yend = y + vy/50),\n      arrow = arrow(length = unit(0.1, \"cm\")),\n      linewidth = 0.25,\n      alpha = 0.3\n    ) +\n    geom_segment(\n      data = filter(out, col == \"Train\"),\n      aes(x, y, xend = x + vx/50, yend = y + vy/50, color = col),\n      arrow = arrow(length = unit(0.1, \"cm\")),\n      linewidth = 0.25\n    ) +\n    geom_segment(\n      data = filter(out, col == \"Test\"),\n      aes(x, y, xend = x + vx_h/50, yend = y + vy_h/50, color = col),\n      arrow = arrow(length = unit(0.1, \"cm\")),\n      linewidth = 0.25\n    ) +\n    scale_color_scico_d(palette = \"imola\", name = \"\", direction = -1) +\n    labs(x = \"Longitude\", y = \"Latitude\") +\n    theme(legend.position = \"bottom\")\n\n  perf &lt;- out |&gt;\n    filter(col == \"Test\") |&gt;\n    summarize(\n      rmse_vx = rmse(vx, vx_h),\n      rmse_vy = rmse(vy, vy_h)\n    )\n\n  return(lst(fig, perf))\n}\n\n\n\n\nICM\n\nNaive ParametersInference on \\(\\rho\\)Inference on \\(\\alpha\\) and \\(\\rho\\)Inference on \\(\\alpha\\), \\(\\rho\\), and \\(\\sigma\\)\n\n\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n1.00\n1.00\n1e-9\n1e-9\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n2.718579\n3.171989\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n1.00\n0.84\n1e-9\n1e-9\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n2.732739\n3.199853\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Gaussian Process Regression for bivariate input and output data, using the\n// Intrinsic Coregionalization Model (ICM).\n// This program performs inference on rho, the length-scale hyperparameter for the\n// exponentiated quadratic kernel function.\n\nfunctions {\n  // Performs the Kronecker product B otimes A, where B \\in R^{2 x 2}\n  matrix kronecker_prod(matrix A, matrix B, int N, int N2) {\n    matrix[N2, N2] K;\n    K[1:N, 1:N] = B[1, 1] * A;\n    K[1:N, (N+1):N2] = B[1, 2] * A;\n    K[(N+1):N2, 1:N] = B[2, 1] * A;\n    K[(N+1):N2, (N+1):N2] = B[2, 2] * A;\n    return K;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; N;  // Total observations\n  matrix[N, 2] X;  // Inputs\n  matrix[N, 2] Y;  // Targets\n  matrix[2, 2] B;  // Coregionalization matrix\n}\n\ntransformed data {\n  real delta = 1e-9;\n  int N2 = N*2;\n  matrix[N, N] I_N = identity_matrix(N);\n  vector[N2] y = to_vector(Y);\n  vector[N2] mu = rep_vector(0, N2);\n\n  array[N] vector[2] x;\n  for (n in 1:N) {\n    x[n] = to_vector(row(X, n));\n  }\n}\n\nparameters {\n  real&lt;lower=0&gt; rho;  // Length-scale\n}\n\nmodel {\n  rho ~ inv_gamma(5, 5);\n\n  matrix[N, N] k_XX = gp_exp_quad_cov(x, 1, rho);\n  matrix[N2, N2] K = kronecker_prod(k_XX, B, N, N2);\n  for (n in 1:N2) {\n    K[n, n] += delta;\n  }\n  matrix[N2, N2] L_K = cholesky_decompose(K);\n\n  y ~ multi_normal_cholesky(mu, L_K);\n}\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n8.34\n0.98\n1e-9\n1e-9\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n2.687784\n3.119549\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Gaussian Process Regression for bivariate input and output data, using the\n// Intrinsic Coregionalization Model (ICM).\n// This program performs inference on alpha and rho, hyperparameters for the\n// exponentiated quadratic kernel function.\n\nfunctions {\n  // Performs the Kronecker product B otimes A, where B \\in R^{2 x 2}\n  matrix kronecker_prod(matrix A, matrix B, int N, int N2) {\n    matrix[N2, N2] K;\n    K[1:N, 1:N] = B[1, 1] * A;\n    K[1:N, (N+1):N2] = B[1, 2] * A;\n    K[(N+1):N2, 1:N] = B[2, 1] * A;\n    K[(N+1):N2, (N+1):N2] = B[2, 2] * A;\n    return K;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; N;  // Total observations\n  matrix[N, 2] X;  // Inputs\n  matrix[N, 2] Y;  // Targets\n  matrix[2, 2] B;  // Coregionalization matrix\n}\n\ntransformed data {\n  real delta = 1e-9;\n  int N2 = N*2;\n  vector[N2] y = to_vector(Y);\n  vector[N2] mu = rep_vector(0, N2);\n\n  array[N] vector[2] x;\n  for (n in 1:N) {\n    x[n] = to_vector(row(X, n));\n  }\n}\n\nparameters {\n  real&lt;lower=0&gt; alpha;  // Marginal standard-deviation (magnitude of the function's range)\n  real&lt;lower=0&gt; rho;    // Length-scale\n}\n\nmodel {\n  alpha ~ std_normal();\n  rho ~ inv_gamma(5, 5);\n\n  matrix[N, N] k_XX = gp_exp_quad_cov(x, alpha, rho);\n  matrix[N2, N2] K = kronecker_prod(k_XX, B, N, N2);\n  for (n in 1:N2) {\n    K[n, n] += delta;\n  }\n  matrix[N2, N2] L_K = cholesky_decompose(K);\n\n  y ~ multi_normal_cholesky(mu, L_K);\n}\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n8.34\n0.98\n6.95\n9.58\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n8.208218\n12.03744\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Gaussian Process Regression for bivariate input and output data, using the\n// Intrinsic Coregionalization Model (ICM).\n// This program performs inference on alpha, rho, and sigma-- hyperparameters for the\n// exponentiated quadratic kernel function.\n\nfunctions {\n  // Performs the Kronecker product B otimes A, where B \\in R^{2 x 2}\n  matrix kronecker_prod(matrix A, matrix B, int N, int N2) {\n    matrix[N2, N2] K;\n    K[1:N, 1:N] = B[1, 1] * A;\n    K[1:N, (N+1):N2] = B[1, 2] * A;\n    K[(N+1):N2, 1:N] = B[2, 1] * A;\n    K[(N+1):N2, (N+1):N2] = B[2, 2] * A;\n    return K;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; N;  // Total observations\n  matrix[N, 2] X;  // Inputs\n  matrix[N, 2] Y;  // Targets\n  matrix[2, 2] B;  // Coregionalization matrix\n}\n\ntransformed data {\n  int N2 = N*2;\n  matrix[N, N] I_N = identity_matrix(N);\n  vector[N2] y = to_vector(Y);\n  vector[N2] mu = rep_vector(0, N2);\n\n  array[N] vector[2] x;\n  for (n in 1:N) {\n    x[n] = to_vector(row(X, n));\n  }\n}\n\nparameters {\n  real&lt;lower=0&gt; alpha;   // Marginal standard-deviation (magnitude of the function's range)\n  real&lt;lower=0&gt; rho;     // Length-scale\n  real&lt;lower=0&gt; sigma1;  // Scale of the noise-term for 1st component of Y\n  real&lt;lower=0&gt; sigma2;  // Scale of the noise-term for 2nd component of Y\n}\n\nmodel {\n  alpha ~ std_normal();\n  rho ~ inv_gamma(5, 5);\n  sigma1 ~ std_normal();\n  sigma2 ~ std_normal();\n\n  matrix[2, 2] V = identity_matrix(2);\n  V[1, 1] = square(sigma1);\n  V[2, 2] = square(sigma2);\n\n  matrix[N, N] k_XX = gp_exp_quad_cov(x, 1, rho);\n  matrix[N2, N2] K = kronecker_prod(k_XX, B, N, N2) + kronecker_prod(I_N, V, N, N2);\n  matrix[N2, N2] L_K = cholesky_decompose(K);\n\n  y ~ multi_normal_cholesky(mu, L_K);\n}\n\n\n\n\n\nIndependent GPs\n\nNaive ParametersInference on \\(\\rho\\)Inference on \\(\\alpha\\) and \\(\\rho\\)Inference on \\(\\alpha\\), \\(\\rho\\), and \\(\\sigma\\)\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n1.00\n1.00\n1e-9\n1e-9\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n2.718579\n3.171989\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n1.00\n0.85\n1e-9\n1e-9\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n2.713324\n3.165199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n8.12\n0.99\n1e-9\n1e-9\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n2.702115\n3.144015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\)\n\\(\\rho\\)\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n8.12\n0.99\n6.99\n9.68\n\n\n\n\n\n\n\n\nrmse_vx\nrmse_vy\n\n\n\n\n3.232751\n5.687826",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput Gaussian Process Regression (GPR)"
    ]
  },
  {
    "objectID": "gaussian_processes/index.html",
    "href": "gaussian_processes/index.html",
    "title": "Overview of Gaussian Processes",
    "section": "",
    "text": "Gaussian Processes (often abbreviated as “GP”s) are stochastic (random) processes that are commonly used to model nonlinear functions. The authors of a popular textbook covering GPs and GP regression (Williams and Rasmussen 2006, pg. 2) provide the following informal definition:\n\nA Gaussian process is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions. Leaving mathematical sophistication aside, one can loosely think of a function as a very long vector, each entry in the vector specifying the function value \\(f(x)\\) at a particular input \\(x\\). It turns out, that although this idea is a little naïve, it is surprisingly close what we need.\n\nThis description helpfully orients us to two key aspects. First is that GPs are often used to specify probability distributions over functions. Second, it cues us to think about collections of function evaluations as (jointly distributed) vectors. Below we’ll discuss a more formal definition.",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/index.html#introduction",
    "href": "gaussian_processes/index.html#introduction",
    "title": "Overview of Gaussian Processes",
    "section": "",
    "text": "Gaussian Processes (often abbreviated as “GP”s) are stochastic (random) processes that are commonly used to model nonlinear functions. The authors of a popular textbook covering GPs and GP regression (Williams and Rasmussen 2006, pg. 2) provide the following informal definition:\n\nA Gaussian process is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions. Leaving mathematical sophistication aside, one can loosely think of a function as a very long vector, each entry in the vector specifying the function value \\(f(x)\\) at a particular input \\(x\\). It turns out, that although this idea is a little naïve, it is surprisingly close what we need.\n\nThis description helpfully orients us to two key aspects. First is that GPs are often used to specify probability distributions over functions. Second, it cues us to think about collections of function evaluations as (jointly distributed) vectors. Below we’ll discuss a more formal definition.",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/index.html#definition",
    "href": "gaussian_processes/index.html#definition",
    "title": "Overview of Gaussian Processes",
    "section": "Definition",
    "text": "Definition\nA Gaussian Process is an uncountably infinite collection of random variables, with any finite sample from the process sharing a joint multivariate Gaussian distribution. If we assume a function \\(f\\) comes from a Gaussian Process, we write \\(f \\sim \\mathcal{GP}(m, k)\\). The functions \\(m\\) and \\(k\\) are referred to as mean and covariance functions, respectively. The mean function is often set to be the zero function \\(\\mathbf{0}\\) for notational convenience. The covariance (kernel) function must produce a positive definite matrix when evaluated on a collection of inputs. For my project, I’ve focused on the squared-exponential function. For the following examples, we’ll define it using its scalar form \\(k: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\): \\[\nk(x, x') = \\alpha^2 \\cdot \\exp \\Bigl( -\\frac{1}{2\\rho^2}(x - x')^2 \\Bigr), \\text{ where } \\alpha, \\rho &gt; 0.\n\\]\nNote that the values of \\(k\\) will decrease exponentially as the inputs (\\(x\\) and \\(x'\\)) get farther apart. This is a desirable property for modeling covariance, as we’d generally expect two nearby inputs to be similar to each other and two distant values to be less similar. The range of the functions drawn from the GP is controlled via \\(\\alpha\\), while the frequency (“wiggliness”) of the functions is controlled by the length-scale \\(\\rho\\). Together, we refer to \\(\\alpha\\) and \\(\\rho\\) as hyperparameters. They can be estimated from sample data using statistical inference, or chosen by the researcher based on expertise or problem context.\nAs an aside, the squared-exponential kernel is a common choice as a covariance function, but any function that produces a positive (semi)definite matrix can be used. David Duvenaud’s website and thesis chapter includes descriptions of various kernel functions that can be used for modeling different types of data (Duvenaud 2014).",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/index.html#gaussian-process-regression",
    "href": "gaussian_processes/index.html#gaussian-process-regression",
    "title": "Overview of Gaussian Processes",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\nIn practice, Gaussian Processes are often brought to bear on regression problems, in which a researcher has collected a dataset \\(S = (\\mathbf{x}, \\mathbf{y}) = \\{ (x_i, y_i) : x_i,y_i \\in \\mathbb{R}, i \\in 1, 2, \\dots, N \\}\\) with the goal of learning the relationship \\(f\\) between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\):\n\\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) \\ \\text{ or} \\\\\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\ \\text{ (with additive noise)}.\n\\end{align*}\n\\]\nOnce \\(f\\) has been estimated, it can then be used to predict the values of future or test points, which we’ll refer to as \\(\\mathbf{x}_* \\in \\mathbb{R}^M\\).\nGaussian Process Regression can be considered a Bayesian method for learning \\(f\\). We’ll consider the plots below to as an example with simulated data.\n\nPrior DistributionTraining DataPosterior Distribution\n\n\nIn orange, we have a function, unknown to our imagined researcher. In grey we have 10 draws from \\(f \\sim \\mathcal{GP}(\\mathbf{0}, k(\\mathbf{x}_*, \\mathbf{x}_*))\\). This represents what the analyst knows before incorporating information from \\(S\\). As we can see, the draws span the domain we’re interested in, but their y-values are chaotic and don’t align with the target.\n\n\nCode\nlibrary(tidyverse)\nset.seed(123)\ntheme_set(theme_minimal(base_size = 15) + theme(panel.grid.minor = element_blank()))\n\nf &lt;- function(x) sin(2*x) + sin(4*x)\nk &lt;- function(x_i, x_j, alpha, rho) alpha^2 * exp(-(x_i - x_j)^2 / (2 * rho^2))\n\nk_xX &lt;- function(x, X, alpha = 1, rho = 0.4) {\n  N &lt;- NROW(x)\n  M &lt;- NROW(X)\n  K &lt;- matrix(0, N, M)\n  for (n in 1:N) {\n    for (m in 1:M) {\n      K[n, m] &lt;- k(x[n], X[m], alpha, rho)\n    }\n  }\n  return(K)\n}\n\nN &lt;- 400\nn &lt;- 30\nx &lt;- seq(-5, 5, length.out = N)\nf_x &lt;- f(x)\n\nD &lt;- tibble(x, y = f_x)\nS &lt;- slice_sample(D, n = n)\nZ &lt;- anti_join(D, S, by = \"x\")\n\nK_xx &lt;- k_xX(x, x)\n\nggplot() +\n  geom_line(data = D, aes(x, y), color = \"orange\") +\n  geom_line(\n    data = map(\n      1:10, \\(i) {\n        tibble(x, y = MASS::mvrnorm(1, mu = rep.int(0, N), Sigma = K_xx))\n    }) |&gt; \n      bind_rows(.id = \"b\"),\n    aes(x, y, group = b), alpha = 0.2\n  )\n\n\n\n\n\n\n\n\n\n\n\nAgain we have our unknown target function in orange. The black dots comprise \\(S\\), the researcher’s dataset (here, N = 30).\n\n\nCode\nggplot() +\n  geom_line(data = D, aes(x, y), color = \"orange\") +\n  geom_point(data = S, aes(x, y))\n\n\n\n\n\n\n\n\n\n\n\nNow we can combine our prior distribution with the observed data to produce a posterior distribution. In essence, the result is a new distribution that “agrees” with the observed training data points. Gaussian Processes have a nice property in that the posterior distribution can be computed analytically1:\n\\[\n\\begin{align*}\n\\mathbf{y}_* | \\mathbf{x}, \\mathbf{y}, \\mathbf{x}_* &\\sim \\mathcal{N}_M(\\hat{\\mu}, \\hat{\\Sigma}) \\\\\n\\hat{\\mu} &= k(\\mathbf{x}_*, \\mathbf{x})(k(\\mathbf{x}, \\mathbf{x}))^{-1}\\mathbf{y} \\\\\n\\hat{\\Sigma} &= k(\\mathbf{x}_*, \\mathbf{x}_*) - k(\\mathbf{x}_*, \\mathbf{x})(k(\\mathbf{x}, \\mathbf{x}))^{-1}k(\\mathbf{x}_*, \\mathbf{x})^\\top \\\\\n\\text{ where } \\ \\ \\ & \\\\\nk(\\mathbf{x}, \\mathbf{x}) &= [k(x_i, x_j)]_{i,j}^N \\in \\mathbb{R}^{N \\times N} \\\\\nk(\\mathbf{x}_*, \\mathbf{x}) &= [k(x_{*i}, x_j)]_{i,j}^{M,N} \\in \\mathbb{R}^{M \\times N} \\\\\nk(\\mathbf{x}_*, \\mathbf{x}_*) &= [k(x_{*i}, x_{*j})]_{i,j}^M \\in \\mathbb{R}^{M \\times M}.\n\\end{align*}\n\\]\nAfter computing \\(\\hat{\\mu}\\) and \\(\\hat{\\Sigma}\\), we can draw samples from the multivariate normal distribution and summarize them to produce estimates for each test point in \\(\\mathbf{x}_*\\). Below in blue is the posterior mean across our domain, with the grey shading representing \\(\\pm 2\\) standard deviations (i.e., a 95% credible interval).\n\n\nCode\neps &lt;- 0.009\nK_XX &lt;- k_xX(S$x, S$x)\nK_xX &lt;- k_xX(D$x, S$x)\n\n# Adding a small amount of noise to the diagonal for numerical stability\nmu &lt;- K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% S$y\nsigma &lt;- K_xx - K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% t(K_xX)\n\ndraws &lt;- MASS::mvrnorm(100, mu, sigma)\n\ndraws &lt;- as_tibble(t(draws)) |&gt;\n  mutate(x = D$x) |&gt;\n  pivot_longer(-x) |&gt;\n  group_by(x) |&gt;\n  summarise(y = mean(value), s = sd(value))\n\nggplot() +\n  geom_line(data = D, aes(x, y), color = \"orange\") +\n  geom_ribbon(data = draws, aes(x, ymin = y - 2*s, ymax = y + 2*s), alpha = 0.2) +\n  geom_line(data = draws, aes(x, y), color = \"blue\") +\n  geom_point(data = S, aes(x, y))",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/index.html#footnotes",
    "href": "gaussian_processes/index.html#footnotes",
    "title": "Overview of Gaussian Processes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe posterior distribution for this example assumes no additive noise in the target vector \\(\\mathbf{y}\\). If measurement error is believed to be present, the computations for \\(\\hat{\\mu}\\) and \\(\\hat{\\Sigma}\\) should replace \\(k(\\mathbf{x}, \\mathbf{x})\\) with \\(k(\\mathbf{x}, \\mathbf{x}) + \\sigma_y^2 I_N\\), in which \\(\\sigma_y\\) is an additional hyperparameter for the outcome’s standard deviation. \\(I_N\\) is the \\(N \\times N\\) identity matrix.↩︎",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html",
    "href": "gaussian_processes/mvnorm/index.html",
    "title": "The Multivariate Gaussian Distribution",
    "section": "",
    "text": "Let \\(\\mathbf{x} \\in \\mathbb{R}^N\\) be a vector whose elements, \\(X_i\\), are random variables (i.e., \\(\\mathbf{x}\\) is a random vector). As elements of \\(\\mathbf{x}\\), we say that \\(X_1, X_2, \\dots, X_N\\) are jointly distributed. Define the vector \\(\\mathbf{\\mu}\\) and matrix \\(\\mathbf{\\Sigma}\\) as \\[\n\\begin{align*}\n\\mathbf{\\mu} \\in \\mathbb{R}^N &= (\\mu_1, \\mu_2, \\dots, \\mu_N)^\\top = (\\mathbb{E}(X_1), \\mathbb{E}(X_2), \\dots, \\mathbb{E}(X_N))^\\top  \\\\\n\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times N} &= Cov(\\mathbf{x}) = \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] = \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^N\n\\end{align*}\n\\]\nand suppose \\(X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})\\). If every linear combination of \\(\\mathbf{x}\\)’s elements results in a univariate normal variable, we say that \\(\\mathbf{x}\\) comes from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), written as \\(\\mathbf{x} \\sim \\mathcal{N}_N (\\mathbf{\\mu}, \\mathbf{\\Sigma}).\\)",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#definition",
    "href": "gaussian_processes/mvnorm/index.html#definition",
    "title": "The Multivariate Gaussian Distribution",
    "section": "",
    "text": "Let \\(\\mathbf{x} \\in \\mathbb{R}^N\\) be a vector whose elements, \\(X_i\\), are random variables (i.e., \\(\\mathbf{x}\\) is a random vector). As elements of \\(\\mathbf{x}\\), we say that \\(X_1, X_2, \\dots, X_N\\) are jointly distributed. Define the vector \\(\\mathbf{\\mu}\\) and matrix \\(\\mathbf{\\Sigma}\\) as \\[\n\\begin{align*}\n\\mathbf{\\mu} \\in \\mathbb{R}^N &= (\\mu_1, \\mu_2, \\dots, \\mu_N)^\\top = (\\mathbb{E}(X_1), \\mathbb{E}(X_2), \\dots, \\mathbb{E}(X_N))^\\top  \\\\\n\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times N} &= Cov(\\mathbf{x}) = \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] = \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^N\n\\end{align*}\n\\]\nand suppose \\(X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})\\). If every linear combination of \\(\\mathbf{x}\\)’s elements results in a univariate normal variable, we say that \\(\\mathbf{x}\\) comes from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), written as \\(\\mathbf{x} \\sim \\mathcal{N}_N (\\mathbf{\\mu}, \\mathbf{\\Sigma}).\\)",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#sampling-algorithm",
    "href": "gaussian_processes/mvnorm/index.html#sampling-algorithm",
    "title": "The Multivariate Gaussian Distribution",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nA commonly used algorithm for sampling from a multivariate Gaussian distribution can be described as the following, given a vector \\(\\mathbf{\\mu} \\in \\mathbb{R}^N\\) and a positive-(semi)definite matrix \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times N}\\):\n\nCompute \\(L L^\\top = \\mathbf{\\Sigma}\\), the Cholesky decomposition of \\(\\mathbf{\\Sigma}\\).\nGenerate \\(\\mathbf{z} = (Z_1, Z_2, \\dots, Z_N)^\\top\\) where \\(Z_1, Z_2, \\dots, Z_N \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1)\\).\nCompute \\(\\mathbf{x} = L\\mathbf{z} + \\mathbf{\\mu}\\).",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#why-does-the-algorithm-work",
    "href": "gaussian_processes/mvnorm/index.html#why-does-the-algorithm-work",
    "title": "The Multivariate Gaussian Distribution",
    "section": "Why does the algorithm work?",
    "text": "Why does the algorithm work?\nThe claim is that \\(\\mathbf{x}\\) is a draw from a multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), i.e., \\(\\mathbf{x} \\sim \\mathcal{N}_N (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). So, why does this work? First, we can show that \\(\\mathbb{E}(\\mathbf{x}) = \\mathbf{\\mu}\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\mathbf{x}) &= \\mathbb{E}(L\\mathbf{z} + \\mathbf{\\mu}) \\\\\n  &= \\mathbb{E}(L\\mathbf{z}) + \\mathbb{E}(\\mathbf{\\mu}) \\\\\n  &= L \\cdot \\mathbb{E}(\\mathbf{z}) + \\mathbf{\\mu} \\\\\n  &= L \\cdot 0 + \\mathbf{\\mu} \\\\\n  &= \\mathbf{\\mu}.\n\\end{align*}\n\\]\nNext, we can see that \\(Cov(\\mathbf{x}) = \\mathbf{\\Sigma}\\):\n\n\nReminder that \\(Cov(\\mathbf{x})\\) is not a scalar, rather, it is the covariance matrix (or variance-covariance matrix) of \\(\\mathbf{x}\\).\n\\[\n\\begin{align*}\nCov(\\mathbf{x}) &= \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[(L\\mathbf{z} + \\mathbf{\\mu} - \\mathbf{\\mu})(L\\mathbf{z} + \\mathbf{\\mu} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[L\\mathbf{z}\\mathbf{z}^\\top L^\\top] \\\\\n  &= L\\ \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top]\\ L^\\top \\\\\n  &= L\\ \\mathbb{E}[(\\mathbf{z} - 0)(\\mathbf{z} - 0)^\\top]\\ L^\\top \\\\\n  &= L\\ Cov(\\mathbf{z})\\ L^\\top \\\\\n  &= L I L^\\top \\\\\n  &= L L^\\top \\\\\n  &= \\mathbf{\\Sigma}.\n\\end{align*}\n\\]\n\n\nThis proof hinges on the definition of covariance and the independence of the \\(Z_i\\)’s. Remember, \\(Cov(Z_i, Z_i) = Var(Z_i) = 1\\) and \\(Cov(Z_i, Z_j) = 0\\) when \\(i \\neq j\\) and \\(Z_i \\perp Z_j\\).\nThe following references ((https://math.stackexchange.com/users/10117/lepidopterist), n.d.), ((https://math.stackexchange.com/users/21550/hkbattousai), n.d.), (ibilion[at]purdue.edu 2022) (specifically this section) were helpful to me as I worked through understanding the algorithm and the proofs for the two results above.\nHaving shown that the expectation and (co)variance of \\(\\mathbf{x}\\) are what we’d assume, we need to show that \\(\\mathbf{x}\\) qualifies as multivariate normal. A random vector is multivariate normal if any linear combination of its elements is normally distributed. From its construction, we know \\(\\mathbf{x}\\) is a linear transformation of a standard normal vector, i.e., \\(\\mathbf{z}\\). We can decompose \\(\\mathbf{x}\\)’s \\(i\\)-th element, \\(X_i\\), into the following:\n\\[\n\\begin{align*}\n\\sigma_i &= \\sum_{j = 0}^n L_{i,j} \\\\\nX_i &= Z_i \\sigma_i + \\mu_i.\n\\end{align*}\n\\]\nWe can then show that \\(X_i \\sim \\mathcal{N}(\\sigma_i \\cdot 0 + \\mu_i, \\sigma_i^2) \\implies X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\). This follows from theorems regarding the transformation of random variables (Casella and Berger 2024). I’ll demonstrate it for an arbitrary \\(Z \\sim \\mathcal{N}(0, 1)\\), \\(\\mu\\), and \\(\\sigma &gt; 0\\) (omitting the subscript \\(i\\) for brevity). The probability density function (pdf) of the standard normal \\(f_Z(z)\\) is defined as \\[\nf_Z(z) = \\frac{1}{\\sqrt{2\\pi}}\\ e^{-z^2 / 2}.\n\\]\nDefine \\(X' = g(Z) = \\sigma \\cdot f_Z(Z)\\). Then, \\(g^{-1}(x) = \\frac{x}{\\sigma}\\), and \\(\\frac{d}{dx}g^{-1}(x) = \\frac{1}{\\sigma}\\). Thus, the probability density function \\(f_{X'}(x)\\) is\n\\[\n\\begin{align*}\nf_{X'}(x) &= f_Z(g^{-1}(x)) \\frac{d}{dx}g^{-1}(x) \\\\\n  &= f_Z \\Bigl(\\frac{x}{\\sigma} \\Bigr) \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\ e^{-(\\frac{x}{\\sigma})^2 \\cdot \\frac{1}{2}} \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{x^2}{2\\sigma^2}}.\n\\end{align*}\n\\]\nThis is the pdf for \\(\\mathcal{N}(0, \\sigma^2)\\). Now, we find \\(X = h(X') = f_{X'}(X') + \\mu\\), where \\(h^{-1}(x) = x - \\mu\\) and \\(\\frac{d}{dx}\\ h^{-1}(x) = 1\\). As with the above, we have \\[\nf_X(x) = f_{X'}(h^{-1}(x)) \\frac{d}{dx}\\ h^{-1}(x) = f_{X'}(x - \\mu) = \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}},\n\\]\nwhich is the classical definition of the normal distribution’s probability density function. This means that scaling a normal variable by a constant, or adding a constant to a normal variable results in another normal variable. So, each \\(X_i\\) is normal. Lastly, as one’s intuition might suspect, the sum of jointly distributed normal variables is also normal. I won’t go through the proof here, but some versions can be found on wikipedia.\n\n\nThus, we can conclude that a linear combination \\(Y = a_1 X_1 + \\cdots + a_n X_n\\) is a normal variable, meaning \\(\\mathbf{x}\\) is a draw from the multivariate Gaussian distribution.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#r-implementation",
    "href": "gaussian_processes/mvnorm/index.html#r-implementation",
    "title": "The Multivariate Gaussian Distribution",
    "section": "R Implementation",
    "text": "R Implementation\n\nset.seed(123)\nmu &lt;- c(1.1, 3, 8)\nSigma &lt;- matrix(c(1, 0.1, 0.3, 0.1, 1, 0.1, 0.3, 0.1, 1), 3, 3)\n\nmvrnorm &lt;- function(n, mu, Sigma) {\n  L &lt;- chol(Sigma) # this returns L', where LL' = Sigma\n  d &lt;- length(mu)\n\n  out &lt;- matrix(0, nrow = n, ncol = d)\n  for (i in 1:n) {\n    u &lt;- rnorm(d)\n    out[i, ] &lt;- t(t(L) %*% u + mu)\n  }\n  return(out)\n}\n\nmvrnorm(10, mu, Sigma)\n\n           [,1]     [,2]     [,3]\n [1,] 0.5395244 2.714929 9.298527\n [2,] 1.1705084 3.135691 9.661861\n [3,] 1.5609162 1.787372 7.395843\n [4,] 0.6543380 4.173380 8.294725\n [5,] 1.5007715 3.150205 7.599224\n [6,] 2.8869131 3.674046 6.700175\n [7,] 1.8013559 2.599714 7.161280\n [8,] 0.8820251 1.957341 7.169001\n [9,] 0.4749607 1.259257 8.490846\n[10,] 1.2533731 1.882905 9.158747",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "reflections/reflection_2.html",
    "href": "reflections/reflection_2.html",
    "title": "Reflection 2",
    "section": "",
    "text": "For this reflection, I met with my advisor to discuss my CV and career goals. As a second degree student, my circumstances are a bit different from others in the program. Specifically, I’ve already gained career experience that most students completing their first degree won’t have. With this in mind, we contemplated the future paths and opportunities my career might take.\nI have been consistently drawn towards work in the public sector. In my first job after completing my first degree, I worked within the University of Michigan’s Survey Research Center, which is responsible for collecting the monthly Survey of Consumer Attitudes (SCA) and the long-running Panel Study of Income Dynamics (PSID). My work within the center, centered on data collection, impressed on me how much our society depends on the regularity and consistency of information to operate. It also fed my existing interests related to survey design and statistical inference. Today, I currently work in an IT context which often provides fulfilling work involving system design and technical challenges. I value the skills I’ve cultivated in this setting, but it may not be work I want to do for the entirety of my career. In the long term, I hope that I can return to working to support (or conduct) scientific or survey work more directly, either at a statistical agency, or within a research center. It’s possible that my background qualifies me for some roles today, but my sense is that this progression likely involves further training at the graduate level.\nWe’re living in a time where the foundations needed to provide this training are endangered. The US government’s threats to student freedoms and safety and disruption of research funding across the country are extremely dire. It’s painful knowing that friends and colleagues across the country have to divert energy away from teaching and scholarly work to resist what’s happening. At minimum, it creates unhelpful distractions and uncertainty. Schools across the country have begun reducing admissions for graduate students, with some programs even rescinding acceptance offers. The targeting and harassment of international students will further chill desires to study in the US. Our institutions are not perfect, and access to them is uneven, but our society is not served by undermining them. I’m straying from the intended purpose of this reflection, but it’s hard to chart a course whose route flows through higher education, and not acknowledge what’s apparent.\nIn spite of our present context, I did follow through on the goals of this reflection. Employment at the federal level seems vulnerable and fraught in the near term; while this is a direction I’ve contemplated, I don’t think this direction is worth pursuing in the next 3-4 years. In lieu of this direction, I further examined graduate programs that I discussed with my advisor: a MS and PhD program at Boise State, and an online MS offered by Colorado State University. I’ll summarize each below. My focus on these options reflects the fact that my partner and I would generally prefer to stay in Boise for the short-term, if possible. While I’ve missed the windows to apply this year, I intend to be ready for the next cycle.\nApplying to enroll for graduate training at Boise State University is a practical choice, given I live where the university is located. Boise State offers a masters in mathematics, with statistics offered as an emphasis area. This program takes approximately 33 credits, which would take roughly 2 years to complete if pursued full-time. Alternatively, I could opt to apply to the doctoral program in computing, likely pursuing either the CMSE or Data Science emphasis. This program requires 60 credits, and is meant to be pursued full-time. Prospective students seeking institutional funding are required to submit their materials in early January, with mid-April being the final deadline.\nColorado State University has a well-regarded masters program in applied statistics. This program is offered online and asynchronously; it is meant to be a terminal degree for professionals interested in working as practicing statisticians. Based on my review of their requirements, completion of my BS this year will have ensured adequate preparation for graduate coursework. Further, the program’s tuition rate is the same for in-state and non-Colorado residents. The program is meant to be completed in 1 year for students enrolled full-time, and within 2 years for those studying part-time. Students begin their studies each summer, with applications opening in the preceding September.",
    "crumbs": [
      "Home",
      "Reflections",
      "Reflection 2"
    ]
  },
  {
    "objectID": "reflections/index.html",
    "href": "reflections/index.html",
    "title": "Reflections",
    "section": "",
    "text": "Here are links to the reflection assignments I completed during MATH 401.\n\nReflection 1\nReflection 2\n\nEssay\nCV\n\nReflection 3",
    "crumbs": [
      "Home",
      "Reflections"
    ]
  },
  {
    "objectID": "krr/index.html",
    "href": "krr/index.html",
    "title": "Kernel Ridge Regression",
    "section": "",
    "text": "library(tidyverse)\nset.seed(123)\n\nf &lt;- function(x) sin(3*x) + sin(7*x)\nk &lt;- function(x_i, x_j, sigma_f = 0.4, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nn &lt;- 400\nx &lt;- seq(-5, 5, length.out = n)\nf_x &lt;- f(x) + rnorm(n, mean = 0, sd = 0.1)\n\n\nn &lt;- 100\nind_train &lt;- sample(1:400, n, replace = FALSE)\nx_train &lt;- x[ind_train]\nf_train &lt;- f_x[ind_train]\n\ncompute_covmat &lt;- function(k, x, y) {\n  n_x &lt;- length(x)\n  n_y &lt;- length(y)\n  K &lt;- matrix(0, n_x, n_y)\n  for (i in 1:n_x) {\n    for (j in 1:n_y) {\n      K[i, j] &lt;- k(x[i], y[j])\n    }\n  }\n  return(K)\n}\n\nk_xx &lt;- compute_covmat(k, x_train, x_train)\n\nlambda &lt;- 0.0001\ncoef &lt;- solve(k_xx + n * lambda * diag(n)) %*% f_train\n\nf_hat &lt;- function(x) {\n  map2_dbl(coef, x_train, ~.x * k(x, .y)) |&gt;\n    reduce(`+`)\n}\n\ny_hat &lt;- map_dbl(x, f_hat)\n\ncolors &lt;- c(\"Actual\" = \"blue\", \"Data\" = \"black\", \"Estimate\" = \"orange\")\n\nout &lt;- tibble(x, f = f(x), f_x, y_hat) |&gt;\n  pivot_longer(-x) |&gt;\n  mutate(\n    name = factor(name, levels = c(\"f\", \"f_x\", \"y_hat\"), labels = names(colors))\n  )\n  \nggplot() +\n  geom_point(data = filter(out, name == \"Data\"), aes(x, y = value, color = name), alpha = 0.2) +\n  geom_line(data = filter(out, name == \"Actual\"), aes(x, y = value, color = name)) +\n  geom_line(data = filter(out, name == \"Estimate\"), aes(x, y = value, color = name)) +\n  scale_color_manual(name = \"\", values = colors) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Kernel Ridge Regression"
    ]
  },
  {
    "objectID": "notes/template.html",
    "href": "notes/template.html",
    "title": "Summary of progress over the previous week",
    "section": "",
    "text": "Summary of progress over the previous week\n\n\nCurrent Questions\n\n\nResources reviewed/cited"
  }
]
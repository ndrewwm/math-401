[
  {
    "objectID": "notes/template.html",
    "href": "notes/template.html",
    "title": "Summary of progress over the previous week",
    "section": "",
    "text": "Summary of progress over the previous week\n\n\nCurrent Questions\n\n\nResources reviewed/cited"
  },
  {
    "objectID": "gaussian_processes/overview.html",
    "href": "gaussian_processes/overview.html",
    "title": "Overview of Gaussian Processes",
    "section": "",
    "text": "This page relies heavily on x, y, z, and t (2018).\n\nDefinition 2.2: Gaussian Process\nLet \\(\\chi\\) be a nonempty set, \\(k: \\chi \\times \\chi \\to \\mathbb{R}\\) a positive definite kernel and \\(m: \\chi \\to \\mathbb{R}\\) be any real-valued function. Then a random function \\(f: \\chi \\to \\mathbb{R}\\) is said to be a Gaussian process (GP) with mean function \\(m\\) and covariance kernel \\(k\\), denoted by \\(\\mathcal{GP}(m, k)\\), if the following holds: For any finite set \\(X = (x_1, \\dots, x_n) \\subset \\chi\\) of any size \\(n \\in \\mathbb{N}\\), the random vector\n\\[\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n\\]\nfollows the multivariate normal distribution \\(\\mathcal{N}(m_X, k_{XX})\\) with covariance matrix \\(k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}\\) and mean vector \\(m_X = (m(x_1), \\dots, m(x_n))^\\top\\).\n\n\nGaussian Process Regression\n\nalso called kriging or Wiener-Kolmogorov prediction\n\nRegression is the task of estimating of an unknown function \\(f\\) based on a provided set of training data, \\((X, Y)\\), where \\(X = (x_1, \\dots, x_n)^\\top\\) and \\(Y = (y_1, \\dots, y_n)^\\top\\) are random vectors (\\(x_i\\) and \\(y_i\\) are realizations, collected by the experimenter). Regression assumes the presence of noise denoted by \\(\\epsilon\\), which completes the additive model:\n\\[\ny_i = f(x_i) + \\epsilon.\n\\]\nIt’s typically assumed that \\(\\epsilon\\) is normally distributed with mean 0, i.e., \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\).\nGaussian process regression is a Bayesian approach that uses a GP as a prior distribution for \\(f\\).\n\n\n\n\n\n\n\n\n\nPrior\nPosterior\n\n\n\n\nHyperparameter: kernel\n\\(k\\)\n\\(\\bar{k}\\)\n\n\nHyperparameter: mean function\n\\(m\\)\n\\(\\bar{m}\\)\n\n\nDistribution\n\\(\\mathcal{GP}(m, k)\\)\n\\(\\mathcal{GP}(\\bar{m}, \\bar{k})\\)\n\n\n\nThe authors show that the posterior distribution \\(f|Y \\sim \\mathcal{GP}(\\bar{m}, \\bar{k})\\) where \\(\\bar{m} : \\chi \\to \\mathbb{R}\\) and \\(\\bar{k}: \\chi \\times \\chi \\to \\mathbb{R}\\) is given by\n\\[\n\\begin{align*}\n\\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}(Y - m_X),\\ x \\in \\chi, \\\\\n\\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}k_{Xx'},\\ x,x' \\in \\chi,\n\\end{align*}\n\\]\nwhere \\(k_{Xx} = k_{xX}^\\top = (k(x_1, x), \\dots, k(x_n, x))^\\top\\).\nThis is interesting, because these are closed-form expressions, and notably, we haven’t made use of Bayes’s Rule. Assuming the kernel and mean functions aren’t expensive or difficult to evaluate, the computation is just linear algebra.\n\n\nDrawing from a Gaussian Process\n\nusing LinearAlgebra, Random, Plots\n\n\"\"\"Kernel provided by authors for example 2.4.\"\"\"\nfunction k(x_i::Float64, x_j::Float64)::Float64\n    return exp(-(x_i - x_j)^2 / 2);\nend;\n\nfunction rbf(x_i, x_j, gamma)\n    return exp(-norm(x_i, x_j)^2 / gamma^2);\nend;\n\n\"\"\"The 0 mean function.\"\"\"\nfunction m(x)::Float64\n    return 0.0;\nend;\n\nfunction gpDraw(k::Function, m::Function, X::Vector)\n    n = length(X);\n    m_X = m.(X);\n    k_XX = zeros(n, n);\n    for i = 1:n\n        for j = 1:n\n            k_XX[i, j] = k(X[i], X[j]);\n        end\n    end\n\n    R = cholesky(k_XX);\n    u = [randn() for i = 1:n];\n    return R.U' * u + m_X;\nend;\n\nX = sin.(range(0, 2pi, length = 10)) .+ [randn() for i = 1:10];\nY = [gpDraw(k, m, X) for i = 1:10];\n\nx = range(0, 2pi, length = 10);\n\nfor i = 2:length(Y)\n    plot!(x, Y[i])\nend\nplot!(x, Y[1])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\ntodos:\n\nupdate citations\n\n\n\n\n\n\n\nReviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing.\n\n\n\n\n\nDiscussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  },
  {
    "objectID": "index.html#upcoming",
    "href": "index.html#upcoming",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\ntodos:\n\nupdate citations"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "MATH 401",
    "section": "",
    "text": "Reviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "MATH 401",
    "section": "",
    "text": "Discussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  }
]
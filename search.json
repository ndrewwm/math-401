[
  {
    "objectID": "notes/template.html",
    "href": "notes/template.html",
    "title": "Summary of progress over the previous week",
    "section": "",
    "text": "Summary of progress over the previous week\n\n\nCurrent Questions\n\n\nResources reviewed/cited"
  },
  {
    "objectID": "gaussian_processes/overview.html",
    "href": "gaussian_processes/overview.html",
    "title": "Overview of Gaussian Processes",
    "section": "",
    "text": "This page relies heavily on x, y, z, and t (2018).\n\nDefinition 2.2: Gaussian Process\nLet \\(\\chi\\) be a nonempty set, \\(k: \\chi \\times \\chi \\to \\mathbb{R}\\) a positive definite kernel and \\(m: \\chi \\to \\mathbb{R}\\) be any real-valued function. Then a random function \\(f: \\chi \\to \\mathbb{R}\\) is said to be a Gaussian process (GP) with mean function \\(m\\) and covariance kernel \\(k\\), denoted by \\(\\mathcal{GP}(m, k)\\), if the following holds: For any finite set \\(X = (x_1, \\dots, x_n) \\subset \\chi\\) of any size \\(n \\in \\mathbb{N}\\), the random vector\n\\[\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n\\]\nfollows the multivariate normal distribution \\(\\mathcal{N}(m_X, k_{XX})\\) with covariance matrix \\(k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}\\) and mean vector \\(m_X = (m(x_1), \\dots, m(x_n))^\\top\\).\n\n\nGaussian Process Regression\n\nalso called kriging or Wiener-Kolmogorov prediction\n\nRegression is the task of estimating of an unknown function \\(f\\) based on a provided set of training data, \\((X, Y)\\), where \\(X = (x_1, \\dots, x_n)^\\top\\) and \\(Y = (y_1, \\dots, y_n)^\\top\\) are random vectors (\\(x_i\\) and \\(y_i\\) are realizations, collected by the experimenter). Regression assumes the presence of noise denoted by \\(\\epsilon\\), which completes the additive model:\n\\[\ny_i = f(x_i) + \\epsilon.\n\\]\nIt’s typically assumed that \\(\\epsilon\\) is normally distributed with mean 0, i.e., \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\).\nGaussian process regression is a Bayesian approach that uses a GP as a prior distribution for \\(f\\).\n\n\n\n\n\n\n\n\n\nPrior\nPosterior\n\n\n\n\nHyperparameter: kernel\n\\(k\\)\n\\(\\bar{k}\\)\n\n\nHyperparameter: mean function\n\\(m\\)\n\\(\\bar{m}\\)\n\n\nDistribution\n\\(\\mathcal{GP}(m, k)\\)\n\\(\\mathcal{GP}(\\bar{m}, \\bar{k})\\)\n\n\n\nThe authors show that the posterior distribution \\(f|Y \\sim \\mathcal{GP}(\\bar{m}, \\bar{k})\\) where \\(\\bar{m} : \\chi \\to \\mathbb{R}\\) and \\(\\bar{k}: \\chi \\times \\chi \\to \\mathbb{R}\\) is given by\n\\[\n\\begin{align*}\n\\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}(Y - m_X),\\ x \\in \\chi, \\\\\n\\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}k_{Xx'},\\ x,x' \\in \\chi,\n\\end{align*}\n\\]\nwhere \\(k_{Xx} = k_{xX}^\\top = (k(x_1, x), \\dots, k(x_n, x))^\\top\\).\nThis is interesting, because these are closed-form expressions, and notably, we haven’t made use of Bayes’s Rule. Assuming the kernel and mean functions aren’t expensive or difficult to evaluate, the computation is just linear algebra.\n\n\nDrawing from a Gaussian Process\nLet’s assume we want to model a noisy function \\(f\\).\n\nlibrary(tidyverse)\n\nf &lt;- function(x) sin(3*x) + sin(7*x)\nk &lt;- function(x_i, x_j, sigma_f = 2, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nn &lt;- 200\nx &lt;- seq(-5, 5, length.out = n)\nf_x &lt;- f(x)\n\nsigma &lt;- 0.4\nepsilon &lt;- rnorm(n, 0, sigma)\ny &lt;- f_x + epsilon\n\nd &lt;- tibble(x, f_x, y)\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\") +\n  geom_point(aes(y = y))\n\n\n\n\n\n\n\n\n\nn_star &lt;- 100\nt_index &lt;- sample(1:200, size = n_star, replace = FALSE)\nx_star &lt;- x[t_index]\ny_star &lt;- y[t_index]\nm_x &lt;- rep(0, 200)\n\nK &lt;- matrix(0, n, n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    K[i, j] &lt;- k(x[i], x[j])\n  }\n}\n\nK_star2 &lt;- matrix(0, n_star, n_star)\nfor (i in 1:n_star) {\n  for (j in 1:n_star) {\n    K_star2[i, j] &lt;- k(x_star[i], x_star[j])\n  }\n}\n\nK_star &lt;- matrix(0, n_star, n)\nfor (i in 1:n_star) {\n  for (j in 1:n) {\n    K_star[i, j] &lt;- k(x_star[i], x[j])\n  }\n}\n\nf_post_star &lt;- K_star %*% solve(K + sigma^2 * diag(n)) %*% (y - m_x)\ncov_post_star &lt;- K_star2 - K_star %*% solve(K + sigma^2 * diag(n)) %*% t(K_star)\n\nz_star &lt;- MASS::mvrnorm(50, f_post_star, cov_post_star)\n\ndraws &lt;- as_tibble(t(z_star)) |&gt;\n  mutate(x = x_star, y = f(x_star)) |&gt;\n  pivot_longer(V1:V50) \n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\npost_mean &lt;- draws |&gt;\n  group_by(x) |&gt;\n  summarize(post_mean = mean(value))\n\ndraws |&gt;\n  ggplot(aes(x = x)) +\n  geom_line(aes(y = value, group = name), alpha = 0.1) +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_line(data = post_mean, aes(x = x, y = post_mean), color = \"orange\") +\n  geom_point(data = post_mean, aes(x = x, y = post_mean), color = \"orange\")\n\n\n\n\n\n\n\n\n\n\nCitations (WIP)\n\nhttps://juanitorduz.github.io/gaussian_process_reg/\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/\nhttps://gregorygundersen.com/blog/2019/06/27/gp-regression/\nhttps://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\ntodos:\n\nupdate citations\n\n\n\n\n\n\n\nReviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing.\n\n\n\n\n\nDiscussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  },
  {
    "objectID": "index.html#upcoming",
    "href": "index.html#upcoming",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\ntodos:\n\nupdate citations"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "MATH 401",
    "section": "",
    "text": "Reviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "MATH 401",
    "section": "",
    "text": "Discussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  }
]
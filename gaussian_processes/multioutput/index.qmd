---
title: Multioutput GPR
bibliography: multioutput.bib
---

# Multioutput Gaussian Process Regression (GPR)

## Motivation: visualizing vector fields

Elsewhere, I've discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data $\mathbf{X} \in \mathbb{R}^{N \times P}$ is used to model $\mathbf{y} \in \mathbb{R}^N$:
$$
\begin{align*}
\mathbf{y} &= f(\mathbf{X}) + \mathbf{\epsilon} \text{ where } \\
f(\mathbf{X}) &\sim \mathcal{N}_N(\mathbf{0}, \mathbf{K}), \\
\mathbf{\epsilon} &\overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma_y), \text{ and } \\
\mathbf{K} &= [k(x_i, x_j)]_{ij}^N.
\end{align*}
$$

::: {.column-margin}
Throughout this page, I'll be using the subscript after $\mathcal{N}$ to indicate we're looking at a multivariate normal distribution, and to show the dimension of the vectors that the distribution produces.
:::

While interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we'll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the `gcookbook` package:

::: {.column-margin}
This example and dataset is drawn from *The R Graphics Cookbook* [@chang2018r], specifically chapter 13.12, on creating vector fields.
:::

> This data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.

Below I've plotted the x and y components of the storm's velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm's winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector:
$$
\mathbf{v} = \begin{bmatrix} v_x \\ v_y \end{bmatrix}.
$$

Velocity captures both the *direction* of the wind, and its *magnitude* (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we're looking at, such as the storm's spiral shape.

```{r}
#| message: false
#| warning: false
#| code-fold: true

library(tidyverse)
library(gcookbook)
library(scico)
set.seed(123)

eps <- sqrt(.Machine$double.eps)

# Keep a subset in the middle of the the z-axis
d_isabel <- isabel |>
  filter(z == 10.035) |>
  as_tibble()

# Keep 1 out of every 'by' values in vector x
every_n <- function(x, by = 4) {
  x <- sort(x)
  x[seq(1, length(x), by = by)]
}

# Keep 1 of every 4 values in x and y
keep_x <- every_n(unique(isabel$x))
keep_y <- every_n(unique(isabel$y))

# Keep only those rows where x value is in keepx and y value is in keepy
d <- d_isabel |>
  filter(x %in% keep_x, y %in% keep_y) |>
  mutate(index = 1:n())

# Need to load grid for arrow() function
library(grid)

# Set a consistent plotting theme
theme_set(theme_minimal(base_size = 15))

# Make a plot with the subset, and use an arrowhead 0.1 cm long
p0 <- ggplot(d, aes(x, y)) +
  geom_segment(
    aes(xend = x + vx/50, yend = y + vy/50),
    arrow = arrow(length = unit(0.1, "cm")),
    linewidth = 0.25
  ) +
  labs(x = "Longitude", y = "Latitude")

p0
```

A similar consideration applies to the task of predicting velocity using statistical models. A "naive" approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, $f_1: \mathbb{R}^2 \to \mathbb{R}$ and $f_2: \mathbb{R}^2 \to \mathbb{R}$. This amounts to treating $v_x$ and $v_y$ as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.

An improved approach would allow us to incorporate natural information on how our observed target values (each $v_x$ and $v_y$) might interrelate. The approach should also (ideally) account for the fact that we're trying to predict a vector at a specific point. In other words, we'd like a single function, whose outputs are vector-valued:

$$
\underbrace{\begin{aligned}[c]
v_x = f_1\Bigl((x, y)^\top \Bigr) + \epsilon \\
v_y = f_2\Bigl((x, y)^\top \Bigr) + \epsilon
\end{aligned}}_{\text{Naive approach}}
\qquad\longrightarrow\qquad
\underbrace{\begin{aligned}[c]
\begin{bmatrix}v_x \\ v_y\end{bmatrix} = f\Bigl((x, y)^\top \Bigr) + \epsilon
\end{aligned}}_{\text{Vector-valued approach}}
$$

These characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as "multioutput", "multitask", "cokriging", or "vector-valued" GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics [@alvarez2012kernels]. In the following section, we'll cover the theory behind the approach as it applies to our example data.

## Theory and notation

We are working in the space of the two-dimensional real numbers, $\mathbb{R}^2$. Let $S = (\mathbf{X}, \mathbf{Y}) = (x_1, y_1), \dots, (x_N, y_N)$ be our set of *training data*, where $x_i, y_i \in \mathbb{R}^2$. It may be convenient to note $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{N \times 2}$, and we may refer to $\mathbf{y} = \text{vec}\ Y \in \mathbb{R}^{2N}$ and $\mathbf{x} = \text{vec}\ X \in \mathbb{R}^{2N}$. We will use $\mathbf{X}_* \in \mathbb{R}^{M \times 2}$ to denote a set of *test points*, for which we want to generate predictions. As with the conventions for the training data, we will use $\mathbf{x}_* = \text{vec}\ \mathbf{X}_* \in \mathbb{R}^{2M}$.

We are engaged in a *regression* task, i.e., we're attempting to learn the functional relationship $f$ between $\mathbf{X}$ and $\mathbf{Y}$, with an assumption that this relationship has been corrupted to some degree by noise. For the following, I've adapted notation from Chapter 2.2 in the well-known Rasmussen and Williams text to the context of our multidimensional $\mathbf{Y}$ [@williams2006gaussian]. Setting up the problem, we have
$$
\begin{align*}
\mathbf{y} &= f(\mathbf{x}) + \epsilon \\
f &\sim \mathcal{GP}(m, k) \\
f(\mathbf{x}) &\sim \mathcal{N}_{2N}(\mathbf{0}, \mathbf{K}_{XX})
\end{align*}
$$

To parse these statements, we assume $f$ is drawn from a Gaussian Process with mean function $m$ and covariance (kernel) function $k$. Thus, our observations $\mathbf{y} = f(\mathbf{x})$ have a multivariate normal probability distribution, parameterized by a *covariance matrix* $\mathbf{K}_{XX}$. By convention, we assume the mean vector is $\mathbf{0}$ (implying that $m$ is the zero function).

The joint distribution of $\mathbf{y}$ and our predictions for the test point(s) $\mathbf{y}_*$ is also described by a multivariate normal distribution, specified as:
$$
\begin{align*}
\begin{bmatrix}
  \mathbf{y} \\
  \mathbf{y_*}
\end{bmatrix} &\sim \mathcal{N}_{2N + 2M}\Biggl( \mathbf{0}, \begin{bmatrix} 
  \mathbf{K}_{XX} + \mathbf{V} \otimes \mathbf{I}_N & \mathbf{K}_{XX_*} \\
  \mathbf{K}_{X_*X} & \mathbf{K}_{X_* X_*}
\end{bmatrix} \Biggr) \\
\mathbf{V} &= \begin{bmatrix} \sigma^2_1 & 0 \\ 0 & \sigma^2_2 \end{bmatrix}
\end{align*}
$$

Here, $\sigma^2_1, \sigma^2_2 > 0$ represent the independent and additive noise associated with each of our outcome's components. The $\otimes$ symbol denotes the Kronecker product. From this joint distribution, we can derive the conditional distribution for $\mathbf{y}_*$:

$$
\begin{align*}
\mathbf{y}_* | \mathbf{X}, \mathbf{y}, \mathbf{X_*} &\sim \mathcal{N}_{2M}(\bar{\mathbf{f}}_*,\ \text{cov}(\mathbf{f}_*)) \\
\bar{\mathbf{f}}_* &= \mathbb{E}[\mathbf{y}_* | \mathbf{X}, \mathbf{y}, \mathbf{X_*}] = \mathbf{K}_{X_*X}(\mathbf{K}_{XX} + \mathbf{V} \otimes \mathbf{I}_N)^{-1} \mathbf{y} \\
\text{cov}(\mathbf{f}_*) &= \mathbf{K}_{X_*X_*} - \mathbf{K}_{X_*X}(\mathbf{K}_{XX} + \mathbf{V} \otimes \mathbf{I}_N)^{-1}\mathbf{K}_{XX_*} \\
\end{align*}
$$

We will now work through how each piece of our distribution is defined. Let $k: \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}$ be a (scalar) kernel function, defined as
$$
k(x_i, x_j) = \alpha^2 \cdot \exp\Bigl(-\frac{1}{2\rho^2} \| x_i - x_j \|_2 \Bigl),
$$ {#eq-k}

where $\alpha, \rho > 0$, and $\| \cdot \|_2$ is the Euclidean norm. This is also known as the squared exponential function, or the radial basis function. The parameter $\alpha$ controls ..., while $\rho$ controls the length-scale. Let $\mathbf{k}_{XX} \in \mathbb{R}^{N \times N}$, the covariance matrix between all points in $\mathbf{X}$, be defined as
$$
\mathbf{k}_{XX} = (k(x_i, x_j))_{i,j}^N = \begin{bmatrix}
  k(x_1, x_1) & \cdots & k(x_1, x_N) \\
  \vdots & \ddots & \vdots \\
  k(x_N, x_1) & \cdots & k(x_N, x_N)
\end{bmatrix}.
$$ {#eq-kXX}

Let $\mathbf{B} \in \mathbb{R}^{2 \times 2}$, the matrix of similarities between the outputs $\mathbf{Y}$, be defined as
$$
\mathbf{B} = \begin{bmatrix}
  b_{11} & b_{12} \\
  b_{21} & b_{22}
\end{bmatrix} = \frac{1}{N} \mathbf{Y}^\top \mathbf{k}_{XX} \mathbf{Y}.
$$ {#eq-B}

The expression for $\mathbf{B}$ comes from Bonilla et al. [@bonilla2007multi], in section 2.3 (the authors use the notation $K^f$ and $K^x$ instead of $\mathbf{B}$ and $\mathbf{k}_{XX}$, respectively).

Let $\mathbf{K}_{XX} \in \mathbb{R}^{2N \times 2N}$ be defined as
$$
\mathbf{K}_{XX} = \mathbf{B} \otimes \mathbf{k}_{XX}.
$$ {#eq-KXX}

We can now define the other pieces needed to establish the covariance matrix used for the joint distribution of $\begin{bmatrix} \mathbf{y} \\ \mathbf{y}_* \end{bmatrix}$:

$$
\begin{align*}
  \mathbf{K}_{X_*X} &= \mathbf{B} \otimes (k(x_{*i}, x_j))_{i,j=1}^{M,N} \in \mathbb{R}^{2M \times 2N} \\
  \mathbf{K}_{XX_*} &= \mathbf{K}_{X_*X}^\top \in \mathbb{R}^{2N \times 2M} \\
  \mathbf{K}_{X_*X_*} &= \mathbf{B} \otimes (k(x_{*i}, x_{*j}))_{i,j=1}^{M} \in \mathbb{R}^{2M \times 2M}.
\end{align*}
$$ {#eq-KxX}

# Training and test data

Here, using the Hurricane Isabel data visualized above, we generate $S$, defined as a 30% random sample of observations.

```{r}
# Set up the training data
S <- d |>
  slice_sample(n = floor(0.3 * nrow(d))) |>
  select(x, y, vx, vy, index)

X <- S |> select(x, y) |> as.matrix()
Y <- S |> select(vx, vy) |> as.matrix()
y <- as.vector(Y)
N <- nrow(X)
```

# Kernel functions

This section defines R functions used to compute $\mathbf{K}_{XX}$.

```{r}
#| code-fold: true

#' Squared exponential (scalar) kernel function
#' 
#' @param xi numeric vector
#' @param xj numeric vector
#' @param alpha scale, alpha > 0
#' @param rho length-scale, rho > 0
#' @return numeric
k <- function(xi, xj, alpha = 1, rho = 1) {
  alpha^2 * exp(-norm(xi - xj, type = "2") / (2 * rho^2))
}

#' Compute the kernel matrix, given a set of input vectors
#' 
#' @param X numeric matrix of dimensions N x P, where N is observations and P is components
#' @param alpha numeric, variance scale, alpha > 0
#' @param rho numeric, length-scale, rho > 0
#' @param err numeric, err > 0
#' @return N x N matrix
k_XX <- function(X, alpha = 1, rho = 1, err = eps) {
  N <- nrow(X)
  K <- matrix(0, N, N)
  for (i in 1:N) {
    for (j in 1:N) {
      K[i, j] <- k(X[i, ], X[j, ], alpha, rho)
    }
  }

  if (!is.null(err)) {
    K <- K + diag(err, ncol(K))
  }

  K
}

#' Compute the covariance between two sets of vectors
#'
#' @param x numeric matrix, N x P
#' @param X numeric matrix, M x P
#' @param alpha numeric, variance scale, alpha > 0
#' @param rho numeric, length-scale, rho > 0
#' @return A numeric matrix, N x M
k_xX <- function(x, X, alpha = 1, rho = 1) {
  N <- nrow(x)
  M <- nrow(X)

  K <- matrix(0, N, M)
  for (i in 1:N) {
    for (j in 1:M) {
      K[i, j] <- k(x[i, ], X[j, ], alpha, rho)
    }
  }

  K
}
```

# Estimation

<!-- requires: quarto add quarto-ext/include-code-files -->
<!-- ```{.stan include="estimate_params.stan"}
``` -->

::: {.callout-caution collapse="true"}

```{r}
#| cache: true
#| eval: false

library(rstan)

fit <- stan(
  file = "./estimate_params.stan",
  model_name = "Parameter estimation, Isabel GPR",
  data = lst(N, X, Y),
  chains = 2,
  iter = 1000,
  warmup = 400,
  seed = 123
)
```

:::

```{r}
#| eval: false

fit
```

# Evaluation

```{r}
K_XX <- k_XX(X, 1.21, 0.85, eps) #theta$par[1], theta$par[2], theta$par[3])
B <- 1/nrow(X) * t(Y) %*% solve(K_XX) %*% Y
Sigma <- kronecker(B, K_XX)
Si <- solve(Sigma) %*% y

f <- data.frame()
for (i in 1:nrow(d)) {
  x_star <- as.matrix(d[i, 1:2])
  K_star <- apply(X[, 1:2], 1, function(x) k(x_star, x))
  f_star <- t(kronecker(B, K_star)) %*% Si |> t()
  f <- rbind(f, f_star)
}
```

```{r}
f <- f |>
  tibble() |>
  set_names(c("vx_h", "vy_h")) |>
  bind_cols(d) |>
  mutate(col = factor(as.numeric(index %in% S$index), levels = 0:1, labels = c("Test", "Train")))

p1 <- p0 +
  geom_segment(
    data = f,
    aes(x, y, xend = x + vx_h/50, yend = y + vy_h/50, color = col),
    arrow = arrow(length = unit(0.1, "cm")),
    linewidth = 0.25,
    alpha = 0.8
  ) +
  scale_color_scico_d(palette = "imola", name = "") +
  labs(x = "Longitude", y = "Latitude") +
  theme(legend.position = "bottom")

p1

rmse <- function(y, yhat) {
  sqrt(1/length(y) * sum((yhat - y)^2))
}

f |>
  group_by(col) |>
  summarize(
    rmse_vx = rmse(vx, vx_h),
    rmse_vy = rmse(vy, vy_h)
  )
```

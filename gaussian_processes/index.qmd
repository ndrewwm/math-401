---
title: Overview of Gaussian Processes
---

This page relies heavily on Kanagawa, Hennig, Sejdinovic, and Sriperumbudur (2018).

### Definition 2.2: Gaussian Process

Let $\chi$ be a nonempty set, $k: \chi \times \chi \to \mathbb{R}$ a positive definite kernel and $m: \chi \to \mathbb{R}$ be any real-valued function. Then a random function $f: \chi \to \mathbb{R}$ is said to be a Gaussian process (GP) with *mean function* $m$ and *covariance kernel* $k$, denoted by $\mathcal{GP}(m, k)$, if the following holds: For any finite set $X = (x_1, \dots, x_n) \subset \chi$ of any size $n \in \mathbb{N}$, the random vector

$$
f_X = (f(x_1), \dots, f(x_n))^\top \in \mathbb{R}^n
$$

follows the multivariate normal distribution $\mathcal{N}(m_X, k_{XX})$ with covariance matrix $k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \in \mathbb{R}^{n \times n}$ and mean vector $m_X = (m(x_1), \dots, m(x_n))^\top$.

### Gaussian Process Regression

- also called *kriging* or *Wiener-Kolmogorov prediction*

Regression is the task of estimating of an unknown function $f$ based on a provided set of *training data*, $(X, Y)$, where $X = (x_1, \dots, x_n)^\top$ and $Y = (y_1, \dots, y_n)^\top$ are random vectors ($x_i$ and $y_i$ are *realizations*, collected by the experimenter). Regression assumes the presence of *noise* denoted by $\epsilon$, which completes the additive model:

$$
y_i = f(x_i) + \epsilon.
$$

It's typically assumed that $\epsilon$ is normally distributed with mean 0, i.e., $\epsilon \sim \mathcal{N}(0, \sigma)$.

Gaussian process regression is a Bayesian approach that uses a GP as a prior distribution for $f$.

|                               | Prior                | Posterior |
| :---------------------------- | :------------------- | :-------- |
| Hyperparameter: kernel        | $k$                  | $\bar{k}$ |
| Hyperparameter: mean function | $m$                  | $\bar{m}$ |
| Distribution                  | $\mathcal{GP}(m, k)$ | $\mathcal{GP}(\bar{m}, \bar{k})$ |

The authors show that the *posterior distribution* $f|Y \sim \mathcal{GP}(\bar{m}, \bar{k})$ where $\bar{m} : \chi \to \mathbb{R}$ and $\bar{k}: \chi \times \chi \to \mathbb{R}$ is given by

$$
\begin{align*}
\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \sigma^2I_n)^{-1}(Y - m_X),\ x \in \chi, \\
\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \sigma^2I_n)^{-1}k_{Xx'},\ x,x' \in \chi,
\end{align*}
$$

where $k_{Xx} = k_{xX}^\top = (k(x_1, x), \dots, k(x_n, x))^\top$.

This is interesting, because these are closed-form expressions, and notably, we haven't made use of Bayes's Rule. Assuming the kernel and mean functions aren't expensive or difficult to evaluate, the computation is just linear algebra.

### Drawing from a Gaussian Process

```{r}
#| message: false
#| warning: false
#| out-width: 100%

library(tidyverse)
set.seed(123)

f <- function(x) sin(3*x) + sin(7*x)
k <- function(x_i, x_j, sigma_f = 1, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))

N <- 400
n <- 120
x <- seq(-5, 5, length.out = N)
f_x <- f(x)

sigma <- 0.15
epsilon <- rnorm(N, 0, sigma)
y <- f_x + epsilon

d <- tibble(x, f_x, y)
ggplot(d, aes(x = x)) +
  geom_line(aes(y = f_x), color = "blue") +
  geom_point(aes(y = y))
```

Here we'll compute the posterior distribution over the whole range $[-5, 5]$, using $n = 100$ training data points.

```{r}
#| out-width: 100%

ind_train <- sample(1:400, n, replace = FALSE)
x_train <- x[ind_train]
y_train <- y[ind_train]

compute_covmat <- function(k, x, y) {
  n_x <- length(x)
  n_y <- length(y)
  K <- matrix(0, n_x, n_y)
  for (i in 1:n_x) {
    for (j in 1:n_y) {
      K[i, j] <- k(x[i], y[j])
    }
  }
  return(K)
}

k_XX <- compute_covmat(k, x_train, x_train)
k_xx <- compute_covmat(k, x, x)
k_xX <- compute_covmat(k, x, x_train)

m_post <- k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% y_train
cov_post <- k_xx - k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% t(k_xX)
```

Here `m_post` represents the mean vector of the posterior distibution, and `cov_post` represents the posterior's covariance matrix.

::: {.column-margin}
In Kanagawa et al. (2018), they note that the final term in the posterior mean function $\bar{m}$ should be something like $(\mathbf{y} - m_\mathbf{x})$ where $m_\mathbf{x} = [m(x_1), \dots, m(x_N)]^\top$. I've omitted the subtraction given that $m(\mathbf{x}) = \mathbf{0}$.
:::

Below in blue is the actual function $f$, with 50 draws (grey) from the posterior distribution. The posterior mean of these draws is plotted as the orange line.
```{r}
#| warning: false

f_post <- MASS::mvrnorm(50, m_post, cov_post)

draws <- as_tibble(t(f_post)) |>
  mutate(x = x, y = f(x)) |>
  pivot_longer(V1:V50)

post_mean <- draws |>
  group_by(x) |>
  summarize(post_mean = mean(value))

draws |>
  ggplot(aes(x = x)) +
  geom_point(
    data = tibble(x = x[ind_train], y = y[ind_train]),
    aes(x, y), alpha = 0.2
  ) +
  geom_line(aes(y = value, group = name), alpha = 0.1) +
  geom_line(aes(y = y), color = "blue", lty = "dashed") +
  geom_line(data = post_mean, aes(x = x, y = post_mean), color = "orange")
```

```{r}
f_post <- MASS::mvrnorm(n = 1000, m_post, cov_post)

draws <- as_tibble(t(f_post)) |>
  mutate(x = x, y = f(x)) |>
  pivot_longer(V1:V1000)

post_summaries <- draws |>
  group_by(x) |>
  summarize(
    post_mean = mean(value),
    post_sd = sd(value)
  )

post_summaries <- draws |>
  distinct(x, y) |>
  left_join(post_summaries, by = "x")

ggplot(data = post_summaries, aes(x = x)) +
  geom_ribbon(aes(ymin = post_mean - 2*post_sd, ymax = post_mean + 2*post_sd), fill = "grey70") +
  geom_line(aes(y = post_mean), color = "orange") +
  geom_line(aes(y = y), color = "blue", lty = "dashed") +
  geom_point(data = tibble(x = x[ind_train], y = y[ind_train]), aes(x, y), alpha = 0.3)
```

### Citations (WIP)

- https://juanitorduz.github.io/gaussian_process_reg/

- https://www.cs.toronto.edu/~duvenaud/cookbook/

- https://gregorygundersen.com/blog/2019/06/27/gp-regression/

- https://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/

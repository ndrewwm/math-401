---
title: Overview of Gaussian Processes
---

This page relies heavily on x, y, z, and t (2018).

### Definition 2.2: Gaussian Process

Let $\chi$ be a nonempty set, $k: \chi \times \chi \to \mathbb{R}$ a positive definite kernel and $m: \chi \to \mathbb{R}$ be any real-valued function. Then a random function $f: \chi \to \mathbb{R}$ is said to be a Gaussian process (GP) with *mean function* $m$ and *covariance kernel* $k$, denoted by $\mathcal{GP}(m, k)$, if the following holds: For any finite set $X = (x_1, \dots, x_n) \subset \chi$ of any size $n \in \mathbb{N}$, the random vector

$$
f_X = (f(x_1), \dots, f(x_n))^\top \in \mathbb{R}^n
$$

follows the multivariate normal distribution $\mathcal{N}(m_X, k_{XX})$ with covariance matrix $k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \in \mathbb{R}^{n \times n}$ and mean vector $m_X = (m(x_1), \dots, m(x_n))^\top$.

### Gaussian Process Regression

- also called *kriging* or *Wiener-Kolmogorov prediction*

Regression is the task of estimating of an unknown function $f$ based on a provided set of *training data*, $(X, Y)$, where $X = (x_1, \dots, x_n)^\top$ and $Y = (y_1, \dots, y_n)^\top$ are random vectors ($x_i$ and $y_i$ are *realizations*, collected by the experimenter). Regression assumes the presence of *noise* denoted by $\epsilon$, which completes the additive model:

$$
y_i = f(x_i) + \epsilon.
$$

It's typically assumed that $\epsilon$ is normally distributed with mean 0, i.e., $\epsilon \sim \mathcal{N}(0, \sigma)$.

Gaussian process regression is a Bayesian approach that uses a GP as a prior distribution for $f$.

|                               | Prior                | Posterior |
| :---------------------------- | :------------------- | :-------- |
| Hyperparameter: kernel        | $k$                  | $\bar{k}$ |
| Hyperparameter: mean function | $m$                  | $\bar{m}$ |
| Distribution                  | $\mathcal{GP}(m, k)$ | $\mathcal{GP}(\bar{m}, \bar{k})$ |

The authors show that the *posterior distribution* $f|Y \sim \mathcal{GP}(\bar{m}, \bar{k})$ where $\bar{m} : \chi \to \mathbb{R}$ and $\bar{k}: \chi \times \chi \to \mathbb{R}$ is given by

$$
\begin{align*}
\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \sigma^2I_n)^{-1}(Y - m_X),\ x \in \chi, \\
\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \sigma^2I_n)^{-1}k_{Xx'},\ x,x' \in \chi,
\end{align*}
$$

where $k_{Xx} = k_{xX}^\top = (k(x_1, x), \dots, k(x_n, x))^\top$.

This is interesting, because these are closed-form expressions, and notably, we haven't made use of Bayes's Rule. Assuming the kernel and mean functions aren't expensive or difficult to evaluate, the computation is just linear algebra.


### Drawing from a Gaussian Process

```{julia}
using LinearAlgebra, Random, Plots

"""Kernel provided by authors for example 2.4."""
function k(x_i::Float64, x_j::Float64)::Float64
    return exp(-(x_i - x_j)^2 / 2);
end;

function rbf(x_i, x_j, gamma)
    return exp(-norm(x_i, x_j)^2 / gamma^2);
end;

"""The 0 mean function."""
function m(x)::Float64
    return 0.0;
end;

function gpDraw(k::Function, m::Function, X::Vector)
    n = length(X);
    m_X = m.(X);
    k_XX = zeros(n, n);
    for i = 1:n
        for j = 1:n
            k_XX[i, j] = k(X[i], X[j]);
        end
    end

    R = cholesky(k_XX);
    u = [randn() for i = 1:n];
    return R.U' * u + m_X;
end;

X = sin.(range(0, 2pi, length = 10)) .+ [randn() for i = 1:10];
Y = [gpDraw(k, m, X) for i = 1:10];

x = range(0, 2pi, length = 10);

for i = 2:length(Y)
    plot!(x, Y[i])
end
plot!(x, Y[1])
```

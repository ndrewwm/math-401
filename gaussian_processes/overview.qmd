---
title: Overview of Gaussian Processes
---

This page relies heavily on x, y, z, and t (2018).

### Definition 2.2: Gaussian Process

Let $\chi$ be a nonempty set, $k: \chi \times \chi \to \mathbb{R}$ a positive definite kernel and $m: \chi \to \mathbb{R}$ be any real-valued function. Then a random function $f: \chi \to \mathbb{R}$ is said to be a Gaussian process (GP) with *mean function* $m$ and *covariance kernel* $k$, denoted by $\mathcal{GP}(m, k)$, if the following holds: For any finite set $X = (x_1, \dots, x_n) \subset \chi$ of any size $n \in \mathbb{N}$, the random vector

$$
f_X = (f(x_1), \dots, f(x_n))^\top \in \mathbb{R}^n
$$

follows the multivariate normal distribution $\mathcal{N}(m_X, k_{XX})$ with covariance matrix $k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \in \mathbb{R}^{n \times n}$ and mean vector $m_X = (m(x_1), \dots, m(x_n))^\top$.

### Gaussian Process Regression

- also called *kriging* or *Wiener-Kolmogorov prediction*

Regression is the task of estimating of an unknown function $f$ based on a provided set of *training data*, $(X, Y)$, where $X = (x_1, \dots, x_n)^\top$ and $Y = (y_1, \dots, y_n)^\top$ are random vectors ($x_i$ and $y_i$ are *realizations*, collected by the experimenter). Regression assumes the presence of *noise* denoted by $\epsilon$, which completes the additive model:

$$
y_i = f(x_i) + \epsilon.
$$

It's typically assumed that $\epsilon$ is normally distributed with mean 0, i.e., $\epsilon \sim \mathcal{N}(0, \sigma)$.

Gaussian process regression is a Bayesian approach that uses a GP as a prior distribution for $f$.

|                               | Prior                | Posterior |
| :---------------------------- | :------------------- | :-------- |
| Hyperparameter: kernel        | $k$                  | $\bar{k}$ |
| Hyperparameter: mean function | $m$                  | $\bar{m}$ |
| Distribution                  | $\mathcal{GP}(m, k)$ | $\mathcal{GP}(\bar{m}, \bar{k})$ |

The authors show that the *posterior distribution* $f|Y \sim \mathcal{GP}(\bar{m}, \bar{k})$ where $\bar{m} : \chi \to \mathbb{R}$ and $\bar{k}: \chi \times \chi \to \mathbb{R}$ is given by

$$
\begin{align*}
\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \sigma^2I_n)^{-1}(Y - m_X),\ x \in \chi, \\
\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \sigma^2I_n)^{-1}k_{Xx'},\ x,x' \in \chi,
\end{align*}
$$

where $k_{Xx} = k_{xX}^\top = (k(x_1, x), \dots, k(x_n, x))^\top$.

This is interesting, because these are closed-form expressions, and notably, we haven't made use of Bayes's Rule. Assuming the kernel and mean functions aren't expensive or difficult to evaluate, the computation is just linear algebra.

### Drawing from a Gaussian Process

Let's assume we want to model a noisy function $f$.
```{r}
#| message: false
#| warning: false

library(tidyverse)

f <- function(x) sin(3*x) + sin(7*x)
k <- function(x_i, x_j, sigma_f = 2, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))

n <- 200
x <- seq(-5, 5, length.out = n)
f_x <- f(x)

sigma <- 0.4
epsilon <- rnorm(n, 0, sigma)
y <- f_x + epsilon

d <- tibble(x, f_x, y)
ggplot(d, aes(x = x)) +
  geom_line(aes(y = f_x), color = "blue") +
  geom_point(aes(y = y))
```

```{r}
n_star <- 100
t_index <- sample(1:200, size = n_star, replace = FALSE)
x_star <- x[t_index]
y_star <- y[t_index]
m_x <- rep(0, 200)

K <- matrix(0, n, n)
for (i in 1:n) {
  for (j in 1:n) {
    K[i, j] <- k(x[i], x[j])
  }
}

K_star2 <- matrix(0, n_star, n_star)
for (i in 1:n_star) {
  for (j in 1:n_star) {
    K_star2[i, j] <- k(x_star[i], x_star[j])
  }
}

K_star <- matrix(0, n_star, n)
for (i in 1:n_star) {
  for (j in 1:n) {
    K_star[i, j] <- k(x_star[i], x[j])
  }
}

f_post_star <- K_star %*% solve(K + sigma^2 * diag(n)) %*% (y - m_x)
cov_post_star <- K_star2 - K_star %*% solve(K + sigma^2 * diag(n)) %*% t(K_star)

z_star <- MASS::mvrnorm(50, f_post_star, cov_post_star)

draws <- as_tibble(t(z_star)) |>
  mutate(x = x_star, y = f(x_star)) |>
  pivot_longer(V1:V50) 

post_mean <- draws |>
  group_by(x) |>
  summarize(post_mean = mean(value))

draws |>
  ggplot(aes(x = x)) +
  geom_line(aes(y = value, group = name), alpha = 0.1) +
  geom_line(aes(y = y), color = "blue", lty = "dashed") +
  geom_line(data = post_mean, aes(x = x, y = post_mean), color = "orange") +
  geom_point(data = post_mean, aes(x = x, y = post_mean), color = "orange")
```

### Citations (WIP)

- https://juanitorduz.github.io/gaussian_process_reg/

- https://www.cs.toronto.edu/~duvenaud/cookbook/

- https://gregorygundersen.com/blog/2019/06/27/gp-regression/

- https://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/

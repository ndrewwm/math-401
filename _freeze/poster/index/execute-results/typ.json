{
  "hash": "8725e9880613ce7f0507a5cdee7ffe96",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Vector-valued Gaussian Processes\nexecute: \n  freeze: true\nknitr:\n  opts_chunk:\n    echo: false\n    warning: false\n    message: false\nformat:\n  poster-typst:\n    size: \"48x36\"\n    poster-authors: \"Andrew Moore, Grady Wright\"\n    departments: \"Department of Mathematics\"\n    institution-logo: \"./images/boisestate-laligned-logo.png\"\n    footer-text: \"Spring 2025, Senior Showcase\"\n    footer-url: \"https://ndrewwm.github.io/math-401\"\n    footer-emails: \"andrewmoore1@u.boisestate.edu\"\n    footer-color: \"0033A0\" #\"D64309\"\n    keywords: [\"Gaussian Processes\", \"Statistics\", \"Velocity Fields\"]\n---\n\n\n\n# Overview of Gaussian Processes\n\nGaussian processes generalize the multivariate Gaussian distribution and can be used to describe a probability distribution over families of functions.\n\n## Multivariate Gaussian Distribution\n\n\n\n```{=typst}\nThe multivariate Gaussian distribution is used to model _random vectors_ (vectors of jointly distributed random variables).\n\n$ bold(x) in RR^N &tilde.op cal(N)_N (bold(mu), bold(Sigma)) \\ \n  bold(mu) in RR^N &= (mu_1, mu_2, ..., mu_N)^top = (EE(x_1), EE(x_2), ..., EE(x_N))^top \\ \n  bold(Sigma) in RR^(N times N) &= EE((bold(x) - mu)(bold(x) - mu)^top) = [op(\"cov\")(x_i, x_j)]_(i,j = 1)^N \\\n  x_i &tilde.op cal(N)(mu_i, bold(Sigma)_(i i)) $\n```\n\n\n\n## Gaussian Processes (GPs)\n\n\n\n```{=typst}\n- _Gaussian process_ (GP): an uncountably infinite collection of random variables; any finite sample is a draw from a MV Gaussian distribution.\n- GPs are fully specified by a _mean function_ $m$ and _covariance (kernel) function_ $k$.\n- The kernel function must produce a positive semi-definite matrix when evaluated on a set of input points (or vectors).\n```\n\n```{=typst}\nWe focus on the _squared exponential kernel_ $k: RR^p times RR^p -> RR$, defined as:\n\n$ k(x, x') = alpha^2 op(\"exp\")(-1/(2rho^2) ||x - x'||^2). $\n\n- $||dot||$ is the Euclidean Norm: $||bold(x)|| = sqrt(x_1 + x_2 + dots.h.c + x_N)$\n- $alpha$ and $rho$ are _hyperparameters_ (chosen, or estimated from data)\n```\n\n\n\n# Gaussian Process Regression -- Univariate $\\mathbf{y}$\n\n\n\n```{=typst}\nLet $S = (bold(x), bold(y)) = { (x_i, y_i) : x_i, y_i in RR, i in 1, 2, ..., N }$ be a researcher's dataset, and let $N = 20$ and $M = 400 - N$. We wish to use $S$ to find an unknown function $f$ that satisfies $bold(y) = f(bold(x))$, possibly subject to additive noise $epsilon$.\n```\n\n::: {.cell}\n\n:::\n\n```{=typst}\nWe can draw samples from the prior distribution:\n\n$\nf \" \" &~ \" \" cal(\"GP\")(bold(0), k) \\\nbold(y)_* \" \" &~ \" \" cal(N)_M (bold(0), k(bold(x)_*, bold(x)_*)) \\\nk(bold(x)_*, bold(x)_*) in RR^(M times M) &= [k(bold(x)_(*i), bold(x)_(*j))]_(i,j = 1)^M. $\n\n\n- Draws from the prior distribution (shown in grey) don't necessarily agree with the data points.\n- Kernel choice determines properties of $f$ (e.g., smoothness)\n```\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-typst/unnamed-chunk-2-1.svg)\n:::\n:::\n\n\n\nOur prior model for $f$ and the observed data $S$ can be combined to form a _posterior_ distribution:\n\n\n\n```{=typst}\n$ bold(y)_* | bold(x), bold(y), bold(x)_* \" \" &~ \" \" cal(N)_M (hat(mu), hat(Sigma)) \\\nhat(mu) in RR^M &= k(bold(x)_*, bold(x)) k(bold(x), bold(x))^(-1) bold(y) \\\nhat(Sigma) in RR^(M times M) &= k(bold(x)_*, bold(x)_*) - k(bold(x)_*, bold(x))k(bold(x), bold(x))^(-1)k(bold(x)_*, bold(x))^top \\\nk(bold(x), bold(x)) in RR^(N times N) &= [k(x_i, x_j)]_(i,j = 1)^N \\\nk(bold(x)_*, bold(x)) in RR^(M times N) &= [k(x_(*i), x_j)]_(i,j = 1)^(M,N). $\n```\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-typst/unnamed-chunk-3-1.svg)\n:::\n:::\n\n\n\n# Multioutput GPR -- Vector-valued $\\mathbf{y}$\n\n\n```{=typst}\n- GPR can be extended to targets with >1 dimensions.\n- Velocity fields: $bold(X), bold(Y) in RR^(N times 2)$\n- Idea: columns of $bold(Y)$ might not be independent\n- Intrinsic Coregionalization Model (ICM): combines the kernel matrix with a similarity matrix $B$\n```\n\n```{=typst}\n$ bold(X), bold(Y) in RR^(N times 2), \" \" bold(y) &in RR^(2N) = \"vec\"(Y), \" \" bold(X)_* in RR^(M times 2) \\\n  bold(y)_* | bold(X), bold(y), bold(X)_* \" \" &~ \" \" cal(N)_(2M) (hat(mu), hat(Sigma)) \\\n  hat(mu) in RR^(2M) &= K_(bold(X)_* bold(X)) K_(bold(X) bold(X))^(-1) bold(y) \\\n  hat(Sigma) in RR^(2M times 2M) &= K_(bold(X)_* bold(X)_*) - K_(bold(X)_* bold(X)) K_(bold(X) bold(X))^(-1) K_(bold(X)_* bold(X))^top \\\n  K_(bold(X) bold(X)) in RR^(2N times 2N) &= B times.circle k(bold(X), bold(X)) \\\n  K_(bold(X)_* bold(X)) in RR^(2M times 2N) &= B times.circle k(bold(X)_*, bold(X)) \\\n  K_(bold(X)_* bold(X)_*) in RR^(2M times 2M) &= B times.circle k(bold(X)_*, bold(X)_*) \\\n  B in RR^(2 times 2) &= \"corr\"(bold(Y)) = (\n  (angle.l bold(y)_i - overline(bold(y))_i, bold(y)_j - overline(bold(y))_j angle.r) / \n    (||bold(y)_i - overline(bold(y))_i|| ||bold(y)_j - overline(bold(y))_j||)\n)_(i,j = 1)^2 \n$\n```\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-typst/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n<!-- PIV DATA -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-typst/unnamed-chunk-9-1.svg){fig-align='center'}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}
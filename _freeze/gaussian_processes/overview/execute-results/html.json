{
  "hash": "641bd69aae26bdfe34495e5cd3b9632b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Overview of Gaussian Processes\n---\n\n\n\nThis page relies heavily on x, y, z, and t (2018).\n\n### Definition 2.2: Gaussian Process\n\nLet $\\chi$ be a nonempty set, $k: \\chi \\times \\chi \\to \\mathbb{R}$ a positive definite kernel and $m: \\chi \\to \\mathbb{R}$ be any real-valued function. Then a random function $f: \\chi \\to \\mathbb{R}$ is said to be a Gaussian process (GP) with *mean function* $m$ and *covariance kernel* $k$, denoted by $\\mathcal{GP}(m, k)$, if the following holds: For any finite set $X = (x_1, \\dots, x_n) \\subset \\chi$ of any size $n \\in \\mathbb{N}$, the random vector\n\n$$\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n$$\n\nfollows the multivariate normal distribution $\\mathcal{N}(m_X, k_{XX})$ with covariance matrix $k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}$ and mean vector $m_X = (m(x_1), \\dots, m(x_n))^\\top$.\n\n### Gaussian Process Regression\n\n- also called *kriging* or *Wiener-Kolmogorov prediction*\n\nRegression is the task of estimating of an unknown function $f$ based on a provided set of *training data*, $(X, Y)$, where $X = (x_1, \\dots, x_n)^\\top$ and $Y = (y_1, \\dots, y_n)^\\top$ are random vectors ($x_i$ and $y_i$ are *realizations*, collected by the experimenter). Regression assumes the presence of *noise* denoted by $\\epsilon$, which completes the additive model:\n\n$$\ny_i = f(x_i) + \\epsilon.\n$$\n\nIt's typically assumed that $\\epsilon$ is normally distributed with mean 0, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$.\n\nGaussian process regression is a Bayesian approach that uses a GP as a prior distribution for $f$.\n\n|                               | Prior                | Posterior |\n| :---------------------------- | :------------------- | :-------- |\n| Hyperparameter: kernel        | $k$                  | $\\bar{k}$ |\n| Hyperparameter: mean function | $m$                  | $\\bar{m}$ |\n| Distribution                  | $\\mathcal{GP}(m, k)$ | $\\mathcal{GP}(\\bar{m}, \\bar{k})$ |\n\nThe authors show that the *posterior distribution* $f|Y \\sim \\mathcal{GP}(\\bar{m}, \\bar{k})$ where $\\bar{m} : \\chi \\to \\mathbb{R}$ and $\\bar{k}: \\chi \\times \\chi \\to \\mathbb{R}$ is given by\n\n$$\n\\begin{align*}\n\\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}(Y - m_X),\\ x \\in \\chi, \\\\\n\\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}k_{Xx'},\\ x,x' \\in \\chi,\n\\end{align*}\n$$\n\nwhere $k_{Xx} = k_{xX}^\\top = (k(x_1, x), \\dots, k(x_n, x))^\\top$.\n\nThis is interesting, because these are closed-form expressions, and notably, we haven't made use of Bayes's Rule. Assuming the kernel and mean functions aren't expensive or difficult to evaluate, the computation is just linear algebra.\n\n### Drawing from a Gaussian Process\n\nLet's assume we want to model a noisy function $f$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nf <- function(x) sin(3*x) + sin(7*x)\nk <- function(x_i, x_j, sigma_f = 2, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nn <- 200\nx <- seq(-5, 5, length.out = n)\nf_x <- f(x)\n\nsigma <- 0.4\nepsilon <- rnorm(n, 0, sigma)\ny <- f_x + epsilon\n\nd <- tibble(x, f_x, y)\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\") +\n  geom_point(aes(y = y))\n```\n\n::: {.cell-output-display}\n![](overview_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_star <- 100\nt_index <- sample(1:200, size = n_star, replace = FALSE)\nx_star <- x[t_index]\ny_star <- y[t_index]\nm_x <- rep(0, 200)\n\nK <- matrix(0, n, n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    K[i, j] <- k(x[i], x[j])\n  }\n}\n\nK_star2 <- matrix(0, n_star, n_star)\nfor (i in 1:n_star) {\n  for (j in 1:n_star) {\n    K_star2[i, j] <- k(x_star[i], x_star[j])\n  }\n}\n\nK_star <- matrix(0, n_star, n)\nfor (i in 1:n_star) {\n  for (j in 1:n) {\n    K_star[i, j] <- k(x_star[i], x[j])\n  }\n}\n\nf_post_star <- K_star %*% solve(K + sigma^2 * diag(n)) %*% (y - m_x)\ncov_post_star <- K_star2 - K_star %*% solve(K + sigma^2 * diag(n)) %*% t(K_star)\n\nz_star <- MASS::mvrnorm(50, f_post_star, cov_post_star)\n\ndraws <- as_tibble(t(z_star)) |>\n  mutate(x = x_star, y = f(x_star)) |>\n  pivot_longer(V1:V50) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n```\n\n\n:::\n\n```{.r .cell-code}\npost_mean <- draws |>\n  group_by(x) |>\n  summarize(post_mean = mean(value))\n\ndraws |>\n  ggplot(aes(x = x)) +\n  geom_line(aes(y = value, group = name), alpha = 0.1) +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_line(data = post_mean, aes(x = x, y = post_mean), color = \"orange\") +\n  geom_point(data = post_mean, aes(x = x, y = post_mean), color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](overview_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n### Citations (WIP)\n\n- https://juanitorduz.github.io/gaussian_process_reg/\n\n- https://www.cs.toronto.edu/~duvenaud/cookbook/\n\n- https://gregorygundersen.com/blog/2019/06/27/gp-regression/\n\n- https://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/\n",
    "supporting": [
      "overview_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
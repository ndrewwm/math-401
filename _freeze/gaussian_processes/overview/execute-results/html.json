{
  "hash": "880ff29c55d1297911c5701e16fac9af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Overview of Gaussian Processes\n---\n\n\n\nThis page relies heavily on Kanagawa, Hennig, Sejdinovic, and Sriperumbudur (2018).\n\n### Definition 2.2: Gaussian Process\n\nLet $\\chi$ be a nonempty set, $k: \\chi \\times \\chi \\to \\mathbb{R}$ a positive definite kernel and $m: \\chi \\to \\mathbb{R}$ be any real-valued function. Then a random function $f: \\chi \\to \\mathbb{R}$ is said to be a Gaussian process (GP) with *mean function* $m$ and *covariance kernel* $k$, denoted by $\\mathcal{GP}(m, k)$, if the following holds: For any finite set $X = (x_1, \\dots, x_n) \\subset \\chi$ of any size $n \\in \\mathbb{N}$, the random vector\n\n$$\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n$$\n\nfollows the multivariate normal distribution $\\mathcal{N}(m_X, k_{XX})$ with covariance matrix $k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}$ and mean vector $m_X = (m(x_1), \\dots, m(x_n))^\\top$.\n\n### Gaussian Process Regression\n\n- also called *kriging* or *Wiener-Kolmogorov prediction*\n\nRegression is the task of estimating of an unknown function $f$ based on a provided set of *training data*, $(X, Y)$, where $X = (x_1, \\dots, x_n)^\\top$ and $Y = (y_1, \\dots, y_n)^\\top$ are random vectors ($x_i$ and $y_i$ are *realizations*, collected by the experimenter). Regression assumes the presence of *noise* denoted by $\\epsilon$, which completes the additive model:\n\n$$\ny_i = f(x_i) + \\epsilon.\n$$\n\nIt's typically assumed that $\\epsilon$ is normally distributed with mean 0, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$.\n\nGaussian process regression is a Bayesian approach that uses a GP as a prior distribution for $f$.\n\n|                               | Prior                | Posterior |\n| :---------------------------- | :------------------- | :-------- |\n| Hyperparameter: kernel        | $k$                  | $\\bar{k}$ |\n| Hyperparameter: mean function | $m$                  | $\\bar{m}$ |\n| Distribution                  | $\\mathcal{GP}(m, k)$ | $\\mathcal{GP}(\\bar{m}, \\bar{k})$ |\n\nThe authors show that the *posterior distribution* $f|Y \\sim \\mathcal{GP}(\\bar{m}, \\bar{k})$ where $\\bar{m} : \\chi \\to \\mathbb{R}$ and $\\bar{k}: \\chi \\times \\chi \\to \\mathbb{R}$ is given by\n\n$$\n\\begin{align*}\n\\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}(Y - m_X),\\ x \\in \\chi, \\\\\n\\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}k_{Xx'},\\ x,x' \\in \\chi,\n\\end{align*}\n$$\n\nwhere $k_{Xx} = k_{xX}^\\top = (k(x_1, x), \\dots, k(x_n, x))^\\top$.\n\nThis is interesting, because these are closed-form expressions, and notably, we haven't made use of Bayes's Rule. Assuming the kernel and mean functions aren't expensive or difficult to evaluate, the computation is just linear algebra.\n\n### Drawing from a Gaussian Process\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nset.seed(123)\n\nf <- function(x) sin(3*x) + sin(7*x)\nk <- function(x_i, x_j, sigma_f = 0.4, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nn <- 400\nx <- seq(-5, 5, length.out = n)\nf_x <- f(x)\n\nsigma <- 0.4\nepsilon <- rnorm(n, 0, sigma)\ny <- f_x + epsilon\n\nd <- tibble(x, f_x, y)\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\") +\n  geom_point(aes(y = y))\n```\n\n::: {.cell-output-display}\n![](overview_files/figure-html/unnamed-chunk-1-1.png){width=100%}\n:::\n:::\n\n\n\nHere we'll compute the posterior distribution over the whole range $[-5, 5]$, using $n = 100$ training data points.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100\nind_train <- sample(1:400, n, replace = FALSE)\nx_train <- x[ind_train]\ny_train <- y[ind_train]\n\ncompute_covmat <- function(k, x, y) {\n  n_x <- length(x)\n  n_y <- length(y)\n  K <- matrix(0, n_x, n_y)\n  for (i in 1:n_x) {\n    for (j in 1:n_y) {\n      K[i, j] <- k(x[i], y[j])\n    }\n  }\n  return(K)\n}\n\nk_XX <- compute_covmat(k, x_train, x_train)\nk_xx <- compute_covmat(k, x, x)\nk_xX <- compute_covmat(k, x, x_train)\n\nm_post <- k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% y_train\ncov_post <- k_xx - k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% t(k_xX)\n```\n:::\n\n\n\nHere `m_post` represents the mean vector of the posterior distibution, and `cov_post` represents the posterior's covariance matrix.\n\n::: {.column-margin}\nIn Kanagawa et al. (2018), they note that the final term in the posterior mean function $\\bar{m}$ should be something like $(\\mathbf{y} - m_\\mathbf{x})$ where $m_\\mathbf{x} = [m(x_1), \\dots, m(x_N)]^\\top$. I've omitted the subtraction given that $m(\\mathbf{x}) = \\mathbf{0}$.\n:::\n\nBelow in blue is the actual function $f$, with 50 draws (grey) from the posterior distribution. The posterior mean of these draws is plotted as the orange line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf_post <- MASS::mvrnorm(50, m_post, cov_post)\n\ndraws <- as_tibble(t(f_post)) |>\n  mutate(x = x, y = f(x)) |>\n  pivot_longer(V1:V50)\n\npost_mean <- draws |>\n  group_by(x) |>\n  summarize(post_mean = mean(value))\n\ndraws |>\n  ggplot(aes(x = x)) +\n  geom_line(aes(y = value, group = name), alpha = 0.1) +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_line(data = post_mean, aes(x = x, y = post_mean), color = \"orange\")\n```\n\n::: {.cell-output-display}\n![](overview_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n### Citations (WIP)\n\n- https://juanitorduz.github.io/gaussian_process_reg/\n\n- https://www.cs.toronto.edu/~duvenaud/cookbook/\n\n- https://gregorygundersen.com/blog/2019/06/27/gp-regression/\n\n- https://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/\n",
    "supporting": [
      "overview_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
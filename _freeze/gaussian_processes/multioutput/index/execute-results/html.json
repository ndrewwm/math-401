{
  "hash": "87fc0cf1ae2b87474dccc84b05be49c9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Multioutput Gaussian Process Regression (GPR)\nbibliography: multioutput.bib\nfilters:\n  - include-code-files\n---\n\n\n\n\n## Motivation: visualizing vector fields\n\nElsewhere, I've discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data $\\mathbf{X} \\in \\mathbb{R}^{N \\times P}$ is used to model $\\mathbf{y} \\in \\mathbb{R}^N$:\n$$\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{X}) + \\mathbf{\\epsilon} \\text{ where } \\\\\nf(\\mathbf{X}) &\\sim \\mathcal{N}_N(\\mathbf{0}, \\mathbf{K}), \\\\\n\\mathbf{\\epsilon} &\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_y), \\text{ and } \\\\\n\\mathbf{K} &= [k(x_i, x_j)]_{ij}^N.\n\\end{align*}\n$$\n\n::: {.column-margin}\nThroughout this page, I'll be using the subscript after $\\mathcal{N}$ to indicate we're looking at a multivariate normal distribution, and to show the dimension of the vectors that the distribution produces.\n:::\n\nWhile interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we'll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the `gcookbook` package:\n\n::: {.column-margin}\nThis example and dataset is drawn from *The R Graphics Cookbook* [@chang2018r], specifically chapter 13.12, on creating vector fields.\n:::\n\n> This data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.\n\nBelow I've plotted the x and y components of the storm's velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm's winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector:\n$$\n\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}.\n$$\n\nVelocity captures both the *direction* of the wind, and its *magnitude* (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we're looking at, such as the storm's spiral shape.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps <- sqrt(.Machine$double.eps)\n\n# Keep a subset in the middle of the the z-axis\nd_isabel <- isabel |>\n  filter(z == 10.035) |>\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n <- function(x, by = 4) {\n  x <- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x <- every_n(unique(isabel$x))\nkeep_y <- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd <- d_isabel |>\n  filter(x %in% keep_x, y %in% keep_y) |>\n  mutate(index = 1:n())\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 <- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nA similar consideration applies to the task of predicting velocity using statistical models. A \"naive\" approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, $f_1: \\mathbb{R}^2 \\to \\mathbb{R}$ and $f_2: \\mathbb{R}^2 \\to \\mathbb{R}$. This amounts to treating $v_x$ and $v_y$ as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.\n\nAn improved approach would allow us to incorporate natural information on how our observed target values (each $v_x$ and $v_y$) might interrelate. The approach should also (ideally) account for the fact that we're trying to predict a vector at a specific point. In other words, we'd like a single function, whose outputs are vector-valued:\n\n$$\n\\underbrace{\\begin{aligned}[c]\nv_x = f_1\\Bigl((x, y)^\\top \\Bigr) + \\epsilon \\\\\nv_y = f_2\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Naive approach}}\n\\qquad\\longrightarrow\\qquad\n\\underbrace{\\begin{aligned}[c]\n\\begin{bmatrix}v_x \\\\ v_y\\end{bmatrix} = f\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Vector-valued approach}}\n$$\n\nThese characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as \"multioutput\", \"multitask\", \"cokriging\", or \"vector-valued\" GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics [@alvarez2012kernels]. In the following section, we'll cover the theory behind the approach as it applies to our example data.\n\n## Theory and notation\n\nWe are working in the space of the two-dimensional real numbers, $\\mathbb{R}^2$. Let $S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)$ be our set of *training data*, where $x_i, y_i \\in \\mathbb{R}^2$. It may be convenient to note $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times 2}$, and we may refer to $\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N}$ and $\\mathbf{x} = \\text{vec}\\ X \\in \\mathbb{R}^{2N}$. We will use $\\mathbf{X}_* \\in \\mathbb{R}^{M \\times 2}$ to denote a set of *test points*, for which we want to generate predictions.\n\nWe are engaged in a *regression* task, i.e., we're attempting to learn the functional relationship $f$ between $\\mathbf{X}$ and $\\mathbf{Y}$, with an assumption that this relationship has been corrupted to some degree by noise. For the following, I've adapted notation from Chapter 2.2 in the well-known Williams and Rasmussen text to the context of our multidimensional $\\mathbf{Y}$ [@williams2006gaussian]. Setting up the problem, we have\n$$\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\\\\nf &\\sim \\mathcal{GP}(m, k) \\\\\nf(\\mathbf{x}) &\\sim \\mathcal{N}_{2N}(\\mathbf{0}, \\mathbf{K}_{XX})\n\\end{align*}\n$$ {#eq-problem-stmt}\n\nTo parse these statements, we assume $f$ is drawn from a Gaussian Process with mean function $m$ and covariance (kernel) function $k$. Thus, our observations $\\mathbf{y} = f(\\mathbf{x})$ have a multivariate normal probability distribution, parameterized by a *covariance matrix* $\\mathbf{K}_{XX}$. By convention, we assume the mean vector is $\\mathbf{0}$ (implying that $m$ is the zero function).\n\nThe joint distribution of $\\mathbf{y}$ and our predictions for the test point(s) $\\mathbf{y}_*$ is also described by a multivariate normal distribution, specified as:\n$$\n\\begin{align*}\n\\begin{bmatrix}\n  \\mathbf{y} \\\\\n  \\mathbf{y_*}\n\\end{bmatrix} &\\sim \\mathcal{N}_{2N + 2M}\\Biggl( \\mathbf{0}, \\begin{bmatrix} \n  \\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N & \\mathbf{K}_{XX_*} \\\\\n  \\mathbf{K}_{X_*X} & \\mathbf{K}_{X_* X_*}\n\\end{bmatrix} \\Biggr) \\\\\n\\mathbf{V} &= \\begin{bmatrix} \\sigma^2_1 & 0 \\\\ 0 & \\sigma^2_2 \\end{bmatrix}\n\\end{align*}\n$$ {#eq-joint-distribution-for-y-and-ystar}\n\nHere, $\\sigma^2_1, \\sigma^2_2 > 0$ represent the independent and additive noise associated with each of our outcome's components. The $\\otimes$ symbol denotes the Kronecker product. Each of the sub-matrices, such as $\\mathbf{K}_{XX_*}$, are defined below in @eq-KxX. From this joint distribution, we can derive the conditional distribution for $\\mathbf{y}_*$:\n\n$$\n\\begin{align*}\n\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*} &\\sim \\mathcal{N}_{2M}(\\hat{\\mu},\\ \\hat{\\mathbf{\\Sigma}}) \\\\\n\\hat{\\mathbf{\\mu}} &= \\mathbb{E}[\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*}] = \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1} \\mathbf{y} \\\\\n\\hat{\\mathbf{\\Sigma}} &= \\mathbf{K}_{X_*X_*} - \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1}\\mathbf{K}_{XX_*} \\\\\n\\end{align*}\n$$ {#eq-conditional-distribution-for-ystar}\n\nWe will now work through how each piece of our distribution is defined. Let $k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ be a (scalar) kernel function, defined as\n$$\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(-\\frac{1}{2\\rho^2} \\| x_i - x_j \\|^2 \\Bigl),\n$$ {#eq-k}\n\nwhere $\\alpha, \\rho > 0$, and $\\| \\cdot \\|$ is the Euclidean (L2) norm. This is also known as the squared exponential function, or the radial basis function. The parameter $\\alpha$ controls ..., while $\\rho$ controls the length-scale. Let $k(\\mathbf{X}, \\mathbf{X}) \\in \\mathbb{R}^{N \\times N}$, the covariance matrix between all points in $\\mathbf{X}$, be defined as\n$$\nk(\\mathbf{X}, \\mathbf{X}) = (k(x_i, x_j))_{i,j}^N = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n$$ {#eq-kXX}\n\nLet $\\mathbf{B} \\in \\mathbb{R}^{2 \\times 2}$, the matrix of similarities between the outputs $\\mathbf{Y}$, be defined as\n$$\n\\mathbf{B} = \\text{corr}(\\mathbf{Y}) = \\Biggl( \\frac{\\langle \\mathbf{y}_i - \\bar{\\mathbf{y}}_i,\\ \\mathbf{y}_j - \\bar{\\mathbf{y}}_j \\rangle}{\\| \\mathbf{y}_i - \\bar{\\mathbf{y}}_i \\| \\| \\mathbf{y}_j - \\bar{\\mathbf{y}}_j \\| } \\Biggr)^{2,2}_{i,j},\n$$ {#eq-B}\n\nwhere $\\mathbf{y}_i$ is the i-th column of $\\mathbf{Y}$ and $\\bar{\\mathbf{y}}_i$ is the arithmetic mean of $\\mathbf{y}_i$. The operation in the numerator, $\\langle \\cdot, \\cdot \\rangle$, denotes the standard inner product. Each entry $b_{i,j}$ of $\\mathbf{B}$ is the Pearson correlation coefficient, as estimated from our training data.\n\nLet $\\mathbf{K}_{XX} \\in \\mathbb{R}^{2N \\times 2N}$ be defined as\n$$\n\\mathbf{K}_{XX} = \\mathbf{B} \\otimes k(\\mathbf{X}, \\mathbf{X}).\n$$ {#eq-KXX}\n\nAlvarez et al. identifies this approach of combining the kernel matrix with a similarity matrix via a Kronecker product as the \"Intrinsic Coregionalization Model\"; see equation (21) in section 4.2.2 [@alvarez2012kernels]. They describe this approach as being more restrictive, but simpler (to implement, I assume).\n\nWe can now define the other pieces needed to establish the covariance matrix used for the joint distribution of $\\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{y}_* \\end{bmatrix}$:\n\n$$\n\\begin{align*}\n  \\mathbf{K}_{X_*X_*} &= \\mathbf{B} \\otimes k(\\mathbf{X}_*, \\mathbf{X}_*) = \\mathbf{B} \\otimes (k(x_{*i}, x_{*j}))_{i,j=1}^{M} \\in \\mathbb{R}^{2M \\times 2M} \\\\\n  \\mathbf{K}_{X_*X} &= \\mathbf{B} \\otimes k(\\mathbf{X}_*, \\mathbf{X}) = \\mathbf{B} \\otimes (k(x_{*i}, x_j))_{i,j=1}^{M,N} \\in \\mathbb{R}^{2M \\times 2N} \\\\\n  \\mathbf{K}_{XX_*} &= \\mathbf{K}_{X_*X}^\\top \\in \\mathbb{R}^{2N \\times 2M}.\n\\end{align*}\n$$ {#eq-KxX}\n\n## Training and test data\n\nHere, using the Hurricane Isabel data visualized above, we generate $S$, defined as a 30% random sample of observations. Here `d` is our subset of the `isabel` dataset, at `z = 10.035`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up the training data\nS <- d |>\n  slice_sample(n = floor(0.3 * nrow(d))) |>\n  select(x, y, vx, vy, index)\n\nX <- S |> select(x, y) |> as.matrix()\nY <- S |> select(vx, vy) |> as.matrix()\ny <- as.vector(Y)\nB <- cor(Y)\nN <- nrow(X)\n\n# Pull out test cases\nZ <- d |>\n  anti_join(S, by = \"index\") |>\n  select(x, y, vx, vy, index)\n\nX_star <- Z |> select(x, y) |> as.matrix()\n```\n:::\n\n\n\n\n## Kernel functions\n\nThis section defines R functions used to compute $\\mathbf{K}_{XX}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#' Squared exponential (scalar) kernel function\n#' \n#' @param xi numeric vector\n#' @param xj numeric vector\n#' @param alpha scale, alpha > 0\n#' @param rho length-scale, rho > 0\n#' @return numeric\nk <- function(xi, xj, alpha = 1, rho = 1) {\n  alpha^2 * exp(-norm(xi - xj, type = \"2\")^2 / (2 * rho^2))\n}\n\n#' Compute the kernel matrix, given a set of input vectors\n#' \n#' @param X numeric matrix of dimensions N x P, where N is observations and P is components\n#' @param alpha numeric, variance scale, alpha > 0\n#' @param rho numeric, length-scale, rho > 0\n#' @param err numeric, err > 0\n#' @return N x N matrix\nk_XX <- function(X, alpha = 1, rho = 1, err = eps) {\n  N <- nrow(X)\n  K <- matrix(0, N, N)\n  for (i in 1:N) {\n    for (j in 1:N) {\n      K[i, j] <- k(X[i, ], X[j, ], alpha, rho)\n    }\n  }\n\n  if (!is.null(err)) {\n    K <- K + diag(err, ncol(K))\n  }\n\n  K\n}\n\n#' Compute the covariance between two sets of vectors\n#'\n#' @param x numeric matrix, N x P\n#' @param X numeric matrix, M x P\n#' @param alpha numeric, variance scale, alpha > 0\n#' @param rho numeric, length-scale, rho > 0\n#' @return A numeric matrix, N x M\nk_xX <- function(x, X, alpha = 1, rho = 1) {\n  N <- nrow(x)\n  M <- nrow(X)\n\n  K <- matrix(0, N, M)\n  for (i in 1:N) {\n    for (j in 1:M) {\n      K[i, j] <- k(x[i, ], X[j, ], alpha, rho)\n    }\n  }\n\n  K\n}\n```\n:::\n\n\n\n\n## Hyperparameter Inference\n\n### Estimation via Stan\n\nThis section contains the `rstan` code used to estimate hyperparameter values using the probabilistic programming language, [Stan](https://mc-stan.org/). Within the scoped blocks of the Stan program we establish the model for $f(x)$ discussed in @eq-problem-stmt, and attempt to find plausible values for $\\rho$ and $\\alpha$ via Hamiltonian Monte-Carlo (HMC). We place prior distributions on each hyperparameter. Constraints are used within Stan to require these parameters be > 0.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(rstan)\n\nfit_stan <- function(filepath, model_name = \"\", data, verbose = FALSE) {\n  stan(\n    file = filepath,\n    model_name = model_name,\n    data = data,\n    chains = 2,\n    iter = 800,\n    warmup = 400,\n    seed = 123,\n    cores = 2,\n    verbose = verbose\n  )\n}\n\n# ICM\n# fit_stan(\"./icm.stan\", \"Isabel GPR, ICM: rho\", lst(N, X, Y, B))\n# fit_stan(\"./icm_w_alpha.stan\", \"Isabel GPR, ICM: alpha & rho\", lst(N, X, Y, B))\n# fit_stan(\"./icm_w_alpha_and_sigma.stan\", \"Isabel GPR, ICM: alpha, rho, & sigma\", lst(N, X, Y, B))\n\n# Independent GPs\n# fit_stan(\"./icm.stan\", \"Isabel GPR, Indep: rho\", lst(N, X, Y, B = diag(1, 2)))\n# fit_stan(\"./icm_w_alpha.stan\", \"Isabel GPR, Indep: alpha & rho\", lst(N, X, Y, B = diag(1, 2)))\nfit_stan(\"./icm_w_alpha_and_sigma.stan\", \"Isabel GPR, Indep: alpha, rho, & sigma\", lst(N, X, Y, B = diag(1, 2)))\n```\n:::\n\n\n\n\n### Evaluation\n\nThis section defines R functions used to compute the analytical posterior distribution for each parameter specification, and summarize the results with a figure and performance with respect to RMSE.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_posterior <- function(X, y, B, Z, alpha = 1, rho = 1, sigma = c(1e-9, 1e-9)) {\n  N <- nrow(X)\n  M <- nrow(Z)\n\n  V <- diag(sigma^2, 2)\n  K_XX <- kronecker(B, k_XX(X, alpha, rho, NULL)) + kronecker(V, diag(N))\n  K_ZX <- kronecker(B, k_xX(Z, X, alpha, rho))\n  K_ZZ <- kronecker(B, k_XX(Z, alpha, rho, NULL))\n\n  mu <- K_ZX %*% solve(K_XX) %*% y\n  mu <- matrix(mu, M, 2)\n  Sigma <- K_ZZ - K_ZX %*% solve(K_XX) %*% t(K_ZX)\n\n  return(lst(mu, Sigma))\n}\n\nrmse <- function(y, yhat) {\n  sqrt(1/length(y) * sum((yhat - y)^2))\n}\n\nget_evaluation <- function(mu, Z, S) {\n  out <- Z |>\n    bind_cols(as_tibble(mu) |> set_names(c(\"vx_h\", \"vy_h\"))) |>\n    select(x, y, vx, vy, vx_h, vy_h, index) |>\n    bind_rows(S) |>\n    mutate(\n      col = factor(\n        as.numeric(index %in% S$index),\n        levels = 0:1,\n        labels = c(\"Test\", \"Train\")\n      )\n    )\n\n  fig <- ggplot() +\n    geom_segment(\n      data = out,\n      aes(x, y, xend = x + vx/50, yend = y + vy/50),\n      arrow = arrow(length = unit(0.1, \"cm\")),\n      linewidth = 0.25,\n      alpha = 0.3\n    ) +\n    geom_segment(\n      data = filter(out, col == \"Train\"),\n      aes(x, y, xend = x + vx/50, yend = y + vy/50, color = col),\n      arrow = arrow(length = unit(0.1, \"cm\")),\n      linewidth = 0.25\n    ) +\n    geom_segment(\n      data = filter(out, col == \"Test\"),\n      aes(x, y, xend = x + vx_h/50, yend = y + vy_h/50, color = col),\n      arrow = arrow(length = unit(0.1, \"cm\")),\n      linewidth = 0.25\n    ) +\n    scale_color_scico_d(palette = \"imola\", name = \"\", direction = -1) +\n    labs(x = \"Longitude\", y = \"Latitude\") +\n    theme(legend.position = \"bottom\")\n\n  perf <- out |>\n    filter(col == \"Test\") |>\n    summarize(\n      rmse_vx = rmse(vx, vx_h),\n      rmse_vy = rmse(vy, vy_h)\n    )\n\n  return(lst(fig, perf))\n}\n```\n:::\n\n\n\n\n### ICM\n\n::: {.panel-tabset}\n\n#### Naive Parameters\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nâ„¹ Using compatibility `.name_repair`.\n```\n\n\n:::\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 1.00 | 1.00 | 1e-9 | 1e-9 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 2.718579| 3.171989|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n#### Inference on $\\rho$\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 1.00 | 0.84 | 1e-9 | 1e-9 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 2.732739| 3.199853|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n```{.stan include=\"./icm.stan\"}\n```\n\n#### Inference on $\\alpha$ and $\\rho$\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 8.34 | 0.98 | 1e-9 | 1e-9 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 2.687784| 3.119549|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n```{.stan include=\"./icm_w_alpha.stan\"}\n```\n\n#### Inference on $\\alpha$, $\\rho$, and $\\sigma$\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 8.34 | 0.98 | 6.95 | 9.58 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 8.208218| 12.03744|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n```{.stan include=\"./icm_w_alpha_and_sigma.stan\"}\n```\n\n:::\n\n### Independent GPs\n\n::: {.panel-tabset}\n\n#### Naive Parameters\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 1.00 | 1.00 | 1e-9 | 1e-9 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 2.718579| 3.171989|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n#### Inference on $\\rho$\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 1.00 | 0.85 | 1e-9 | 1e-9 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 2.713324| 3.165199|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n\n\n#### Inference on $\\alpha$ and $\\rho$\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 8.12 | 0.99 | 1e-9 | 1e-9 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 2.702115| 3.144015|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n#### Inference on $\\alpha$, $\\rho$, and $\\sigma$\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n| $\\alpha$ | $\\rho$ | $\\sigma_1$ | $\\sigma_2$ |\n| :------- | :----- | :--------- | :--------- |\n| 8.12 | 0.99 | 6.99 | 9.68 |\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  rmse_vx|  rmse_vy|\n|--------:|--------:|\n| 3.232751| 5.687826|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
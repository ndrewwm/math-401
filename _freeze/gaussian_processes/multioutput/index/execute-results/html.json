{
  "hash": "28422244d081845f5d6c12ec9c3be8b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Multioutput GPR\nbibliography: multioutput.bib\n---\n\n\n\n<!--\n- https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf\n- https://ruglio.github.io/Web/blog/dkl/s2/\n\n- Liu, Cai, and Ong (2018) https://www.sciencedirect.com/science/article/abs/pii/S0950705117306123\n\n> Existing MOGPs can in general be classified into two categories: (1) symmetric MOGPs and (2) asymmetric MOGPs. Symmetric MOGPs use a symmetric dependency structure to capture the output correlations and approximate the T outputs simultaneously. Therefore, these MOGPs usually have an integrated modeling process, i.e., fusing all the information in an entire covariance matrix, which leads to bidirectional information transfer between the outputs. **Typically, the symmetric MOGPs attempt to improve the predictions of all the outputs in symmetric scenarios, where the outputs are of equal importance and have roughly equivalent training information.**\n\nThey use the following notation.\n\nTraining data, $\\mathcal{D} = \\{X, \\mathbf{y}\\}$:\n$$\n\\begin{align*}\nX &= \\{ \\mathbf{x}_{t,i} | t = 1, \\dots, T; i = 1, \\dots, n_t \\} \\in \\mathbb{R}^{N \\times d} \\\\\n\\mathbf{y} &= \\{ y_{t,i} = y_t(\\mathbf{x}_{t,i}) | t = 1, \\dots, T; i = 1, \\dots, n_t \\} \\in \n\\end{align*}\n$$\n\n- $N = \\sum.{t = 1}^T n_t$\n\n- $X_t = \\{ x_{t,1}, \\dots, x_{t, n_t} \\}^\\top$ is the training data corresponding to output $f_t$\n\n-->\n\n## Multitask Gaussian Process Prediction\n\nThis section draws from a well-cited paper [@bonilla2007multi] on the topic of *\"multitask Gaussian process prediction\".* Christopher Williams, along with his coauthor, Carl Rasmussen, is the same behind another well-cited textbook, *Gaussian processes for machine learning* [@williams2006gaussian].  Bonilla, Chai, & Williams use the following notation to set up the methodology.\n\nLet $X$ be a set of $N$ distinct inputs: $X = \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N$. Let $Y$ be a set of $N$ responses to $M$ tasks: $\\mathbf{y} = (y_{11}, \\dots, y_{N1}, \\dots, y_{N2}, \\dots, y_{1M}, \\dots, y_{NM} )^\\top$. The variable/value written as $\\mathbf{y}_{il}$ is the response for observation $i$ on task $l$. They also define $Y \\in \\mathbb{R}^{N \\times M}$ such that $\\mathbf{y} = \\text{vec}\\ Y$. Each $\\mathbf{x}_i$ represents the set of inputs corresponding to $\\mathbf{y}_i$.\n\n## Visualizing vector fields\n\nThis example and dataset is drawn from *The R Graphics Cookbook* [@chang2018r], specifically chapter 13.12, on creating vector fields.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(gcookbook)\nset.seed(123)\n\neps <- sqrt(.Machine$double.eps)\nglimpse(isabel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 156,250\nColumns: 8\n$ x     <dbl> -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83,…\n$ y     <dbl> 41.70000, 41.55571, 41.41142, 41.26713, 41.12285, 40.97856, 40.8…\n$ z     <dbl> 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0…\n$ vx    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ vy    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ vz    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ t     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ speed <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Keep a subset in the middle of the the z-axis\nd <- isabel |>\n  filter(z == 10.035) |>\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n <- function(x, by = 4) {\n  x <- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x <- every_n(unique(isabel$x))\nkeep_y <- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd_sub <- filter(d, x %in% keep_x, y %in% keep_y)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Make the plot with the subset, and use an arrowhead 0.1 cm long\nggplot(d_sub, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.25\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_sub <- mutate(d_sub, index = 1:n())\n\nX <- d_sub |>\n  slice_sample(n = 200) |>\n  select(x, y, vx, vy, index)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrbf <- function(xi, xj, alpha = 1, rho = 1) {\n  alpha^2 * exp(-norm(xi - xj, type = \"2\") / (2 * rho^2))\n}\n\nk_XX <- function(X, err = eps) {\n  N <- nrow(X)\n  K <- matrix(0, N, N)\n  for (i in 1:N) {\n    for (j in 1:N) {\n      K[i, j] <- rbf(X[i, ], X[j, ])\n    }\n  }\n\n  if (!is.null(err)) {\n    K <- K + diag(err, ncol(K))\n  }\n\n  K\n}\n\nk_xX <- function(x, X) {\n  N <- nrow(x)\n  M <- nrow(X)\n\n  K <- matrix(0, N, M)\n  for (i in 1:N) {\n    for (j in 1:M) {\n      K[i, j] <- rbf(x[i, ], X[j, ])\n    }\n  }\n\n  K\n}\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
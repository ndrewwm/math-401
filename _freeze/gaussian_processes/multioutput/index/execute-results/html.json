{
  "hash": "f2c5efa48d02831368039c3b3fff318d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Multioutput GPR\nbibliography: multioutput.bib\n---\n\n\n\n\n# Multioutput Gaussian Process Regression (GPR)\n\n## Motivation: visualizing vector fields\n\nElsewhere, I've discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data $\\mathbf{X} \\in \\mathbb{R}^{N \\times P}$ is used to model $\\mathbf{y} \\in \\mathbb{R}^N$:\n$$\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{X}) + \\mathbf{\\epsilon} \\text{ where } \\\\\nf(\\mathbf{X}) &\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}), \\\\\n\\mathbf{\\epsilon} &\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_y), \\text{ and } \\\\\n\\mathbf{K} &= [k(x_i, x_j)]_{ij}^N.\n\\end{align*}\n$$\n\nWhile interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we'll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the `gcookbook` package:\n\n::: {.column-margin}\nThis example and dataset is drawn from *The R Graphics Cookbook* [@chang2018r], specifically chapter 13.12, on creating vector fields.\n:::\n\n> This data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.\n\nBelow I've plotted the x and y components of the storm's velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm's winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector:\n$$\n\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}.\n$$\n\nVelocity captures both the *direction* of the wind, and its *magnitude* (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we're looking at, such as the storm's spiral shape.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps <- sqrt(.Machine$double.eps)\n\n# Keep a subset in the middle of the the z-axis\nd_isabel <- isabel |>\n  filter(z == 10.035) |>\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n <- function(x, by = 4) {\n  x <- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x <- every_n(unique(isabel$x))\nkeep_y <- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd <- d_isabel |>\n  filter(x %in% keep_x, y %in% keep_y) |>\n  mutate(index = 1:n())\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 <- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nA similar consideration applies to the task of predicting velocity using statistical models. A \"naive\" approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, $f_1: \\mathbb{R}^2 \\to \\mathbb{R}$ and $f_2: \\mathbb{R}^2 \\to \\mathbb{R}$. This amounts to treating $v_x$ and $v_y$ as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.\n\nAn improved approach would allow us to incorporate natural information on how our observed target values (each $v_x$ and $v_y$) might interrelate. The approach should also (ideally) account for the fact that we're trying to predict a vector at a specific point. In other words, we'd like a single function, whose outputs are vector-valued:\n\n$$\n\\underbrace{\\begin{aligned}[c]\nv_x = f_1\\Bigl((x, y)^\\top \\Bigr) + \\epsilon \\\\\nv_y = f_2\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Naive approach}}\n\\qquad\\longrightarrow\\qquad\n\\underbrace{\\begin{aligned}[c]\n\\begin{bmatrix}v_x \\\\ v_y\\end{bmatrix} = f\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Vector-valued approach}}\n$$\n\nThese characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as \"multioutput\", \"multitask\", \"cokriging\", or \"vector-valued\" GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics [@alvarez2012kernels]. In the following section, we'll cover the theory behind the approach as it applies to our example data.\n\n## Theory and notation\n\nWe are working in the space of the two-dimensional real numbers, $\\mathbb{R}^2$. Let $S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)$ be our set of *training data*, where $x_i, y_i \\in \\mathbb{R}^2$. It may be convenient to note $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times 2}$, and we may refer to $\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N}$ and $\\mathbf{x} = \\text{vec}\\ X \\in \\mathbb{R}^{2N}$. We will use $\\mathbf{X}_* \\in \\mathbb{R}^{M \\times 2}$ to denote a set of *test points*, for which we want to generate predictions. As with the conventions for the training data, we will use $\\mathbf{x}_* = \\text{vec}\\ \\mathbf{X}_* \\in \\mathbb{R}^{2M}$.\n\nWe are engaged in a *regression* task, i.e., we're attempting to learn the functional relationship $f$ between $\\mathbf{X}$ and $\\mathbf{Y}$, with an assumption that this relationship has been corrupted to some degree by noise. To set up the problem, we have\n$$\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\\\\nf &\\sim \\mathcal{GP}(m, K) \\\\\nf(\\mathbf{x}) &\\sim \\mathcal{N}_{2N}(\\mathbf{0}, \\mathbf{K}_{XX})\n\\end{align*}\n$$\n\nTo parse these statements, we assume $f$ is drawn from a Gaussian Process with mean function $m$ and covariance (kernel) function $K$. Thus, our observations $\\mathbf{y} = f(\\mathbf{x})$ have a multivariate normal probability distribution, parameterized by a *covariance matrix* $\\mathbf{K}_{XX}$. By convention, we assume the mean vector is $\\mathbf{0}$.\n\n$$\n\\begin{align*}\n\\begin{bmatrix}\n  \\mathbf{y} \\\\\n  \\mathbf{f_*}\n\\end{bmatrix} &\\sim \\mathcal{N}_{2N + 2M}\\Biggl( \\mathbf{0}, \\begin{bmatrix} \n  \\mathbf{K}_{XX} & \\mathbf{K}_{XX_*} \\\\\n  \\mathbf{K}_{X_*X} & \\mathbf{K}_{X_* X_*}\n\\end{bmatrix} \\Biggr) \\\\\n\\mathbf{f}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*} &\\sim \\mathcal{N}_{2M}(\\bar{\\mathbf{f}}_*,\\ \\text{cov}(\\mathbf{f}_*)) \\\\\n\\bar{\\mathbf{f}}_* &= \\mathbb{E}[\\mathbf{f}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*}] = \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1} \\mathbf{y} \\\\\n\\text{cov}(\\mathbf{f}_*) &= \\mathbf{K}_{X_*X_*} - \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1}\\mathbf{K}_{XX_*} \\\\\n\\mathbf{V} &= \\begin{bmatrix} \\sigma^2_1 & 0 \\\\ 0 & \\sigma^2_2 \\end{bmatrix}\n\\end{align*}\n$$\n\nHere I'm trying to apply the function-space notation from Williams & Rasmussen, applied to the multi-dimensional nature of $\\mathbf{Y}$ [@williams2006gaussian].\n\n<!-- We start with the assumption that $\\mathbf{f}$ comes from a Gaussian Process, written as $\\mathbf{f} \\sim \\mathcal{GP}(\\mathbf{m}, \\mathbf{K})$, where $\\mathbf{m}$ is a *mean* function and $\\mathbf{K}$ is a *covariance* (or kernel) function. This assumption means that we believe for a (finite) set $\\mathbf{X}$ of $N$ observations, $\\mathbf{f}(\\mathbf{X}) \\in \\mathbb{R}^{2N}$ is described by the multivariate normal distribution $\\mathbf{f}(\\mathbf{X}) \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{\\Sigma})$. Here, $\\mathbf{m}$ is a *mean* function, and $\\mathbf{\\Sigma}$ is a *covariance* (or kernel) function. -->\n\nWe will now work through how each component of our distribution is defined. Let $k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ be the (scalar) kernel function, defined as\n$$\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(-\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n$$ {#eq-k}\n\nwhere $\\alpha, \\rho > 0$; again, $x_i, x_j \\in \\mathbb{R}^2$. This is also known as the squared exponential function, or the radial basis function. The parameter $\\alpha$ controls ..., while $\\rho$ controls the length-scale. Let $\\mathbf{k}_{XX} \\in \\mathbb{R}^{N \\times N}$, the covariance matrix between all points in $\\mathbf{X}$, be defined as\n$$\n\\mathbf{k}_{XX} = (k(x_i, x_j))_{i,j}^N = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n$$ {#eq-kXX}\n\nLet the matrix of similarities between the outputs $\\mathbf{Y}$, $\\mathbf{B} \\in \\mathbb{R}^{2 \\times 2}$ be defined as\n$$\n\\mathbf{B} = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix} = \\frac{1}{N} \\mathbf{Y}^\\top \\mathbf{k}_{XX} \\mathbf{Y}.\n$$ {#eq-B}\n\nThe expression for $\\mathbf{B}$ comes from Bonilla et al. [@bonilla2007multi], in section 2.3 (the authors use the notation $K^f$ and $K^x$ instead of $\\mathbf{B}$ and $\\mathbf{k}_{XX}$, respectively).\n\nLet $\\mathbf{K}_{XX} \\in \\mathbb{R}^{2N \\times 2N}$ be defined as\n$$\n\\mathbf{K}_{XX} = \\mathbf{B} \\otimes \\mathbf{K}_{XX}\n$$ {#eq-Sigma}\n\nwhere $\\otimes$ is the Kronecker product, $\\mathbf{I}$ is the identity matrix, and $\\sigma^2_1, \\sigma^2_2 > 0$ represent the variances of the noise for each component of $\\mathbf{Y}$.\n\n<!-- FIXME: may not be needed -->\nFor a single new vector, $x' \\in \\mathbb{R}^2$, the predicted value for $y' \\in \\mathbb{R}^2$ is given by\n$$\ny' = (\\mathbf{B} \\otimes \\mathbf{k}_{x'})^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{y},\n$$ {#eq-yprime}\n\nwhere $\\mathbf{k}_{x'} \\in \\mathbb{R}^{N}$ is defined as\n$$\n\\mathbf{k}_{x'} = \\begin{bmatrix}\n  k(x', x_1) \\\\\n  k(x', x_2) \\\\\n  \\vdots \\\\\n  k(x', x_N)\n\\end{bmatrix}.\n$$ {#eq-k_xprime}\n\n# Training and test data\n\nHere, using the Hurricane Isabel data visualized above, we generate $S$, defined as a 30% random sample of observations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up the training data\nS <- d |>\n  slice_sample(n = floor(0.3 * nrow(d))) |>\n  select(x, y, vx, vy, index)\n\nX <- S |> select(x, y) |> as.matrix()\nY <- S |> select(vx, vy) |> as.matrix()\ny <- as.vector(Y)\n```\n:::\n\n\n\n\n# Kernel functions\n\nThis section defines R functions used to compute $\\mathbf{K}_{XX}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#' Squared exponential (scalar) kernel function\n#' \n#' @param xi numeric vector\n#' @param xj numeric vector\n#' @param alpha scale, alpha > 0\n#' @param rho length-scale, rho > 0\n#' @return numeric\nk <- function(xi, xj, alpha = 1, rho = 1) {\n  alpha^2 * exp(-norm(xi - xj, type = \"2\") / (2 * rho^2))\n}\n\n#' Compute the kernel matrix, given a set of input vectors\n#' \n#' @param X numeric matrix of dimensions N x P, where N is observations and P is components\n#' @param alpha numeric, variance scale, alpha > 0\n#' @param rho numeric, length-scale, rho > 0\n#' @param err numeric, err > 0\n#' @return N x N matrix\nk_XX <- function(X, alpha = 1, rho = 1, err = eps) {\n  N <- nrow(X)\n  K <- matrix(0, N, N)\n  for (i in 1:N) {\n    for (j in 1:N) {\n      K[i, j] <- k(X[i, ], X[j, ], alpha, rho)\n    }\n  }\n\n  if (!is.null(err)) {\n    K <- K + diag(err, ncol(K))\n  }\n\n  K\n}\n\n#' Compute the covariance between two sets of vectors\n#'\n#' @param x numeric matrix, N x P\n#' @param X numeric matrix, M x P\n#' @param alpha numeric, variance scale, alpha > 0\n#' @param rho numeric, length-scale, rho > 0\n#' @return A numeric matrix, N x M\nk_xX <- function(x, X, alpha = 1, rho = 1) {\n  N <- nrow(x)\n  M <- nrow(X)\n\n  K <- matrix(0, N, M)\n  for (i in 1:N) {\n    for (j in 1:M) {\n      K[i, j] <- k(x[i, ], X[j, ], alpha, rho)\n    }\n  }\n\n  K\n}\n```\n:::\n\n\n\n\n# Estimation\n\n$$\n\\ln p(y | X, \\alpha, \\rho) = -\\frac{1}{2}(N \\ln 2\\pi + \\ln |K| + y^\\top K^{-1}y)\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nll <- function(theta) {\n  alpha <- theta[1]\n  rho <- theta[2]\n  N <- nrow(X)\n\n  K_XX <- k_XX(X, alpha, rho)\n  B <- 1/N * t(Y) %*% solve(K_XX) %*% Y\n  K <- kronecker(K_XX, B)\n\n  invK <- solve(K)\n  detK <- det(K)\n  print(detK)\n  t(y) %*% invK %*% y - log(detK)\n}\ntheta <- optim(c(1, 1), ll, control = list(trace = 1))\n```\n:::\n\n\n\n\n# Evaluation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK_XX <- k_XX(X)\nB <- 1/nrow(X) * t(Y) %*% solve(K_XX) %*% Y\nSigma <- kronecker(B, K_XX) + kronecker(diag(c(0.001, 0.001)), diag(1, nrow = nrow(X)))\nSi <- solve(Sigma) %*% y\n\nf <- data.frame()\nfor (i in 1:nrow(d)) {\n  x_star <- as.matrix(d[i, 1:2])\n  K_star <- apply(X[, 1:2], 1, function(x) k(x_star, x))\n  f_star <- t(kronecker(B, K_star)) %*% Si |> t()\n  f <- rbind(f, f_star)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- f |>\n  tibble() |>\n  set_names(c(\"vx_h\", \"vy_h\")) |>\n  bind_cols(d) |>\n  mutate(col = factor(as.numeric(index %in% S$index), levels = 0:1, labels = c(\"Test\", \"Train\")))\n\np1 <- p0 +\n  geom_segment(\n    data = f,\n    aes(x, y, xend = x + vx_h/50, yend = y + vy_h/50, color = col),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25,\n    alpha = 0.8\n  ) +\n  scale_color_scico_d(palette = \"imola\", name = \"\") +\n  labs(x = \"Longitude\", y = \"Latitude\") +\n  theme(legend.position = \"bottom\")\n\np1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrmse <- function(y, yhat) {\n  sqrt(1/length(y) * sum((yhat - y)^2))\n}\n\nf |>\n  group_by(col) |>\n  summarize(\n    rmse_vx = rmse(vx, vx_h),\n    rmse_vy = rmse(vy, vy_h)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 3\n  col    rmse_vx  rmse_vy\n  <fct>    <dbl>    <dbl>\n1 Test  2.16     2.26    \n2 Train 0.000221 0.000155\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
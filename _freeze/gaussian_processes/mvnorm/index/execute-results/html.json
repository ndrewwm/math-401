{
  "hash": "90d76b23c80705d39f0d5edddd6af041",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: The Multivariate Gaussian Distribution\nbibliography: mvnorm.bib\n---\n\n\n\n\n## Definition\n\nLet $\\mathbf{x} \\in \\mathbb{R}^N$ be a vector whose elements, $X_i$, are random variables (i.e., $\\mathbf{x}$ is a *random vector*). As elements of $\\mathbf{x}$, we say that $X_1, X_2, \\dots, X_N$ are jointly distributed. Define the vector $\\mathbf{\\mu}$ and matrix $\\mathbf{\\Sigma}$ as\n$$\n\\begin{align*}\n\\mathbf{\\mu} \\in \\mathbb{R}^N &= (\\mu_1, \\mu_2, \\dots, \\mu_N)^\\top = (\\mathbb{E}(X_1), \\mathbb{E}(X_2), \\dots, \\mathbb{E}(X_N))^\\top  \\\\\n\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times N} &= Cov(\\mathbf{x}) = \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] = \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^N\n\\end{align*}\n$$\n\nand suppose $X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})$. If every linear combination of $\\mathbf{x}$'s elements results in a univariate normal variable, we say that $\\mathbf{x}$ comes from the multivariate Gaussian distribution parameterized by $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$, written as $\\mathbf{x} \\sim \\mathcal{N}_N (\\mathbf{\\mu}, \\mathbf{\\Sigma}).$\n\n## Sampling Algorithm\n\nA commonly used algorithm for sampling from a multivariate Gaussian distribution can be described as the following, given a vector $\\mathbf{\\mu} \\in \\mathbb{R}^N$ and a positive-(semi)definite matrix $\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times N}$:\n\n1. Compute $L L^\\top = \\mathbf{\\Sigma}$, the Cholesky decomposition of $\\mathbf{\\Sigma}$.\n2. Generate $\\mathbf{z} = (Z_1, Z_2, \\dots, Z_N)^\\top$ where $Z_1, Z_2, \\dots, Z_N \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1)$.\n3. Compute $\\mathbf{x} = L\\mathbf{z} + \\mathbf{\\mu}$.\n\n## Why does the algorithm work?\n\nThe claim is that $\\mathbf{x}$ is a draw from a multivariate Gaussian distribution parameterized by $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$, i.e., $\\mathbf{x} \\sim \\mathcal{N}_N (\\mathbf{\\mu}, \\mathbf{\\Sigma})$. So, why does this work? First, we can show that $\\mathbb{E}(\\mathbf{x}) = \\mathbf{\\mu}$:\n\n$$\n\\begin{align*}\n\\mathbb{E}(\\mathbf{x}) &= \\mathbb{E}(L\\mathbf{z} + \\mathbf{\\mu}) \\\\\n  &= \\mathbb{E}(L\\mathbf{z}) + \\mathbb{E}(\\mathbf{\\mu}) \\\\\n  &= L \\cdot \\mathbb{E}(\\mathbf{z}) + \\mathbf{\\mu} \\\\\n  &= L \\cdot 0 + \\mathbf{\\mu} \\\\\n  &= \\mathbf{\\mu}.\n\\end{align*}\n$$\n\nNext, we can see that $Cov(\\mathbf{x}) = \\mathbf{\\Sigma}$:\n\n::: {.column-margin}\nReminder that $Cov(\\mathbf{x})$ is not a scalar, rather, it is the *covariance matrix* (or *variance-covariance matrix*) of $\\mathbf{x}$.\n:::\n\n$$\n\\begin{align*}\nCov(\\mathbf{x}) &= \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[(L\\mathbf{z} + \\mathbf{\\mu} - \\mathbf{\\mu})(L\\mathbf{z} + \\mathbf{\\mu} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[L\\mathbf{z}\\mathbf{z}^\\top L^\\top] \\\\\n  &= L\\ \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top]\\ L^\\top \\\\\n  &= L\\ \\mathbb{E}[(\\mathbf{z} - 0)(\\mathbf{z} - 0)^\\top]\\ L^\\top \\\\\n  &= L\\ Cov(\\mathbf{z})\\ L^\\top \\\\\n  &= L I L^\\top \\\\\n  &= L L^\\top \\\\\n  &= \\mathbf{\\Sigma}.\n\\end{align*}\n$$\n\n::: {.column-margin}\nThis proof hinges on the definition of covariance and the independence of the $Z_i$'s. Remember, $Cov(Z_i, Z_i) = Var(Z_i) = 1$ and $Cov(Z_i, Z_j) = 0$ when $i \\neq j$ and $Z_i \\perp Z_j$.\n:::\n\nThe following references [@2691254], [@332722], [@Bilionis] (specifically [this section](https://predictivesciencelab.github.io/data-analytics-se/lecture06/hands-on-06.2.html#sampling-the-multivariate-normal-with-diagonal-covariance-using-the-standard-normal)) were helpful to me as I worked through understanding the algorithm and the proofs for the two results above.\n\nHaving shown that the expectation and (co)variance of $\\mathbf{x}$ are what we'd assume, we need to show that $\\mathbf{x}$ qualifies as multivariate normal. A random vector is multivariate normal if any linear combination of its elements is normally distributed. From its construction, we know $\\mathbf{x}$ is a linear transformation of a standard normal vector, i.e., $\\mathbf{z}$. We can decompose $\\mathbf{x}$'s $i$-th element, $X_i$, into the following:\n\n$$\n\\begin{align*}\n\\sigma_i &= \\sum_{j = 0}^n L_{i,j} \\\\\nX_i &= Z_i \\sigma_i + \\mu_i.\n\\end{align*}\n$$\n\nWe can then show that $X_i \\sim \\mathcal{N}(\\sigma_i \\cdot 0 + \\mu_i, \\sigma_i^2) \\implies X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$. This follows from theorems regarding the transformation of random variables [@casella2024statistical]. I'll demonstrate it for an arbitrary $Z \\sim \\mathcal{N}(0, 1)$, $\\mu$, and $\\sigma > 0$ (omitting the subscript $i$ for brevity). The probability density function (pdf) of the standard normal $f_Z(z)$ is defined as \n$$\nf_Z(z) = \\frac{1}{\\sqrt{2\\pi}}\\ e^{-z^2 / 2}.\n$$\n\nDefine $X' = g(Z) = \\sigma \\cdot f_Z(Z)$. Then, $g^{-1}(x) = \\frac{x}{\\sigma}$, and $\\frac{d}{dx}g^{-1}(x) = \\frac{1}{\\sigma}$. Thus, the probability density function $f_{X'}(x)$ is\n\n$$\n\\begin{align*}\nf_{X'}(x) &= f_Z(g^{-1}(x)) \\frac{d}{dx}g^{-1}(x) \\\\\n  &= f_Z \\Bigl(\\frac{x}{\\sigma} \\Bigr) \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\ e^{-(\\frac{x}{\\sigma})^2 \\cdot \\frac{1}{2}} \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{x^2}{2\\sigma^2}}.\n\\end{align*}\n$$\n\nThis is the pdf for $\\mathcal{N}(0, \\sigma^2)$. Now, we find $X = h(X') = f_{X'}(X') + \\mu$, where $h^{-1}(x) = x - \\mu$ and $\\frac{d}{dx}\\ h^{-1}(x) = 1$. As with the above, we have\n$$\nf_X(x) = f_{X'}(h^{-1}(x)) \\frac{d}{dx}\\ h^{-1}(x) = f_{X'}(x - \\mu) = \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}},\n$$\n\nwhich is the classical definition of the normal distribution's probability density function. This means that scaling a normal variable by a constant, or adding a constant to a normal variable results in another normal variable. So, each $X_i$ is normal. Lastly, as one's intuition might suspect, the sum of jointly distributed normal variables is also normal. I won't go through the proof here, but some versions can be found on [wikipedia.](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Correlated_random_variables)\n\n<!-- We showed that the vector x has cov(x) = Sigma; it's possible that cov(x_i, x_j) > 0 for some i,j -->\n<!-- https://stats.stackexchange.com/a/548360 -->\n\nThus, we can conclude that a linear combination $Y = a_1 X_1 + \\cdots + a_n X_n$ is a normal variable, meaning $\\mathbf{x}$ is a draw from the multivariate Gaussian distribution.\n\n## R Implementation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nmu <- c(1.1, 3, 8)\nSigma <- matrix(c(1, 0.1, 0.3, 0.1, 1, 0.1, 0.3, 0.1, 1), 3, 3)\n\nmvrnorm <- function(n, mu, Sigma) {\n  L <- chol(Sigma) # this returns L', where LL' = Sigma\n  d <- length(mu)\n\n  out <- matrix(0, nrow = n, ncol = d)\n  for (i in 1:n) {\n    u <- rnorm(d)\n    out[i, ] <- t(t(L) %*% u + mu)\n  }\n  return(out)\n}\n\nmvrnorm(10, mu, Sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]     [,2]     [,3]\n [1,] 0.5395244 2.714929 9.298527\n [2,] 1.1705084 3.135691 9.661861\n [3,] 1.5609162 1.787372 7.395843\n [4,] 0.6543380 4.173380 8.294725\n [5,] 1.5007715 3.150205 7.599224\n [6,] 2.8869131 3.674046 6.700175\n [7,] 1.8013559 2.599714 7.161280\n [8,] 0.8820251 1.957341 7.169001\n [9,] 0.4749607 1.259257 8.490846\n[10,] 1.2533731 1.882905 9.158747\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
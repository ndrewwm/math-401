{
  "hash": "a51d65df1ee0090f9875aec88bfb7dd1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Overview of Gaussian Processes\nbibliography: gaussian_processes.bib\nknitr:\n  opts_chunk:\n    out.width: \"100%\"\nnocite: |\n  @GundersonGPJune, @GundersonGPSep, @Orduz\n---\n\n\n\n## Introduction\n\n*Gaussian Processes* (often abbreviated as \"GP\"s) are stochastic (random) processes that are commonly used to model nonlinear functions. The authors of a popular textbook covering GPs and GP regression [@williams2006gaussian, pg. 2] provide the following informal definition:\n\n> A Gaussian process is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions. Leaving mathematical sophistication aside, one can loosely think of a function as a very long vector, each entry in the vector specifying the function value $f(x)$ at a particular input $x$. It turns out, that although this idea is a little naÃ¯ve, it is surprisingly close what we need.\n\nThis description helpfully orients us to two key aspects. First is that GPs are often used to specify probability distributions over *functions*. Second, it cues us to think about collections of function evaluations as (jointly distributed) *vectors*. Below we'll discuss a more formal definition.\n\n## Definition\n\nA *Gaussian Process* is an uncountably infinite collection of random variables, with any finite sample from the process sharing a joint [multivariate Gaussian distribution](./mvnorm/index.qmd). If we assume a function $f(\\mathbf{x})$ comes from a Gaussian Process, we write\n$$\n\\begin{align*}\nf(\\mathbf{x}) &\\sim \\mathcal{GP}(m, k)\\ \\text{ where } \\\\\nm(\\mathbf{x}) &= \\mathbb{E}(f(\\mathbf{x})), \\text{ and } \\\\\nk(\\mathbf{x}, \\mathbf{x}') &= \\mathbb{E}[(f(\\mathbf{x}) - m(\\mathbf{x}))(f(\\mathbf{x}') - m(\\mathbf{x}'))].\n\\end{align*}\n$$\n\nThe functions $m$ and $k$ are referred to as *mean* and *covariance* functions, respectively. The mean function is often set to be the zero function $\\mathbf{0}$ for notational convenience. The covariance (kernel) function must produce a positive definite matrix when evaluated on a collection of inputs. For my project, I've focused on the squared-exponential function. For the following examples, we'll define it using its scalar form $k: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$:\n$$\nk(x, x') = \\alpha^2 \\cdot \\exp \\Bigl( -\\frac{1}{2\\rho^2}(x - x')^2 \\Bigr), \\text{ where } \\alpha, \\rho > 0.\n$$\n\nNote that the values of $k$ will decrease exponentially as the inputs ($x$ and $x'$) get farther apart. This is a desirable property for modeling covariance, as we'd generally expect two nearby inputs to be similar to each other and two distant values to be less similar. The range of the functions drawn from the GP is controlled via $\\alpha$, while the frequency (\"wiggliness\") of the functions is controlled by the length-scale $\\rho$. Together, we refer to $\\alpha$ and $\\rho$ are as *hyperparameters*. They can be estimated from sample data using statistical inference, or chosen by the analyst based on expertise or problem context.\n\nAs an aside, the squared-exponential kernel is a common choice as a covariance function, but any function that produces a positive (semi)definite matrix can be used. David Duvenaud's website and thesis chapter includes descriptions of various kernel functions that can be used for modeling different types of data [@DuvenaudKernelCookbook].\n\n## Gaussian Process Regression\n\nIn practice, Gaussian Processes are often brought to bear on *regression problems*, in which an analyst has collected a dataset $S = (\\mathbf{x}, \\mathbf{y}) = \\{ (x_i, y_i) : x_i,y_i \\in \\mathbb{R}, i \\in 1, 2, \\dots, N \\}$ with the goal of learning the relationship $f$ between $\\mathbf{x}$ and $\\mathbf{y}$:\n\n$$\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) \\ \\text{ or} \\\\\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\ \\text{ (with additive noise)}.\n\\end{align*}\n$$\n\nOnce $f$ has been estimated, it can then be used to predict the values of future or test points, which we'll refer to as $\\mathbf{x}_* \\in \\mathbb{R}^M$.\n\nGaussian Process Regression can be considered a Bayesian method for learning $f$. We'll consider the plots below to as an example with simulated data.\n\n::: {.panel-tabset}\n\n### Prior Distribution\n\nIn <span style=\"color: orange;\"><strong>orange</strong></span>, we have a function, unknown to our imagined analyst. In <span style=\"color: grey;\"><strong>grey</strong></span> we have 10 draws from $f \\sim \\mathcal{GP}(\\mathbf{0}, k(\\mathbf{x}_*, \\mathbf{x}_*))$. This represents what the analyst knows before incorporating information from $S$. As we can see, the <span style=\"color: grey;\"><strong>draws</strong></span> span the domain we're interested in, but their y-values are chaotic and don't align with the <span style=\"color: orange;\"><strong>target</strong></span>.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nset.seed(123)\ntheme_set(theme_minimal(base_size = 15) + theme(panel.grid.minor = element_blank()))\n\nf <- function(x) sin(2*x) + sin(4*x)\nk <- function(x_i, x_j, alpha, rho) alpha^2 * exp(-(x_i - x_j)^2 / (2 * rho^2))\n\nk_xX <- function(x, X, alpha = 1, rho = 0.4) {\n  N <- NROW(x)\n  M <- NROW(X)\n  K <- matrix(0, N, M)\n  for (n in 1:N) {\n    for (m in 1:M) {\n      K[n, m] <- k(x[n], X[m], alpha, rho)\n    }\n  }\n  return(K)\n}\n\nN <- 400\nn <- 30\nx <- seq(-5, 5, length.out = N)\nf_x <- f(x)\n\nD <- tibble(x, y = f_x)\nS <- slice_sample(D, n = n)\nZ <- anti_join(D, S, by = \"x\")\n\nK_xx <- k_xX(x, x)\n\nggplot() +\n  geom_line(data = D, aes(x, y), color = \"orange\") +\n  geom_line(\n    data = map(\n      1:10, \\(i) {\n        tibble(x, y = MASS::mvrnorm(1, mu = rep.int(0, N), Sigma = K_xx))\n    }) |> \n      bind_rows(.id = \"b\"),\n    aes(x, y, group = b), alpha = 0.2\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=100%}\n:::\n:::\n\n\n\n### Training Data\n\nAgain we have our unknown target function in <span style=\"color: orange;\"><strong>orange</strong></span>. The **black** dots comprise $S$, the analyst's dataset (here, N = 30).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_line(data = D, aes(x, y), color = \"orange\") +\n  geom_point(data = S, aes(x, y))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=100%}\n:::\n:::\n\n\n\n### Posterior Distribution\n\nNow we can combine our prior distribution with the observed data to produce a posterior distribution. In essence, the result is a new distribution that \"agrees\" with the observed training data points. Gaussian Processes have a nice property in that the posterior distribution can be computed analytically[^1]:\n\n$$\n\\begin{align*}\n\\mathbf{y}_* | \\mathbf{x}, \\mathbf{y}, \\mathbf{x}_* &\\sim \\mathcal{N}_M(\\hat{\\mu}, \\hat{\\Sigma}) \\\\\n\\hat{\\mu} &= k(\\mathbf{x}_*, \\mathbf{x})(k(\\mathbf{x}, \\mathbf{x}))^{-1}\\mathbf{y} \\\\\n\\hat{\\Sigma} &= k(\\mathbf{x}_*, \\mathbf{x}_*) - k(\\mathbf{x}_*, \\mathbf{x})(k(\\mathbf{x}, \\mathbf{x}))^{-1}k(\\mathbf{x}_*, \\mathbf{x})^\\top \\\\\n\\text{ where } \\ \\ \\ & \\\\\nk(\\mathbf{x}, \\mathbf{x}) &= [k(x_i, x_j)]_{i,j}^N \\in \\mathbb{R}^{N \\times N} \\\\\nk(\\mathbf{x}_*, \\mathbf{x}) &= [k(x_{*i}, x_j)]_{i,j}^{M,N} \\in \\mathbb{R}^{M \\times N} \\\\\nk(\\mathbf{x}_*, \\mathbf{x}_*) &= [k(x_{*i}, x_{*j})]_{i,j}^M \\in \\mathbb{R}^{M \\times M}.\n\\end{align*}\n$$\n\nAfter computing $\\hat{\\mu}$ and $\\hat{\\Sigma}$, we can draw samples from the multivariate normal distribution and summarize them to produce estimates for each test point in $\\mathbf{x}_*$. Below in <span style=\"color: blue;\"><strong>blue</strong></span> is the posterior mean across our domain, with the <span style=\"color: grey;\"><strong>grey</strong></span> shading representing $\\pm 2$ standard deviations (i.e., a 95% credible interval).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\neps <- 0.009\nK_XX <- k_xX(S$x, S$x)\nK_xX <- k_xX(D$x, S$x)\n\n# Adding a small amount of noise to the diagonal for numerical stability\nmu <- K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% S$y\nsigma <- K_xx - K_xX %*% solve(K_XX + eps^2 * diag(n)) %*% t(K_xX)\n\ndraws <- MASS::mvrnorm(100, mu, sigma)\n\ndraws <- as_tibble(t(draws)) |>\n  mutate(x = D$x) |>\n  pivot_longer(-x) |>\n  group_by(x) |>\n  summarise(y = mean(value), s = sd(value))\n\nggplot() +\n  geom_line(data = D, aes(x, y), color = \"orange\") +\n  geom_ribbon(data = draws, aes(x, ymin = y - 2*s, ymax = y + 2*s), alpha = 0.2) +\n  geom_line(data = draws, aes(x, y), color = \"blue\") +\n  geom_point(data = S, aes(x, y))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=100%}\n:::\n:::\n\n\n\n:::\n\n[^1]: The posterior distribution for this example assumes no additive noise in the target vector $\\mathbf{y}$. If measurement error is believed to be present, the computations for $\\hat{\\mu}$ and $\\hat{\\Sigma}$ should replace $k(\\mathbf{x}, \\mathbf{x})$ with $k(\\mathbf{x}, \\mathbf{x}) + \\sigma_y^2 I_N$, in which $\\sigma_y$ is an additional hyperparameter for the outcome's standard deviation. $I_N$ is the $N \\times N$ identity matrix.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
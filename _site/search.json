[
  {
    "objectID": "notes/template.html",
    "href": "notes/template.html",
    "title": "Summary of progress over the previous week",
    "section": "",
    "text": "Summary of progress over the previous week\n\n\nCurrent Questions\n\n\nResources reviewed/cited"
  },
  {
    "objectID": "krr/index.html",
    "href": "krr/index.html",
    "title": "Kernel Ridge Regression",
    "section": "",
    "text": "library(tidyverse)\nset.seed(123)\n\nf &lt;- function(x) sin(3*x) + sin(7*x)\nk &lt;- function(x_i, x_j, sigma_f = 0.4, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nn &lt;- 400\nx &lt;- seq(-5, 5, length.out = n)\nf_x &lt;- f(x) + rnorm(n, mean = 0, sd = 0.1)\n\n\nn &lt;- 100\nind_train &lt;- sample(1:400, n, replace = FALSE)\nx_train &lt;- x[ind_train]\nf_train &lt;- f_x[ind_train]\n\ncompute_covmat &lt;- function(k, x, y) {\n  n_x &lt;- length(x)\n  n_y &lt;- length(y)\n  K &lt;- matrix(0, n_x, n_y)\n  for (i in 1:n_x) {\n    for (j in 1:n_y) {\n      K[i, j] &lt;- k(x[i], y[j])\n    }\n  }\n  return(K)\n}\n\nk_xx &lt;- compute_covmat(k, x_train, x_train)\n\nlambda &lt;- 0.0001\ncoef &lt;- solve(k_xx + n * lambda * diag(n)) %*% f_train\n\nf_hat &lt;- function(x) {\n  map2_dbl(coef, x_train, ~.x * k(x, .y)) |&gt;\n    reduce(`+`)\n}\n\ny_hat &lt;- map_dbl(x, f_hat)\n\ncolors &lt;- c(\"Actual\" = \"blue\", \"Data\" = \"black\", \"Estimate\" = \"orange\")\n\nout &lt;- tibble(x, f = f(x), f_x, y_hat) |&gt;\n  pivot_longer(-x) |&gt;\n  mutate(\n    name = factor(name, levels = c(\"f\", \"f_x\", \"y_hat\"), labels = names(colors))\n  )\n  \nggplot() +\n  geom_point(data = filter(out, name == \"Data\"), aes(x, y = value, color = name), alpha = 0.2) +\n  geom_line(data = filter(out, name == \"Actual\"), aes(x, y = value, color = name)) +\n  geom_line(data = filter(out, name == \"Estimate\"), aes(x, y = value, color = name)) +\n  scale_color_manual(name = \"\", values = colors) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Kernel Ridge Regression"
    ]
  },
  {
    "objectID": "reflections/reflection_1.html",
    "href": "reflections/reflection_1.html",
    "title": "Reflection 1",
    "section": "",
    "text": "A key aspect of my experience at Boise state is that I am a second-degree student. My first B.A. degree was in the field of psychology. After graduating in 2013, I worked as a research technician in various settings and as a data analyst. Coursework and independent research experience from my first degree prepared me for the work I would perform, but I was constantly drawn to the study of mathematics and statistical science. Modern software makes computing and visualizing statistical data quick and often easy to perform. To me, this convenience highlights the importance of understanding the fundamentals beneath a software implementation. I was computing summaries and building/visualizing small statistical models. I wanted to make sure that I wasn’t mishandling these “golems”, as Richard McElreath (McElreath 2018, chap. 1) describes them.\nWhen I began my program at Boise State, I had already completed Calculus I, Calculus II, Calculus III, and an introduction to Linear Algebra. I believe there are two themes visible in my coursework, and will elaborate on them in the following paragraphs. Below is a list of courses I completed during my second degree, with courses as part of the statistics emphasis in italics and courses I want to highlight in bold.\n\n\n\nTerm\nCourse\nTitle\n\n\n\n\n2021 Spring\nMATH 361\nProbability & Statistics I\n\n\n2021 Summer\nMATH 189\nDiscrete Mathematics\n\n\n2021 Fall\nMATH 287\nMathematical Proofs & Methods\n\n\n\nMATH 471\nData Analysis\n\n\n2022 Spring\nMATH 305\nAbstract Algebra/Number Theory\n\n\n\nMATH 314\nFoundations of Analysis\n\n\n2022 Fall\nMATH 333\nDifferential Equations w/ Matrix Theory\n\n\n\nMATH 462\nProbability & Statistics II\n\n\n2023 Spring\nMATH 472\nComputational Statistics\n\n\n2023 Fall\nMATH 365\nIntroduction to Computational Mathematics\n\n\n2024 Spring\nMATH 308\nIntroduction to Algebraic Cryptology\n\n\n2024 Fall\nMATH 403\nLinear Algebra\n\n\n\nMATH 465\nIntroduction to Numerical Methods\n\n\n\nThe first theme is the development of literacy for mathematical writing. I greatly appreciated the first few classes I took that emphasized the examination and writing of mathematical proofs. I remember MATH 287, MATH 305, and MATH 314 being very challenging, but deeply rewarding in terms of strengthening my ability to read and decompose theorems that I’d encounter in other classes. I credit MATH 287 in particular for helping me learn about the structure of proofs, and the kinds of logical arguments used to integrate and extend mathematical facts/topics.\nThe second theme is an appreciation for the “species” of mathematical objects and spaces that one might encounter. I think MATH-314 and MATH-403 are my favorite examples of this, but I should also mention MATH-305 for similar reasons. MATH-314 was interesting in that it cracks open several things we almost take for granted, such as calculus and the real number line. Calculus is incredibly far-reaching in its impact on technology and science, but we don’t really get a sense for the mathematical results that provide its foundation until studying analysis. Similarly, linear algebra is deeply influential for modern computing, but I’ve grown to appreciate how useful it is for translating mathematical concepts into multiple dimensions. Being aware of the space(s) or set(s) of numbers you’re reasoning about is crucial to framing the problems you’re working on.\nUnifying these two themes is an interest in computation and statistical inference. This stems from my motivations to pursue the degree, but my learning in MATH-462 and MATH-472 would have been much more shallow if I hadn’t taken MATH-287 and MATH-314. Most importantly, courses like MATH-365, MATH-465, and MATH-472 emphasized the implementation of mathematical methods via programming and simulation. I’ve found that defining and conducting a simulation and implementing algorithms to be the best way I learn when exploring a mathematical property or topics within a course. Looking back, I think the absence of an application area was a reason I struggled with MATH-189 and MATH-305.\n\n\n\n\nReferences\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Home",
      "Reflections",
      "Reflection 1"
    ]
  },
  {
    "objectID": "gaussian_processes/v2.html",
    "href": "gaussian_processes/v2.html",
    "title": "Gaussian Process Regression",
    "section": "",
    "text": "Assume our sample space is 400 evenly spaced points between -5 and 5. Let \\(\\mathbf{x}\\) denote the set of \\(N\\)-many points at which we have collected observations, defined as:\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}.\n\\]\nLet \\(\\mathbf{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2)\\) be an \\(N\\)-dimensional vector. Let \\(\\mathbf{y}\\) denote the values of \\(f\\) observed at each \\(x_i\\), with the addition of noise (\\(\\epsilon_i\\)): \\[\n\\mathbf{y} = f(\\mathbf{x}) + \\mathbf{\\epsilon} \\iff \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \\begin{bmatrix} f(x_1) \\\\ f(x_2) \\\\ \\vdots \\\\ f(x_N) \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{bmatrix}.\n\\]\nTogether, \\((\\mathbf{x}, \\mathbf{y})\\) can be referred to as training data. Let \\((\\mathbf{x_*}, \\mathbf{y_*})\\) denote test data (i.e., points \\(x^*_i\\) at which we want to predict a value of \\(y^*_i\\)). We will use \\(M\\) to denote the size of \\(\\mathbf{x_*}\\) and \\(\\mathbf{y_*}\\).\nWe assume that \\(\\mathbf{y}\\) can be modeled by a Gaussian process, i.e., \\(\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{\\Sigma})\\). Let \\(m: \\mathbb{R} \\to \\mathbb{R}\\), and the squared exponential kernel \\(k: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\) be defined as \\[\n\\begin{align*}\nm(x) &= 0, \\\\\nk(x, x') &= \\sigma \\cdot exp\\Bigl(-\\frac{1}{2l^2}(x - x')^2\\Bigr)\n\\end{align*}\n\\]\nwith \\(\\sigma\\) and \\(l\\) as hyperparameters.\n\n\nTo-do: elaborate on what \\(\\sigma\\) and \\(l\\) control within \\(k\\).\nThe conditional distribution \\(\\mathbf{y_*} | \\mathbf{y}\\) can be defined as another Gaussian process: \\(\\mathbf{y}_* | \\mathbf{y} \\sim \\mathcal{N}(\\bar{m}, \\bar{\\Sigma})\\) with \\(\\bar{m}\\) and \\(\\bar{\\Sigma}\\) are defined as\n\\[\n\\begin{align*}\n\\bar{m} &= K(\\mathbf{x_*}, \\mathbf{x}) (K(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}, \\\\\n\\bar{\\Sigma} &= K(\\mathbf{x_*}, \\mathbf{x_*}) - K(\\mathbf{x_*}, \\mathbf{x})(K(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I})^{-1} K(\\mathbf{x}, \\mathbf{x_*}),\n\\end{align*}\n\\]\n\n\nIn Kanagawa et al. (2018), they note that the final term in \\(\\bar{m}\\) should be something like \\((\\mathbf{y} - m_\\mathbf{x})\\) where \\(m_\\mathbf{x} = [m(x_1), \\dots, m(x_N)]^\\top\\). I’ve omitted the subtraction given that \\(m(\\mathbf{x}) = \\mathbf{0}\\).\nwhere\n\\[\n\\begin{align*}\nK(\\mathbf{x_*}, \\mathbf{x}) &= [k(x^*_i, x_j)]_{i,j = 1}^{M,N} \\in \\mathbb{R}^{M \\times N}, \\\\\nK(\\mathbf{x}, \\mathbf{x_*}) &= [k(x_i, x^*_j)]_{i,j = 1}^{N,M} \\in \\mathbb{R}^{N \\times M}, \\\\\nK(\\mathbf{x}, \\mathbf{x}) &= [k(x_i, x_j)]_{i,j = 1}^{N,N} \\in \\mathbb{R}^{N \\times N}, \\text{ and } \\\\\nK(\\mathbf{x_*}, \\mathbf{x_*}) &= [k(x^*_i, x^*_j)]_{i,j}^{M,M} \\in \\mathbb{R}^{M \\times M}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nNon-functional code\n\n\n\nI haven’t been able to get a comparable implementation in Julia to work. The issue appears to be roundoff errors, with some entries of the posterior covariance matrix being very small, but negative.\n\n\nWe’ll now apply these definitions to simulated data. For this simulation, we’ll arbitrarily pick \\(\\sigma = 2\\) and \\(l = 0.4\\) as our hyperparameter values.\n\n\nThe functions \\(m\\) and \\(k\\) are themselves hyperparameters? So, in the case of \\(\\sigma\\) and \\(l\\), are they “sub-hyperparameters”?\n\nusing LinearAlgebra, Distributions, Random, Plots\n\nσ = 0.4;\nl = 0.4;\n\nf(x) = sin(3x) + sin(7x);\n\nx_all = Vector(range(-5, 5, 400));\nx = sort(rand(x_all, 50));\nx_star = sort(setdiff(x_all, x));\n\nN = length(x);\nM = length(x_star);\n\nϵ = rand(Normal(0, σ), N);\nϵ_star = rand(Normal(0, σ), M);\n\ny = f.(x) + ϵ;\ny_star = f.(x_star) + ϵ_star;\n\n\nplot(x_all, f.(x_all))\nscatter!(vcat(x, x_star), vcat(y, y_star))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaving defined \\((\\mathbf{x}, \\mathbf{y})\\) and \\((\\mathbf{x_*}, \\mathbf{y_*})\\), we can now compute each \\(K\\).\n\nk(x_i, x_j) = σ * exp(-(x_i - x_j)^2 / (2 * l^2));\n\nfunction compute_covmat(k::Function, X::Vector, Y::Vector)::Matrix\n    n_x = length(X);\n    n_y = length(Y);\n    K = zeros(n_x, n_y);\n    for i = 1:n_x\n        for j = 1:n_y\n            K[i, j] = k(X[i], Y[j]);\n        end\n    end\n    return K;\nend;\n\nK = compute_covmat(k, x, x);\nK_star = compute_covmat(k, x_star, x);\nK_star2 = compute_covmat(k, x_star, x_star);\n\n\nm_post = K_star * inv(K + σ^2 * I) * y;\ncov_post = K_star2 - K_star * inv(K + σ^2 * I) * K_star';\n\ndisplay(cov_post)\n\n# cov_post = cov_post - minimum(eigvals(Symmetric(cov_post))) * I;\n\n# rand(MultivariateNormal(m_post, cov_post), 10)\n\n351×351 Matrix{Float64}:\n  0.203458      0.195848      0.187314     …  1.13036e-15  1.01467e-15\n  0.195848      0.189374      0.181968        2.17469e-15  1.95633e-15\n  0.187314      0.181968      0.175691        3.27675e-15  2.95011e-15\n  0.17796       0.173715      0.168556        4.41647e-15  3.97792e-15\n  0.167901      0.164717      0.160647        5.56986e-15  5.01816e-15\n  0.157259      0.155082      0.15206      …  6.70923e-15  6.04586e-15\n  0.146164      0.14493       0.1429          7.80343e-15  7.03294e-15\n  0.134752      0.134384      0.133279        8.81839e-15  7.94868e-15\n  0.123159      0.123571      0.123313        9.71777e-15  8.76032e-15\n  0.11152       0.11262       0.113123        1.04638e-14  9.43379e-15\n  0.0886203     0.090804      0.092544     …  1.13439e-14  1.02293e-14\n  0.0776025     0.0801771     0.0823863       1.14057e-14  1.0286e-14\n  0.067018      0.0698839     0.0724613       1.11721e-14  1.00766e-14\n  ⋮                                        ⋱               ⋮\n -9.44505e-16  -1.83887e-15  -2.78306e-15     0.0151902    0.0125775\n -5.13807e-16  -1.0149e-15   -1.54415e-15  …  0.0198644    0.0173185\n  2.37941e-16   4.26845e-16   6.25642e-16     0.0301797    0.0279962\n  8.01725e-16   1.51236e-15   2.26175e-15     0.0414494    0.0399572\n  1.00543e-15   1.90631e-15   2.85649e-15     0.0472927    0.0462729\n  1.15665e-15   2.20011e-15   3.3008e-15      0.0531858    0.0527209\n  1.25707e-15   2.39683e-15   3.59919e-15  …  0.0590578    0.0592269\n  1.30963e-15   2.50194e-15   3.75984e-15     0.0648365    0.0657133\n  1.31829e-15   2.5229e-15    3.79385e-15     0.0704496    0.0721006\n  1.28775e-15   2.46861e-15   3.71459e-15     0.0758264    0.0783095\n  1.13036e-15   2.17469e-15   3.27675e-15     0.0856039    0.0898831\n  1.01467e-15   1.95633e-15   2.95011e-15  …  0.0898831    0.0951026"
  },
  {
    "objectID": "gaussian_processes/index.html",
    "href": "gaussian_processes/index.html",
    "title": "Overview of Gaussian Processes",
    "section": "",
    "text": "This page relies heavily on Kanagawa, Hennig, Sejdinovic, and Sriperumbudur (2018).\n\nDefinition 2.2: Gaussian Process\nLet \\(\\chi\\) be a nonempty set, \\(k: \\chi \\times \\chi \\to \\mathbb{R}\\) a positive definite kernel and \\(m: \\chi \\to \\mathbb{R}\\) be any real-valued function. Then a random function \\(f: \\chi \\to \\mathbb{R}\\) is said to be a Gaussian process (GP) with mean function \\(m\\) and covariance kernel \\(k\\), denoted by \\(\\mathcal{GP}(m, k)\\), if the following holds: For any finite set \\(X = (x_1, \\dots, x_n) \\subset \\chi\\) of any size \\(n \\in \\mathbb{N}\\), the random vector\n\\[\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n\\]\nfollows the multivariate normal distribution \\(\\mathcal{N}(m_X, k_{XX})\\) with covariance matrix \\(k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}\\) and mean vector \\(m_X = (m(x_1), \\dots, m(x_n))^\\top\\).\n\n\nGaussian Process Regression\n\nalso called kriging or Wiener-Kolmogorov prediction\n\nRegression is the task of estimating of an unknown function \\(f\\) based on a provided set of training data, \\((X, Y)\\), where \\(X = (x_1, \\dots, x_n)^\\top\\) and \\(Y = (y_1, \\dots, y_n)^\\top\\) are random vectors (\\(x_i\\) and \\(y_i\\) are realizations, collected by the experimenter). Regression assumes the presence of noise denoted by \\(\\epsilon\\), which completes the additive model:\n\\[\ny_i = f(x_i) + \\epsilon.\n\\]\nIt’s typically assumed that \\(\\epsilon\\) is normally distributed with mean 0, i.e., \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\).\nGaussian process regression is a Bayesian approach that uses a GP as a prior distribution for \\(f\\).\n\n\n\n\n\n\n\n\n\nPrior\nPosterior\n\n\n\n\nHyperparameter: kernel\n\\(k\\)\n\\(\\bar{k}\\)\n\n\nHyperparameter: mean function\n\\(m\\)\n\\(\\bar{m}\\)\n\n\nDistribution\n\\(\\mathcal{GP}(m, k)\\)\n\\(\\mathcal{GP}(\\bar{m}, \\bar{k})\\)\n\n\n\nThe authors show that the posterior distribution \\(f|Y \\sim \\mathcal{GP}(\\bar{m}, \\bar{k})\\) where \\(\\bar{m} : \\chi \\to \\mathbb{R}\\) and \\(\\bar{k}: \\chi \\times \\chi \\to \\mathbb{R}\\) is given by\n\\[\n\\begin{align*}\n\\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}(Y - m_X),\\ x \\in \\chi, \\\\\n\\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}k_{Xx'},\\ x,x' \\in \\chi,\n\\end{align*}\n\\]\nwhere \\(k_{Xx} = k_{xX}^\\top = (k(x_1, x), \\dots, k(x_n, x))^\\top\\).\nThis is interesting, because these are closed-form expressions, and notably, we haven’t made use of Bayes’s Rule. Assuming the kernel and mean functions aren’t expensive or difficult to evaluate, the computation is just linear algebra.\n\n\nDrawing from a Gaussian Process\n\nlibrary(tidyverse)\nset.seed(123)\n\nf &lt;- function(x) sin(3*x) + sin(7*x)\nk &lt;- function(x_i, x_j, sigma_f = 1, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nN &lt;- 400\nn &lt;- 120\nx &lt;- seq(-5, 5, length.out = N)\nf_x &lt;- f(x)\n\nsigma &lt;- 0.15\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- f_x + epsilon\n\nd &lt;- tibble(x, f_x, y)\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\") +\n  geom_point(aes(y = y))\n\n\n\n\n\n\n\n\nHere we’ll compute the posterior distribution over the whole range \\([-5, 5]\\), using \\(n = 100\\) training data points.\n\nind_train &lt;- sample(1:400, n, replace = FALSE)\nx_train &lt;- x[ind_train]\ny_train &lt;- y[ind_train]\n\ncompute_covmat &lt;- function(k, x, y) {\n  n_x &lt;- length(x)\n  n_y &lt;- length(y)\n  K &lt;- matrix(0, n_x, n_y)\n  for (i in 1:n_x) {\n    for (j in 1:n_y) {\n      K[i, j] &lt;- k(x[i], y[j])\n    }\n  }\n  return(K)\n}\n\nk_XX &lt;- compute_covmat(k, x_train, x_train)\nk_xx &lt;- compute_covmat(k, x, x)\nk_xX &lt;- compute_covmat(k, x, x_train)\n\nm_post &lt;- k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% y_train\ncov_post &lt;- k_xx - k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% t(k_xX)\n\nHere m_post represents the mean vector of the posterior distibution, and cov_post represents the posterior’s covariance matrix.\n\n\nIn Kanagawa et al. (2018), they note that the final term in the posterior mean function \\(\\bar{m}\\) should be something like \\((\\mathbf{y} - m_\\mathbf{x})\\) where \\(m_\\mathbf{x} = [m(x_1), \\dots, m(x_N)]^\\top\\). I’ve omitted the subtraction given that \\(m(\\mathbf{x}) = \\mathbf{0}\\).\nBelow in blue is the actual function \\(f\\), with 50 draws (grey) from the posterior distribution. The posterior mean of these draws is plotted as the orange line.\n\nf_post &lt;- MASS::mvrnorm(50, m_post, cov_post)\n\ndraws &lt;- as_tibble(t(f_post)) |&gt;\n  mutate(x = x, y = f(x)) |&gt;\n  pivot_longer(V1:V50)\n\npost_mean &lt;- draws |&gt;\n  group_by(x) |&gt;\n  summarize(post_mean = mean(value))\n\ndraws |&gt;\n  ggplot(aes(x = x)) +\n  geom_point(\n    data = tibble(x = x[ind_train], y = y[ind_train]),\n    aes(x, y), alpha = 0.2\n  ) +\n  geom_line(aes(y = value, group = name), alpha = 0.1) +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_line(data = post_mean, aes(x = x, y = post_mean), color = \"orange\")\n\n\n\n\n\n\n\n\n\nf_post &lt;- MASS::mvrnorm(n = 1000, m_post, cov_post)\n\ndraws &lt;- as_tibble(t(f_post)) |&gt;\n  mutate(x = x, y = f(x)) |&gt;\n  pivot_longer(V1:V1000)\n\npost_summaries &lt;- draws |&gt;\n  group_by(x) |&gt;\n  summarize(\n    post_mean = mean(value),\n    post_sd = sd(value)\n  )\n\npost_summaries &lt;- draws |&gt;\n  distinct(x, y) |&gt;\n  left_join(post_summaries, by = \"x\")\n\nggplot(data = post_summaries, aes(x = x)) +\n  geom_ribbon(aes(ymin = post_mean - 2*post_sd, ymax = post_mean + 2*post_sd), fill = \"grey70\") +\n  geom_line(aes(y = post_mean), color = \"orange\") +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_point(data = tibble(x = x[ind_train], y = y[ind_train]), aes(x, y), alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\nCitations (WIP)\n\nhttps://juanitorduz.github.io/gaussian_process_reg/\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/\nhttps://gregorygundersen.com/blog/2019/06/27/gp-regression/\nhttps://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html",
    "href": "gaussian_processes/mvnorm/index.html",
    "title": "The Multivariate Gaussian Distribution",
    "section": "",
    "text": "Let \\(\\mathbf{X} \\in \\mathbb{R}^n\\) be a vector whose elements, \\(X_i\\), are random variables (i.e., \\(\\mathbf{X}\\) is a random vector). Define the vector \\(\\mathbf{\\mu}\\) and matrix \\(\\mathbf{\\Sigma}\\) as \\[\n\\begin{align*}\n\\mathbf{\\mu} &= \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix} \\in \\mathbb{R}^n \\\\ \\\\\n\\mathbf{\\Sigma} &= \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^n \\in \\mathbb{R}^{n \\times n}\n\\end{align*}\n\\]\nand suppose \\(X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})\\). Then, we say that \\(\\mathbf{X}\\) comes from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), written as \\[\n\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma}).\n\\]",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#definition",
    "href": "gaussian_processes/mvnorm/index.html#definition",
    "title": "The Multivariate Gaussian Distribution",
    "section": "",
    "text": "Let \\(\\mathbf{X} \\in \\mathbb{R}^n\\) be a vector whose elements, \\(X_i\\), are random variables (i.e., \\(\\mathbf{X}\\) is a random vector). Define the vector \\(\\mathbf{\\mu}\\) and matrix \\(\\mathbf{\\Sigma}\\) as \\[\n\\begin{align*}\n\\mathbf{\\mu} &= \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix} \\in \\mathbb{R}^n \\\\ \\\\\n\\mathbf{\\Sigma} &= \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^n \\in \\mathbb{R}^{n \\times n}\n\\end{align*}\n\\]\nand suppose \\(X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})\\). Then, we say that \\(\\mathbf{X}\\) comes from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), written as \\[\n\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma}).\n\\]",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#sampling-algorithm",
    "href": "gaussian_processes/mvnorm/index.html#sampling-algorithm",
    "title": "The Multivariate Gaussian Distribution",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nA commonly used algorithm for sampling from a multivariate Gaussian distribution can be described as the following, given a vector \\(\\mathbf{\\mu} \\in \\mathbb{R}^n\\) and a positive-(semi)definite matrix \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\):\n\nCompute the Cholesky decomposition of \\(\\mathbf{\\Sigma}\\), \\(R^\\top R = \\mathbf{\\Sigma}\\).\nGenerate \\(\\mathbf{z} = \\begin{bmatrix} Z_1 \\\\ Z_2 \\\\ \\vdots \\\\ Z_n \\end{bmatrix}\\) where \\(Z_1, Z_2, \\dots, Z_n \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1)\\).\nCompute \\(\\mathbf{x} = R\\mathbf{z} + \\mathbf{\\mu}\\).",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#why-does-the-algorithm-work",
    "href": "gaussian_processes/mvnorm/index.html#why-does-the-algorithm-work",
    "title": "The Multivariate Gaussian Distribution",
    "section": "Why does the algorithm work?",
    "text": "Why does the algorithm work?\nThe claim is that \\(\\mathbf{x}\\) is a draw from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), i.e., \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). So, why does this work? First, we can show that \\(\\mathbb{E}(\\mathbf{x}) = \\mathbf{\\mu}\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\mathbf{x}) &= \\mathbb{E}(R\\mathbf{z} + \\mathbf{\\mu}) \\\\\n  &= \\mathbb{E}(R\\mathbf{z}) + \\mathbb{E}(\\mathbf{\\mu}) \\\\\n  &= R \\cdot \\mathbb{E}(\\mathbf{z}) + \\mathbf{\\mu} \\\\\n  &= R \\cdot 0 + \\mathbf{\\mu} \\\\\n  &= \\mathbf{\\mu}.\n\\end{align*}\n\\]\nNext, we can see that \\(Cov(\\mathbf{x}) = \\mathbf{\\Sigma}\\):\n\\[\n\\begin{align*}\nCov(\\mathbf{x}) &= \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[R\\mathbf{z}\\mathbf{z}^\\top R^\\top] \\\\\n  &= R\\ \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top]\\ R^\\top \\\\\n  &= R^\\top I R \\tag{why?} \\\\\n  &= R^\\top R \\\\\n  &= \\mathbf{\\Sigma}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nIncomplete proof\n\n\n\nIn working on this one I drew upon two sources, one from StackOverflow, the other from an online textbook/learning reference. However, the former’s proof which is essentially what I have above I think must be incorrect. The expectation of two independent normal variables should be 0, not 1. The other reference’s proof doesn’t elaborate fully.\n\n\nThis hinges on the definition of covariance and the independence of the \\(Z_i\\)’s (\\(Cov(Z_i, Z_i) = Var(Z_i) = 1\\) and \\(Cov(Z_i, Z_j) = 0\\) for \\(i \\neq j\\) and \\(Z_i \\perp Z_j\\)).\nHaving shown that the expectation and (co)variance of \\(\\mathbf{x}\\) are what we’d assume, we need to show that \\(\\mathbf{x}\\) qualifies as multivariate normal. A random vector is multivariate normal if any linear combination of its elements are normally distributed. From its construction, we know \\(\\mathbf{x}\\) is a linear transformation of a standard normal vector, i.e., \\(\\mathbf{z}\\). We can decompose \\(\\mathbf{x}\\)’s \\(i\\)-th element, \\(X_i\\), into the following:\n\\[\n\\begin{align*}\n\\sigma_i &= \\sum_{j = 0}^n R_{i,j} \\\\\nX_i &= Z_i \\sigma_i + \\mu_i.\n\\end{align*}\n\\]\nWe can then show that \\(X_i \\sim \\mathcal{N}(\\sigma_i \\cdot 0 + \\mu_i, \\sigma_i^2) \\implies X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\). This follows from theorems regarding the transformation of random variables (Casella & Berger, 2002). I’ll demonstrate it for an arbitrary \\(Z \\sim \\mathcal{N}(0, 1)\\), \\(\\mu\\), and \\(\\sigma &gt; 0\\) (omitting the subscript \\(i\\) for brevity). The probability density function (pdf) of the standard normal \\(f_Z(z)\\) is defined as \\[\nf_Z(z) = \\frac{1}{\\sqrt{2\\pi}}\\ e^{-z^2 / 2}.\n\\]\nDefine \\(X' = g(Z) = \\sigma \\cdot f_Z(Z)\\). Then, \\(g^{-1}(x) = \\frac{x}{\\sigma}\\), and \\(\\frac{d}{dx}g^{-1}(x) = \\frac{1}{\\sigma}\\). Thus, the probability density function \\(f_{X'}(x)\\) is\n\\[\n\\begin{align*}\nf_{X'}(x) &= f_Z(g^{-1}(x)) \\frac{d}{dx}g^{-1}(x) \\\\\n  &= f_Z \\Bigl(\\frac{x}{\\sigma} \\Bigr) \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\ e^{-(\\frac{x}{\\sigma})^2 \\cdot \\frac{1}{2}} \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{x^2}{2\\sigma^2}}\n\\end{align*}\n\\]\nThis is the pdf for \\(\\mathcal{N}(0, \\sigma^2)\\). Now, we find \\(X = h(X') = f_{X'}(X') + \\mu\\), where \\(h^{-1}(x) = x - \\mu\\) and \\(\\frac{d}{dx}\\ h^{-1}(x) = 1\\). As with the above, we have \\[\nf_X(x) = f_{X'}(h^{-1}(x)) \\frac{d}{dx}\\ h^{-1}(x) = f_{X'}(x - \\mu) = \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}},\n\\]\nwhich is the classical definition of the normal distribution’s probability density function. This means that scaling a normal variable by a constant, or adding a constant to a normal variable results in another normal variable. So, each \\(X_i\\) is normal. Lastly, as one’s intuition might suspect, the sum of two independent normal variables is also normal. I won’t go through the proof here, but some versions can be found on wikipedia.\nThus, we can conclude that a linear combination \\(Y = a_1 X_1 + \\cdots + a_n X_n\\) is a normal variable, meaning \\(\\mathbf{x}\\) is a draw from the multivariate Gaussian distribution.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#r-implementation",
    "href": "gaussian_processes/mvnorm/index.html#r-implementation",
    "title": "The Multivariate Gaussian Distribution",
    "section": "R Implementation",
    "text": "R Implementation\n\nmu &lt;- c(1.1, 3, 8)\nSigma &lt;- matrix(c(1, 0.1, 0.3, 0.1, 1, 0.1, 0.3, 0.1, 1), 3, 3)\n\nmvrnorm &lt;- function(n, mu, Sigma) {\n  L &lt;- chol(Sigma) # this is L', where L'L = Sigma\n  d &lt;- length(mu)\n\n  out &lt;- matrix(0, nrow = n, ncol = d)\n  for (i in 1:n) {\n    u &lt;- rnorm(d)\n    out[i, ] &lt;- t(L %*% u + mu)\n  }\n  return(out)\n}",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "reflections/index.html",
    "href": "reflections/index.html",
    "title": "Reflections",
    "section": "",
    "text": "Here are links to the reflection assignments I completed during MATH 401.\nReflection 1",
    "crumbs": [
      "Home",
      "Reflections"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\nI have a more formal writeup (in the sense of notation) of GPR here\n\nWorking on an overview of KRR\n\ntodos:\n\nupdate citations\n\n\n\n\n\n\n\nReviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing.\n\n\n\n\n\nDiscussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  },
  {
    "objectID": "index.html#upcoming",
    "href": "index.html#upcoming",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\nI have a more formal writeup (in the sense of notation) of GPR here\n\nWorking on an overview of KRR\n\ntodos:\n\nupdate citations"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "MATH 401",
    "section": "",
    "text": "Reviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "MATH 401",
    "section": "",
    "text": "Discussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  }
]
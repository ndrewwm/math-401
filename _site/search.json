[
  {
    "objectID": "notes/template.html",
    "href": "notes/template.html",
    "title": "Summary of progress over the previous week",
    "section": "",
    "text": "Summary of progress over the previous week\n\n\nCurrent Questions\n\n\nResources reviewed/cited"
  },
  {
    "objectID": "krr/index.html",
    "href": "krr/index.html",
    "title": "Kernel Ridge Regression",
    "section": "",
    "text": "library(tidyverse)\nset.seed(123)\n\nf &lt;- function(x) sin(3*x) + sin(7*x)\nk &lt;- function(x_i, x_j, sigma_f = 0.4, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nn &lt;- 400\nx &lt;- seq(-5, 5, length.out = n)\nf_x &lt;- f(x) + rnorm(n, mean = 0, sd = 0.1)\n\n\nn &lt;- 100\nind_train &lt;- sample(1:400, n, replace = FALSE)\nx_train &lt;- x[ind_train]\nf_train &lt;- f_x[ind_train]\n\ncompute_covmat &lt;- function(k, x, y) {\n  n_x &lt;- length(x)\n  n_y &lt;- length(y)\n  K &lt;- matrix(0, n_x, n_y)\n  for (i in 1:n_x) {\n    for (j in 1:n_y) {\n      K[i, j] &lt;- k(x[i], y[j])\n    }\n  }\n  return(K)\n}\n\nk_xx &lt;- compute_covmat(k, x_train, x_train)\n\nlambda &lt;- 0.0001\ncoef &lt;- solve(k_xx + n * lambda * diag(n)) %*% f_train\n\nf_hat &lt;- function(x) {\n  map2_dbl(coef, x_train, ~.x * k(x, .y)) |&gt;\n    reduce(`+`)\n}\n\ny_hat &lt;- map_dbl(x, f_hat)\n\ncolors &lt;- c(\"Actual\" = \"blue\", \"Data\" = \"black\", \"Estimate\" = \"orange\")\n\nout &lt;- tibble(x, f = f(x), f_x, y_hat) |&gt;\n  pivot_longer(-x) |&gt;\n  mutate(\n    name = factor(name, levels = c(\"f\", \"f_x\", \"y_hat\"), labels = names(colors))\n  )\n  \nggplot() +\n  geom_point(data = filter(out, name == \"Data\"), aes(x, y = value, color = name), alpha = 0.2) +\n  geom_line(data = filter(out, name == \"Actual\"), aes(x, y = value, color = name)) +\n  geom_line(data = filter(out, name == \"Estimate\"), aes(x, y = value, color = name)) +\n  scale_color_manual(name = \"\", values = colors) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Kernel Ridge Regression"
    ]
  },
  {
    "objectID": "reflections/index.html",
    "href": "reflections/index.html",
    "title": "Reflections",
    "section": "",
    "text": "Here are links to the reflection assignments I completed during MATH 401.\n\nReflection 1\nReflection 2\n\nEssay\nCV",
    "crumbs": [
      "Home",
      "Reflections"
    ]
  },
  {
    "objectID": "gaussian_processes/bivariate_case/index.html",
    "href": "gaussian_processes/bivariate_case/index.html",
    "title": "Modelling two variables using GPR",
    "section": "",
    "text": "Dr. Gramacy’s textbook, Surrogates (Gramacy 2020), particularly chapter 5, was very helpful in putting together the code for this simulation.\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lhs)\nlibrary(plotly)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\n\n# Our target function\n# x is an N x 2 matrix\nf &lt;- function(x) {\n  x[, 1] * exp(-x[, 1]^2 - x[, 2]^2)\n}\n\n# x_i and x_j can be vectors or scalars \nrbf &lt;- function(xi, xj, alpha = 1, rho = 1) {\n  alpha^2 * exp(-norm(xi - xj, type = \"2\") / (2 * rho^2))\n}\n\nk_XX &lt;- function(X, err = eps) {\n  N &lt;- nrow(X)\n  K &lt;- matrix(0, N, N)\n  for (i in 1:N) {\n    for (j in 1:N) {\n      K[i, j] &lt;- rbf(X[i, ], X[j, ])\n    }\n  }\n\n  if (!is.null(err)) {\n    K &lt;- K + diag(err, ncol(K))\n  }\n\n  K\n}\n\nk_xX &lt;- function(x, X) {\n  N &lt;- nrow(x)\n  M &lt;- nrow(X)\n\n  K &lt;- matrix(0, N, M)\n  for (i in 1:N) {\n    for (j in 1:M) {\n      K[i, j] &lt;- rbf(x[i, ], X[j, ])\n    }\n  }\n\n  K\n}\n\n# Training dataset: D = (X, Y)\nX &lt;- randomLHS(100, 2)\nX[, 1] &lt;- -4 + 8*X[, 1]\nX[, 2] &lt;- -4 + 8*X[, 2]\nY &lt;- f(X)\n\n# Test points: Z\nx &lt;- seq(-4, 4, length.out = 20)\nZ &lt;- expand.grid(x, x) |&gt; as.matrix()\n\nK_XX &lt;- k_XX(X) + diag(eps, nrow(X))\nK_ZX &lt;- k_xX(Z, X)\nK_ZZ &lt;- k_XX(Z)\n\nm_post &lt;- K_ZX %*% solve(K_XX) %*% Y\ns_post &lt;- K_ZZ - K_ZX %*% solve(K_XX) %*% t(K_ZX)\n\n\nresults &lt;- as_tibble(Z) |&gt;\n  rename(x = Var1, y = Var2) |&gt;\n  mutate(z_m = as.vector(m_post), z_sd = sqrt(diag(s_post)))\n\np1 &lt;- ggplot(results, aes(x, y, fill = z_m)) +\n  geom_tile() +\n  scale_fill_viridis_c(name = expression(mu)) +\n  labs(title = \"Posterior mean\")\n\np2 &lt;- ggplot(results, aes(x, y, fill = z_sd)) +\n  geom_tile() +\n  scale_fill_viridis_c(name = expression(sigma)) +\n  labs(title = \"Posterior standard deviation\")\n\np1 | p2\n\n\n\n\n\n\n\n\n\nplot_ly(z = ~matrix(f(Z), ncol = 20)) |&gt; add_surface()\n\n\n\n\nplot_ly(z = matrix(results$z_m, ncol = 20)) |&gt; add_surface()\n\n\n\n\n\n\n\n\n\nReferences\n\nGramacy, Robert B. 2020. Surrogates: Gaussian Process Modeling, Design and  Optimization for the Applied Sciences. Boca Raton, Florida: Chapman Hall/CRC. https://bookdown.org/rbg/surrogates/.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Modelling two variables using GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/v2.html",
    "href": "gaussian_processes/v2.html",
    "title": "Gaussian Process Regression",
    "section": "",
    "text": "Assume our sample space is 400 evenly spaced points between -5 and 5. Let \\(\\mathbf{x}\\) denote the set of \\(N\\)-many points at which we have collected observations, defined as:\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}.\n\\]\nLet \\(\\mathbf{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2)\\) be an \\(N\\)-dimensional vector. Let \\(\\mathbf{y}\\) denote the values of \\(f\\) observed at each \\(x_i\\), with the addition of noise (\\(\\epsilon_i\\)): \\[\n\\mathbf{y} = f(\\mathbf{x}) + \\mathbf{\\epsilon} \\iff \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \\begin{bmatrix} f(x_1) \\\\ f(x_2) \\\\ \\vdots \\\\ f(x_N) \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{bmatrix}.\n\\]\nTogether, \\((\\mathbf{x}, \\mathbf{y})\\) can be referred to as training data. Let \\((\\mathbf{x_*}, \\mathbf{y_*})\\) denote test data (i.e., points \\(x^*_i\\) at which we want to predict a value of \\(y^*_i\\)). We will use \\(M\\) to denote the size of \\(\\mathbf{x_*}\\) and \\(\\mathbf{y_*}\\).\nWe assume that \\(\\mathbf{y}\\) can be modeled by a Gaussian process, i.e., \\(\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{\\Sigma})\\). Let \\(m: \\mathbb{R} \\to \\mathbb{R}\\), and the squared exponential kernel \\(k: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\) be defined as \\[\n\\begin{align*}\nm(x) &= 0, \\\\\nk(x, x') &= \\sigma \\cdot exp\\Bigl(-\\frac{1}{2l^2}(x - x')^2\\Bigr)\n\\end{align*}\n\\]\nwith \\(\\sigma\\) and \\(l\\) as hyperparameters.\n\n\nTo-do: elaborate on what \\(\\sigma\\) and \\(l\\) control within \\(k\\).\nThe conditional distribution \\(\\mathbf{y_*} | \\mathbf{y}\\) can be defined as another Gaussian process: \\(\\mathbf{y}_* | \\mathbf{y} \\sim \\mathcal{N}(\\bar{m}, \\bar{\\Sigma})\\) with \\(\\bar{m}\\) and \\(\\bar{\\Sigma}\\) are defined as\n\\[\n\\begin{align*}\n\\bar{m} &= K(\\mathbf{x_*}, \\mathbf{x}) (K(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}, \\\\\n\\bar{\\Sigma} &= K(\\mathbf{x_*}, \\mathbf{x_*}) - K(\\mathbf{x_*}, \\mathbf{x})(K(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I})^{-1} K(\\mathbf{x}, \\mathbf{x_*}),\n\\end{align*}\n\\]\n\n\nIn Kanagawa et al. (2018), they note that the final term in \\(\\bar{m}\\) should be something like \\((\\mathbf{y} - m_\\mathbf{x})\\) where \\(m_\\mathbf{x} = [m(x_1), \\dots, m(x_N)]^\\top\\). I’ve omitted the subtraction given that \\(m(\\mathbf{x}) = \\mathbf{0}\\).\nwhere\n\\[\n\\begin{align*}\nK(\\mathbf{x_*}, \\mathbf{x}) &= [k(x^*_i, x_j)]_{i,j = 1}^{M,N} \\in \\mathbb{R}^{M \\times N}, \\\\\nK(\\mathbf{x}, \\mathbf{x_*}) &= [k(x_i, x^*_j)]_{i,j = 1}^{N,M} \\in \\mathbb{R}^{N \\times M}, \\\\\nK(\\mathbf{x}, \\mathbf{x}) &= [k(x_i, x_j)]_{i,j = 1}^{N,N} \\in \\mathbb{R}^{N \\times N}, \\text{ and } \\\\\nK(\\mathbf{x_*}, \\mathbf{x_*}) &= [k(x^*_i, x^*_j)]_{i,j}^{M,M} \\in \\mathbb{R}^{M \\times M}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nNon-functional code\n\n\n\nI haven’t been able to get a comparable implementation in Julia to work. The issue appears to be roundoff errors, with some entries of the posterior covariance matrix being very small, but negative.\n\n\nWe’ll now apply these definitions to simulated data. For this simulation, we’ll arbitrarily pick \\(\\sigma = 2\\) and \\(l = 0.4\\) as our hyperparameter values.\n\n\nThe functions \\(m\\) and \\(k\\) are themselves hyperparameters? So, in the case of \\(\\sigma\\) and \\(l\\), are they “sub-hyperparameters”?\n\nusing LinearAlgebra, Distributions, Random, Plots\n\nσ = 0.4;\nl = 0.4;\n\nf(x) = sin(3x) + sin(7x);\n\nx_all = Vector(range(-5, 5, 400));\nx = sort(rand(x_all, 50));\nx_star = sort(setdiff(x_all, x));\n\nN = length(x);\nM = length(x_star);\n\nϵ = rand(Normal(0, σ), N);\nϵ_star = rand(Normal(0, σ), M);\n\ny = f.(x) + ϵ;\ny_star = f.(x_star) + ϵ_star;\n\n\nplot(x_all, f.(x_all))\nscatter!(vcat(x, x_star), vcat(y, y_star))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaving defined \\((\\mathbf{x}, \\mathbf{y})\\) and \\((\\mathbf{x_*}, \\mathbf{y_*})\\), we can now compute each \\(K\\).\n\nk(x_i, x_j) = σ * exp(-(x_i - x_j)^2 / (2 * l^2));\n\nfunction compute_covmat(k::Function, X::Vector, Y::Vector)::Matrix\n    n_x = length(X);\n    n_y = length(Y);\n    K = zeros(n_x, n_y);\n    for i = 1:n_x\n        for j = 1:n_y\n            K[i, j] = k(X[i], Y[j]);\n        end\n    end\n    return K;\nend;\n\nK = compute_covmat(k, x, x);\nK_star = compute_covmat(k, x_star, x);\nK_star2 = compute_covmat(k, x_star, x_star);\n\n\nm_post = K_star * inv(K + σ^2 * I) * y;\ncov_post = K_star2 - K_star * inv(K + σ^2 * I) * K_star';\n\ndisplay(cov_post)\n\n# cov_post = cov_post - minimum(eigvals(Symmetric(cov_post))) * I;\n\n# rand(MultivariateNormal(m_post, cov_post), 10)\n\n351×351 Matrix{Float64}:\n  0.203458      0.195848      0.187314     …  1.13036e-15  1.01467e-15\n  0.195848      0.189374      0.181968        2.17469e-15  1.95633e-15\n  0.187314      0.181968      0.175691        3.27675e-15  2.95011e-15\n  0.17796       0.173715      0.168556        4.41647e-15  3.97792e-15\n  0.167901      0.164717      0.160647        5.56986e-15  5.01816e-15\n  0.157259      0.155082      0.15206      …  6.70923e-15  6.04586e-15\n  0.146164      0.14493       0.1429          7.80343e-15  7.03294e-15\n  0.134752      0.134384      0.133279        8.81839e-15  7.94868e-15\n  0.123159      0.123571      0.123313        9.71777e-15  8.76032e-15\n  0.11152       0.11262       0.113123        1.04638e-14  9.43379e-15\n  0.0886203     0.090804      0.092544     …  1.13439e-14  1.02293e-14\n  0.0776025     0.0801771     0.0823863       1.14057e-14  1.0286e-14\n  0.067018      0.0698839     0.0724613       1.11721e-14  1.00766e-14\n  ⋮                                        ⋱               ⋮\n -9.44505e-16  -1.83887e-15  -2.78306e-15     0.0151902    0.0125775\n -5.13807e-16  -1.0149e-15   -1.54415e-15  …  0.0198644    0.0173185\n  2.37941e-16   4.26845e-16   6.25642e-16     0.0301797    0.0279962\n  8.01725e-16   1.51236e-15   2.26175e-15     0.0414494    0.0399572\n  1.00543e-15   1.90631e-15   2.85649e-15     0.0472927    0.0462729\n  1.15665e-15   2.20011e-15   3.3008e-15      0.0531858    0.0527209\n  1.25707e-15   2.39683e-15   3.59919e-15  …  0.0590578    0.0592269\n  1.30963e-15   2.50194e-15   3.75984e-15     0.0648365    0.0657133\n  1.31829e-15   2.5229e-15    3.79385e-15     0.0704496    0.0721006\n  1.28775e-15   2.46861e-15   3.71459e-15     0.0758264    0.0783095\n  1.13036e-15   2.17469e-15   3.27675e-15     0.0856039    0.0898831\n  1.01467e-15   1.95633e-15   2.95011e-15  …  0.0898831    0.0951026"
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html",
    "href": "gaussian_processes/multioutput/index.html",
    "title": "Multioutput GPR",
    "section": "",
    "text": "Elsewhere, I’ve discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times P}\\) is used to model \\(\\mathbf{y} \\in \\mathbb{R}^N\\): \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{X}) + \\mathbf{\\epsilon} \\text{ where } \\\\\nf(\\mathbf{X}) &\\sim \\mathcal{N}_N(\\mathbf{0}, \\mathbf{K}), \\\\\n\\mathbf{\\epsilon} &\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_y), \\text{ and } \\\\\n\\mathbf{K} &= [k(x_i, x_j)]_{ij}^N.\n\\end{align*}\n\\]\n\n\nThroughout this page, I’ll be using the subscript after \\(\\mathcal{N}\\) to indicate we’re looking at a multivariate normal distribution, and to show the dimension of the vectors that the distribution produces.\nWhile interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we’ll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the gcookbook package:\n\n\nThis example and dataset is drawn from The R Graphics Cookbook (Chang 2018), specifically chapter 13.12, on creating vector fields.\n\nThis data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.\n\nBelow I’ve plotted the x and y components of the storm’s velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm’s winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector: \\[\n\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}.\n\\]\nVelocity captures both the direction of the wind, and its magnitude (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we’re looking at, such as the storm’s spiral shape.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\n\n# Keep a subset in the middle of the the z-axis\nd_isabel &lt;- isabel |&gt;\n  filter(z == 10.035) |&gt;\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n &lt;- function(x, by = 4) {\n  x &lt;- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x &lt;- every_n(unique(isabel$x))\nkeep_y &lt;- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd &lt;- d_isabel |&gt;\n  filter(x %in% keep_x, y %in% keep_y) |&gt;\n  mutate(index = 1:n())\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 &lt;- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0\n\n\n\n\n\n\n\n\n\nA similar consideration applies to the task of predicting velocity using statistical models. A “naive” approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, \\(f_1: \\mathbb{R}^2 \\to \\mathbb{R}\\) and \\(f_2: \\mathbb{R}^2 \\to \\mathbb{R}\\). This amounts to treating \\(v_x\\) and \\(v_y\\) as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.\nAn improved approach would allow us to incorporate natural information on how our observed target values (each \\(v_x\\) and \\(v_y\\)) might interrelate. The approach should also (ideally) account for the fact that we’re trying to predict a vector at a specific point. In other words, we’d like a single function, whose outputs are vector-valued:\n\\[\n\\underbrace{\\begin{aligned}[c]\nv_x = f_1\\Bigl((x, y)^\\top \\Bigr) + \\epsilon \\\\\nv_y = f_2\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Naive approach}}\n\\qquad\\longrightarrow\\qquad\n\\underbrace{\\begin{aligned}[c]\n\\begin{bmatrix}v_x \\\\ v_y\\end{bmatrix} = f\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Vector-valued approach}}\n\\]\nThese characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as “multioutput”, “multitask”, “cokriging”, or “vector-valued” GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics (Alvarez et al. 2012). In the following section, we’ll cover the theory behind the approach as it applies to our example data.\n\n\n\nWe are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times 2}\\), and we may refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N}\\) and \\(\\mathbf{x} = \\text{vec}\\ X \\in \\mathbb{R}^{2N}\\). We will use \\(\\mathbf{X}_* \\in \\mathbb{R}^{M \\times 2}\\) to denote a set of test points, for which we want to generate predictions. As with the conventions for the training data, we will use \\(\\mathbf{x}_* = \\text{vec}\\ \\mathbf{X}_* \\in \\mathbb{R}^{2M}\\).\nWe are engaged in a regression task, i.e., we’re attempting to learn the functional relationship \\(f\\) between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), with an assumption that this relationship has been corrupted to some degree by noise. For the following, I’ve adapted notation from Chapter 2.2 in the well-known Rasmussen and Williams text to the context of our multidimensional \\(\\mathbf{Y}\\) (Williams and Rasmussen 2006). Setting up the problem, we have \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\\\\nf &\\sim \\mathcal{GP}(m, k) \\\\\nf(\\mathbf{x}) &\\sim \\mathcal{N}_{2N}(\\mathbf{0}, \\mathbf{K}_{XX})\n\\end{align*}\n\\]\nTo parse these statements, we assume \\(f\\) is drawn from a Gaussian Process with mean function \\(m\\) and covariance (kernel) function \\(k\\). Thus, our observations \\(\\mathbf{y} = f(\\mathbf{x})\\) have a multivariate normal probability distribution, parameterized by a covariance matrix \\(\\mathbf{K}_{XX}\\). By convention, we assume the mean vector is \\(\\mathbf{0}\\) (implying that \\(m\\) is the zero function).\nThe joint distribution of \\(\\mathbf{y}\\) and our predictions for the test point(s) \\(\\mathbf{y}_*\\) is also described by a multivariate normal distribution, specified as: \\[\n\\begin{align*}\n\\begin{bmatrix}\n  \\mathbf{y} \\\\\n  \\mathbf{y_*}\n\\end{bmatrix} &\\sim \\mathcal{N}_{2N + 2M}\\Biggl( \\mathbf{0}, \\begin{bmatrix}\n  \\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N & \\mathbf{K}_{XX_*} \\\\\n  \\mathbf{K}_{X_*X} & \\mathbf{K}_{X_* X_*}\n\\end{bmatrix} \\Biggr) \\\\\n\\mathbf{V} &= \\begin{bmatrix} \\sigma^2_1 & 0 \\\\ 0 & \\sigma^2_2 \\end{bmatrix}\n\\end{align*}\n\\]\nHere, \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the independent and additive noise associated with each of our outcome’s components. The \\(\\otimes\\) symbol denotes the Kronecker product. From this joint distribution, we can derive the conditional distribution for \\(\\mathbf{y}_*\\):\n\\[\n\\begin{align*}\n\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*} &\\sim \\mathcal{N}_{2M}(\\bar{\\mathbf{f}}_*,\\ \\text{cov}(\\mathbf{f}_*)) \\\\\n\\bar{\\mathbf{f}}_* &= \\mathbb{E}[\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*}] = \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1} \\mathbf{y} \\\\\n\\text{cov}(\\mathbf{f}_*) &= \\mathbf{K}_{X_*X_*} - \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1}\\mathbf{K}_{XX_*} \\\\\n\\end{align*}\n\\]\nWe will now work through how each piece of our distribution is defined. Let \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be a (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(-\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n\\tag{1}\\]\nwhere \\(\\alpha, \\rho &gt; 0\\), and \\(\\| \\cdot \\|_2\\) is the Euclidean norm. This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let \\(\\mathbf{k}_{XX} \\in \\mathbb{R}^{N \\times N}\\), the covariance matrix between all points in \\(\\mathbf{X}\\), be defined as \\[\n\\mathbf{k}_{XX} = (k(x_i, x_j))_{i,j}^N = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\tag{2}\\]\nLet \\(\\mathbf{B} \\in \\mathbb{R}^{2 \\times 2}\\), the matrix of similarities between the outputs \\(\\mathbf{Y}\\), be defined as \\[\n\\mathbf{B} = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix} = \\frac{1}{N} \\mathbf{Y}^\\top \\mathbf{k}_{XX} \\mathbf{Y}.\n\\tag{3}\\]\nThe expression for \\(\\mathbf{B}\\) comes from Bonilla et al. (Bonilla, Chai, and Williams 2007), in section 2.3 (the authors use the notation \\(K^f\\) and \\(K^x\\) instead of \\(\\mathbf{B}\\) and \\(\\mathbf{k}_{XX}\\), respectively).\nLet \\(\\mathbf{K}_{XX} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{K}_{XX} = \\mathbf{B} \\otimes \\mathbf{k}_{XX}.\n\\tag{4}\\]\nWe can now define the other pieces needed to establish the covariance matrix used for the joint distribution of \\(\\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{y}_* \\end{bmatrix}\\):\n\\[\n\\begin{align*}\n  \\mathbf{K}_{X_*X} &= \\mathbf{B} \\otimes (k(x_{*i}, x_j))_{i,j=1}^{M,N} \\in \\mathbb{R}^{2M \\times 2N} \\\\\n  \\mathbf{K}_{XX_*} &= \\mathbf{K}_{X_*X}^\\top \\in \\mathbb{R}^{2N \\times 2M} \\\\\n  \\mathbf{K}_{X_*X_*} &= \\mathbf{B} \\otimes (k(x_{*i}, x_{*j}))_{i,j=1}^{M} \\in \\mathbb{R}^{2M \\times 2M}.\n\\end{align*}\n\\tag{5}\\]",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#multitask-gaussian-process-prediction",
    "href": "gaussian_processes/multioutput/index.html#multitask-gaussian-process-prediction",
    "title": "Multioutput GPR",
    "section": "",
    "text": "We are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(X, Y \\in \\mathbb{R}^{N \\times 2}\\), and it may be convenient to refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N \\times 1}\\). Let \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be the (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n\\]\nwhere \\(\\alpha, \\rho &gt; 0\\); again, \\(x_i, x_j \\in \\mathbb{R}^2\\). This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let the covariance matrix between all points in \\(\\mathbf{X}\\), \\(K_{XX} \\in \\mathbb{R}^{N \\times N}\\), be defined as \\[\nK_{XX} = K(\\mathbf{X}, \\mathbf{X}) = (k(x_i, x_j))_{i,j} = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\]\nLet the matrix of similarities between the outputs \\(\\mathbf{Y}\\), \\(B \\in \\mathbb{R}^{2 \\times 2}\\) be defined as \\[\nB = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix}.\n\\]\nLet \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{\\Sigma} = B \\otimes K_{XX} + \\begin{bmatrix}\n  \\sigma^2_1 & 0 \\\\\n  0 & \\sigma^2_2\n\\end{bmatrix} \\otimes I_N,\n\\]\nwhere \\(\\otimes\\) is the Kronecker product, \\(I\\) is the identity matrix, and \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the variances of the noise for each component of \\(Y\\).\nFor a single new vector, \\(x_*\\)",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#visualizing-vector-fields",
    "href": "gaussian_processes/multioutput/index.html#visualizing-vector-fields",
    "title": "Multioutput GPR",
    "section": "",
    "text": "This example and dataset is drawn from The R Graphics Cookbook (Chang 2018), specifically chapter 13.12, on creating vector fields.\n\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\nglimpse(isabel)\n\nRows: 156,250\nColumns: 8\n$ x     &lt;dbl&gt; -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83,…\n$ y     &lt;dbl&gt; 41.70000, 41.55571, 41.41142, 41.26713, 41.12285, 40.97856, 40.8…\n$ z     &lt;dbl&gt; 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0…\n$ vx    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ vy    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ vz    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ t     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ speed &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\n# Keep a subset in the middle of the the z-axis\nd_isabel &lt;- isabel |&gt;\n  filter(z == 10.035) |&gt;\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n &lt;- function(x, by = 4) {\n  x &lt;- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x &lt;- every_n(unique(isabel$x))\nkeep_y &lt;- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd &lt;- d_isabel |&gt;\n  filter(x %in% keep_x, y %in% keep_y) |&gt;\n  mutate(index = 1:n())\n\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 &lt;- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/index.html",
    "href": "gaussian_processes/index.html",
    "title": "Overview of Gaussian Processes",
    "section": "",
    "text": "This page relies heavily on Kanagawa, Hennig, Sejdinovic, and Sriperumbudur (2018).\n\nDefinition 2.2: Gaussian Process\nLet \\(\\chi\\) be a nonempty set, \\(k: \\chi \\times \\chi \\to \\mathbb{R}\\) a positive definite kernel and \\(m: \\chi \\to \\mathbb{R}\\) be any real-valued function. Then a random function \\(f: \\chi \\to \\mathbb{R}\\) is said to be a Gaussian process (GP) with mean function \\(m\\) and covariance kernel \\(k\\), denoted by \\(\\mathcal{GP}(m, k)\\), if the following holds: For any finite set \\(X = (x_1, \\dots, x_n) \\subset \\chi\\) of any size \\(n \\in \\mathbb{N}\\), the random vector\n\\[\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n\\]\nfollows the multivariate normal distribution \\(\\mathcal{N}(m_X, k_{XX})\\) with covariance matrix \\(k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}\\) and mean vector \\(m_X = (m(x_1), \\dots, m(x_n))^\\top\\).\n\n\nGaussian Process Regression\n\nalso called kriging or Wiener-Kolmogorov prediction\n\nRegression is the task of estimating of an unknown function \\(f\\) based on a provided set of training data, \\((X, Y)\\), where \\(X = (x_1, \\dots, x_n)^\\top\\) and \\(Y = (y_1, \\dots, y_n)^\\top\\) are random vectors (\\(x_i\\) and \\(y_i\\) are realizations, collected by the experimenter). Regression assumes the presence of noise denoted by \\(\\epsilon\\), which completes the additive model:\n\\[\ny_i = f(x_i) + \\epsilon.\n\\]\nIt’s typically assumed that \\(\\epsilon\\) is normally distributed with mean 0, i.e., \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\).\nGaussian process regression is a Bayesian approach that uses a GP as a prior distribution for \\(f\\).\n\n\n\n\n\n\n\n\n\nPrior\nPosterior\n\n\n\n\nHyperparameter: kernel\n\\(k\\)\n\\(\\bar{k}\\)\n\n\nHyperparameter: mean function\n\\(m\\)\n\\(\\bar{m}\\)\n\n\nDistribution\n\\(\\mathcal{GP}(m, k)\\)\n\\(\\mathcal{GP}(\\bar{m}, \\bar{k})\\)\n\n\n\nThe authors show that the posterior distribution \\(f|Y \\sim \\mathcal{GP}(\\bar{m}, \\bar{k})\\) where \\(\\bar{m} : \\chi \\to \\mathbb{R}\\) and \\(\\bar{k}: \\chi \\times \\chi \\to \\mathbb{R}\\) is given by\n\\[\n\\begin{align*}\n\\bar{m}(x) &= m(x) + k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}(Y - m_X),\\ x \\in \\chi, \\\\\n\\bar{k}(x, x') &= k(x, x') - k_{xX}(k_{XX} + \\sigma^2I_n)^{-1}k_{Xx'},\\ x,x' \\in \\chi,\n\\end{align*}\n\\]\nwhere \\(k_{Xx} = k_{xX}^\\top = (k(x_1, x), \\dots, k(x_n, x))^\\top\\).\nThis is interesting, because these are closed-form expressions, and notably, we haven’t made use of Bayes’s Rule. Assuming the kernel and mean functions aren’t expensive or difficult to evaluate, the computation is just linear algebra.\n\n\nDrawing from a Gaussian Process\n\nlibrary(tidyverse)\nset.seed(123)\n\nf &lt;- function(x) sin(3*x) + sin(7*x)\nk &lt;- function(x_i, x_j, sigma_f = 1, l = 0.4) sigma_f * exp(-(x_i - x_j)^2 / (2 * l^2))\n\nN &lt;- 400\nn &lt;- 120\nx &lt;- seq(-5, 5, length.out = N)\nf_x &lt;- f(x)\n\nsigma &lt;- 0.15\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- f_x + epsilon\n\nd &lt;- tibble(x, f_x, y)\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = f_x), color = \"blue\") +\n  geom_point(aes(y = y))\n\n\n\n\n\n\n\n\nHere we’ll compute the posterior distribution over the whole range \\([-5, 5]\\), using \\(n = 100\\) training data points.\n\nind_train &lt;- sample(1:400, n, replace = FALSE)\nx_train &lt;- x[ind_train]\ny_train &lt;- y[ind_train]\n\ncompute_covmat &lt;- function(k, x, y) {\n  n_x &lt;- length(x)\n  n_y &lt;- length(y)\n  K &lt;- matrix(0, n_x, n_y)\n  for (i in 1:n_x) {\n    for (j in 1:n_y) {\n      K[i, j] &lt;- k(x[i], y[j])\n    }\n  }\n  return(K)\n}\n\nk_XX &lt;- compute_covmat(k, x_train, x_train)\nk_xx &lt;- compute_covmat(k, x, x)\nk_xX &lt;- compute_covmat(k, x, x_train)\n\nm_post &lt;- k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% y_train\ncov_post &lt;- k_xx - k_xX %*% solve(k_XX + sigma^2 * diag(n)) %*% t(k_xX)\n\nHere m_post represents the mean vector of the posterior distibution, and cov_post represents the posterior’s covariance matrix.\n\n\nIn Kanagawa et al. (2018), they note that the final term in the posterior mean function \\(\\bar{m}\\) should be something like \\((\\mathbf{y} - m_\\mathbf{x})\\) where \\(m_\\mathbf{x} = [m(x_1), \\dots, m(x_N)]^\\top\\). I’ve omitted the subtraction given that \\(m(\\mathbf{x}) = \\mathbf{0}\\).\nBelow in blue is the actual function \\(f\\), with 50 draws (grey) from the posterior distribution. The posterior mean of these draws is plotted as the orange line.\n\nf_post &lt;- MASS::mvrnorm(50, m_post, cov_post)\n\ndraws &lt;- as_tibble(t(f_post)) |&gt;\n  mutate(x = x, y = f(x)) |&gt;\n  pivot_longer(V1:V50)\n\npost_mean &lt;- draws |&gt;\n  group_by(x) |&gt;\n  summarize(post_mean = mean(value))\n\ndraws |&gt;\n  ggplot(aes(x = x)) +\n  geom_point(\n    data = tibble(x = x[ind_train], y = y[ind_train]),\n    aes(x, y), alpha = 0.2\n  ) +\n  geom_line(aes(y = value, group = name), alpha = 0.1) +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_line(data = post_mean, aes(x = x, y = post_mean), color = \"orange\")\n\n\n\n\n\n\n\n\n\nf_post &lt;- MASS::mvrnorm(n = 1000, m_post, cov_post)\n\ndraws &lt;- as_tibble(t(f_post)) |&gt;\n  mutate(x = x, y = f(x)) |&gt;\n  pivot_longer(V1:V1000)\n\npost_summaries &lt;- draws |&gt;\n  group_by(x) |&gt;\n  summarize(\n    post_mean = mean(value),\n    post_sd = sd(value)\n  )\n\npost_summaries &lt;- draws |&gt;\n  distinct(x, y) |&gt;\n  left_join(post_summaries, by = \"x\")\n\nggplot(data = post_summaries, aes(x = x)) +\n  geom_ribbon(aes(ymin = post_mean - 2*post_sd, ymax = post_mean + 2*post_sd), fill = \"grey70\") +\n  geom_line(aes(y = post_mean), color = \"orange\") +\n  geom_line(aes(y = y), color = \"blue\", lty = \"dashed\") +\n  geom_point(data = tibble(x = x[ind_train], y = y[ind_train]), aes(x, y), alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\nCitations (WIP)\n\nhttps://juanitorduz.github.io/gaussian_process_reg/\nhttps://www.cs.toronto.edu/~duvenaud/cookbook/\nhttps://gregorygundersen.com/blog/2019/06/27/gp-regression/\nhttps://gregorygundersen.com/blog/2019/09/12/practical-gp-regression/",
    "crumbs": [
      "Home",
      "Gaussian Processes"
    ]
  },
  {
    "objectID": "gaussian_processes/gaussian_process_regression/index.html",
    "href": "gaussian_processes/gaussian_process_regression/index.html",
    "title": "Gaussian Processes and Gaussian Process Regression",
    "section": "",
    "text": "Elsewhere, I described the multivariate Gaussian distribution, which can be used to model random vectors of jointly distributed Gaussian random variables. A Gaussian Process (GP) can be used to model functions over arbitrarily sized sets. They’re an extension of the multivariate Gaussian, in which we move from thinking about vectors of a fixed dimension to functions that describe random vectors of any (potentially infinite) size. GPs are parameterized with a mean function and covariance kernel, rather than a fixed mean vector \\(\\mu\\) and covariance matrix \\(\\mathbf{\\Sigma}\\).\n\n\n\n\n\nLet \\(\\chi\\) be a nonempty set, \\(k: \\chi \\times \\chi \\to \\mathbb{R}\\) a positive definite kernel and \\(m: \\chi \\to \\mathbb{R}\\) be any real-valued function. Then, a random function \\(f: \\chi \\to \\mathbb{R}\\) is said to be a Gaussian process (GP) with mean function \\(m\\) and covariance kernel \\(k\\), denoted by \\(\\mathcal{GP}(m, k)\\), if the following holds: For any finite set \\(X = (x_1, \\dots, x_n) \\subset \\chi\\) of any size \\(n \\in \\mathbb{N}\\), the random vector\n\\[\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n\\]\nfollows the multivariate normal distribution \\(\\mathcal{N}(m_X, k_{XX})\\) with covariance matrix \\(k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}\\) and mean vector \\(m_X = (m(x_1), \\dots, m(x_n))^\\top\\).\n\n\n\nBy convention, the zero function is used for \\(m(x)\\), allowing the GP to be fully described by the choice of kernel (and any associated hyperparameters).",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Gaussian Processes and Gaussian Process Regression"
    ]
  },
  {
    "objectID": "gaussian_processes/gaussian_process_regression/index.html#gaussian-processes",
    "href": "gaussian_processes/gaussian_process_regression/index.html#gaussian-processes",
    "title": "Gaussian Processes and Gaussian Process Regression",
    "section": "",
    "text": "Elsewhere, I described the multivariate Gaussian distribution, which can be used to model random vectors of jointly distributed Gaussian random variables. A Gaussian Process (GP) can be used to model functions over arbitrarily sized sets. They’re an extension of the multivariate Gaussian, in which we move from thinking about vectors of a fixed dimension to functions that describe random vectors of any (potentially infinite) size. GPs are parameterized with a mean function and covariance kernel, rather than a fixed mean vector \\(\\mu\\) and covariance matrix \\(\\mathbf{\\Sigma}\\).\n\n\n\n\n\nLet \\(\\chi\\) be a nonempty set, \\(k: \\chi \\times \\chi \\to \\mathbb{R}\\) a positive definite kernel and \\(m: \\chi \\to \\mathbb{R}\\) be any real-valued function. Then, a random function \\(f: \\chi \\to \\mathbb{R}\\) is said to be a Gaussian process (GP) with mean function \\(m\\) and covariance kernel \\(k\\), denoted by \\(\\mathcal{GP}(m, k)\\), if the following holds: For any finite set \\(X = (x_1, \\dots, x_n) \\subset \\chi\\) of any size \\(n \\in \\mathbb{N}\\), the random vector\n\\[\nf_X = (f(x_1), \\dots, f(x_n))^\\top \\in \\mathbb{R}^n\n\\]\nfollows the multivariate normal distribution \\(\\mathcal{N}(m_X, k_{XX})\\) with covariance matrix \\(k_{XX} = [k(x_i, x_j)]_{i, j = 1}^{n} \\in \\mathbb{R}^{n \\times n}\\) and mean vector \\(m_X = (m(x_1), \\dots, m(x_n))^\\top\\).\n\n\n\nBy convention, the zero function is used for \\(m(x)\\), allowing the GP to be fully described by the choice of kernel (and any associated hyperparameters).",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Gaussian Processes and Gaussian Process Regression"
    ]
  },
  {
    "objectID": "gaussian_processes/gaussian_process_regression/index.html#gaussian-process-regression",
    "href": "gaussian_processes/gaussian_process_regression/index.html#gaussian-process-regression",
    "title": "Gaussian Processes and Gaussian Process Regression",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Gaussian Processes and Gaussian Process Regression"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html",
    "href": "gaussian_processes/mvnorm/index.html",
    "title": "The Multivariate Gaussian Distribution",
    "section": "",
    "text": "Let \\(\\mathbf{x} \\in \\mathbb{R}^n\\) be a vector whose elements, \\(X_i\\), are random variables (i.e., \\(\\mathbf{x}\\) is a random vector). As elements of \\(\\mathbf{x}\\), we say that \\(X_1, X_2, \\dots, X_n\\) are jointly distributed. Define the vector \\(\\mathbf{\\mu}\\) and matrix \\(\\mathbf{\\Sigma}\\) as \\[\n\\begin{align*}\n\\mathbf{\\mu} &= \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix} = \\begin{bmatrix} \\mathbb{E}(X_1) \\\\ \\mathbb{E}(X_2) \\\\ \\vdots \\\\ \\mathbb{E}(X_n) \\end{bmatrix} \\in \\mathbb{R}^n \\\\ \\\\\n\\mathbf{\\Sigma} &= Cov(\\mathbf{x}) = \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] = \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^n \\in \\mathbb{R}^{n \\times n}\n\\end{align*}\n\\]\nand suppose \\(X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})\\). If every linear combination of \\(\\mathbf{x}\\)’s elements results in a univariate normal variable, we say that \\(\\mathbf{x}\\) comes from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), written as \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma}).\\)",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#definition",
    "href": "gaussian_processes/mvnorm/index.html#definition",
    "title": "The Multivariate Gaussian Distribution",
    "section": "",
    "text": "Let \\(\\mathbf{x} \\in \\mathbb{R}^n\\) be a vector whose elements, \\(X_i\\), are random variables (i.e., \\(\\mathbf{x}\\) is a random vector). As elements of \\(\\mathbf{x}\\), we say that \\(X_1, X_2, \\dots, X_n\\) are jointly distributed. Define the vector \\(\\mathbf{\\mu}\\) and matrix \\(\\mathbf{\\Sigma}\\) as \\[\n\\begin{align*}\n\\mathbf{\\mu} &= \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix} = \\begin{bmatrix} \\mathbb{E}(X_1) \\\\ \\mathbb{E}(X_2) \\\\ \\vdots \\\\ \\mathbb{E}(X_n) \\end{bmatrix} \\in \\mathbb{R}^n \\\\ \\\\\n\\mathbf{\\Sigma} &= Cov(\\mathbf{x}) = \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] = \\Bigl[Cov(X_i, X_j) \\Bigr]_{i, j = 1}^n \\in \\mathbb{R}^{n \\times n}\n\\end{align*}\n\\]\nand suppose \\(X_i \\sim \\mathcal{N}(\\mu_i, \\mathbf{\\Sigma}_{i, i})\\). If every linear combination of \\(\\mathbf{x}\\)’s elements results in a univariate normal variable, we say that \\(\\mathbf{x}\\) comes from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), written as \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma}).\\)",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#sampling-algorithm",
    "href": "gaussian_processes/mvnorm/index.html#sampling-algorithm",
    "title": "The Multivariate Gaussian Distribution",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nA commonly used algorithm for sampling from a multivariate Gaussian distribution can be described as the following, given a vector \\(\\mathbf{\\mu} \\in \\mathbb{R}^n\\) and a positive-(semi)definite matrix \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\):\n\nCompute \\(L L^\\top = \\mathbf{\\Sigma}\\), the Cholesky decomposition of \\(\\mathbf{\\Sigma}\\).\nGenerate \\(\\mathbf{z} = \\begin{bmatrix} Z_1 \\\\ Z_2 \\\\ \\vdots \\\\ Z_n \\end{bmatrix}\\) where \\(Z_1, Z_2, \\dots, Z_n \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1)\\).\nCompute \\(\\mathbf{x} = L\\mathbf{z} + \\mathbf{\\mu}\\).",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#why-does-the-algorithm-work",
    "href": "gaussian_processes/mvnorm/index.html#why-does-the-algorithm-work",
    "title": "The Multivariate Gaussian Distribution",
    "section": "Why does the algorithm work?",
    "text": "Why does the algorithm work?\nThe claim is that \\(\\mathbf{x}\\) is a draw from the multivariate Gaussian distribution parameterized by \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\), i.e., \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). So, why does this work? First, we can show that \\(\\mathbb{E}(\\mathbf{x}) = \\mathbf{\\mu}\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\mathbf{x}) &= \\mathbb{E}(L\\mathbf{z} + \\mathbf{\\mu}) \\\\\n  &= \\mathbb{E}(L\\mathbf{z}) + \\mathbb{E}(\\mathbf{\\mu}) \\\\\n  &= L \\cdot \\mathbb{E}(\\mathbf{z}) + \\mathbf{\\mu} \\\\\n  &= L \\cdot 0 + \\mathbf{\\mu} \\\\\n  &= \\mathbf{\\mu}.\n\\end{align*}\n\\]\nNext, we can see that \\(Cov(\\mathbf{x}) = \\mathbf{\\Sigma}\\):\n\n\nReminder that \\(Cov(\\mathbf{x})\\) is not a scalar, rather, it is the covariance matrix (or variance-covariance matrix) of \\(\\mathbf{x}\\).\n\\[\n\\begin{align*}\nCov(\\mathbf{x}) &= \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[(L\\mathbf{z} + \\mathbf{\\mu} - \\mathbf{\\mu})(L\\mathbf{z} + \\mathbf{\\mu} - \\mathbf{\\mu})^\\top] \\\\\n  &= \\mathbb{E}[L\\mathbf{z}\\mathbf{z}^\\top L^\\top] \\\\\n  &= L\\ \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top]\\ L^\\top \\\\\n  &= L\\ \\mathbb{E}[(\\mathbf{z} - 0)(\\mathbf{z} - 0)^\\top]\\ L^\\top \\\\\n  &= L\\ Cov(\\mathbf{z})\\ L^\\top \\\\\n  &= L I L^\\top \\\\\n  &= L L^\\top \\\\\n  &= \\mathbf{\\Sigma}.\n\\end{align*}\n\\]\n\n\nThis proof hinges on the definition of covariance and the independence of the \\(Z_i\\)’s. Remember, \\(Cov(Z_i, Z_i) = Var(Z_i) = 1\\) and \\(Cov(Z_i, Z_j) = 0\\) when \\(i \\neq j\\) and \\(Z_i \\perp Z_j\\).\nThe following references ((https://math.stackexchange.com/users/10117/lepidopterist), n.d.), ((https://math.stackexchange.com/users/21550/hkbattousai), n.d.), (ibilion[at]purdue.edu 2022) (specifically this section) were helpful to me as I worked through understanding the algorithm and the proofs for the two results above.\nHaving shown that the expectation and (co)variance of \\(\\mathbf{x}\\) are what we’d assume, we need to show that \\(\\mathbf{x}\\) qualifies as multivariate normal. A random vector is multivariate normal if any linear combination of its elements is normally distributed. From its construction, we know \\(\\mathbf{x}\\) is a linear transformation of a standard normal vector, i.e., \\(\\mathbf{z}\\). We can decompose \\(\\mathbf{x}\\)’s \\(i\\)-th element, \\(X_i\\), into the following:\n\\[\n\\begin{align*}\n\\sigma_i &= \\sum_{j = 0}^n L_{i,j} \\\\\nX_i &= Z_i \\sigma_i + \\mu_i.\n\\end{align*}\n\\]\nWe can then show that \\(X_i \\sim \\mathcal{N}(\\sigma_i \\cdot 0 + \\mu_i, \\sigma_i^2) \\implies X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\). This follows from theorems regarding the transformation of random variables (Casella and Berger 2024). I’ll demonstrate it for an arbitrary \\(Z \\sim \\mathcal{N}(0, 1)\\), \\(\\mu\\), and \\(\\sigma &gt; 0\\) (omitting the subscript \\(i\\) for brevity). The probability density function (pdf) of the standard normal \\(f_Z(z)\\) is defined as \\[\nf_Z(z) = \\frac{1}{\\sqrt{2\\pi}}\\ e^{-z^2 / 2}.\n\\]\nDefine \\(X' = g(Z) = \\sigma \\cdot f_Z(Z)\\). Then, \\(g^{-1}(x) = \\frac{x}{\\sigma}\\), and \\(\\frac{d}{dx}g^{-1}(x) = \\frac{1}{\\sigma}\\). Thus, the probability density function \\(f_{X'}(x)\\) is\n\\[\n\\begin{align*}\nf_{X'}(x) &= f_Z(g^{-1}(x)) \\frac{d}{dx}g^{-1}(x) \\\\\n  &= f_Z \\Bigl(\\frac{x}{\\sigma} \\Bigr) \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\ e^{-(\\frac{x}{\\sigma})^2 \\cdot \\frac{1}{2}} \\cdot \\frac{1}{\\sigma} \\\\\n  &= \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{x^2}{2\\sigma^2}}.\n\\end{align*}\n\\]\nThis is the pdf for \\(\\mathcal{N}(0, \\sigma^2)\\). Now, we find \\(X = h(X') = f_{X'}(X') + \\mu\\), where \\(h^{-1}(x) = x - \\mu\\) and \\(\\frac{d}{dx}\\ h^{-1}(x) = 1\\). As with the above, we have \\[\nf_X(x) = f_{X'}(h^{-1}(x)) \\frac{d}{dx}\\ h^{-1}(x) = f_{X'}(x - \\mu) = \\frac{1}{\\sqrt{2\\pi} \\sigma}\\ e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}},\n\\]\nwhich is the classical definition of the normal distribution’s probability density function. This means that scaling a normal variable by a constant, or adding a constant to a normal variable results in another normal variable. So, each \\(X_i\\) is normal. Lastly, as one’s intuition might suspect, the sum of jointly distributed normal variables is also normal. I won’t go through the proof here, but some versions can be found on wikipedia.\n\n\nThus, we can conclude that a linear combination \\(Y = a_1 X_1 + \\cdots + a_n X_n\\) is a normal variable, meaning \\(\\mathbf{x}\\) is a draw from the multivariate Gaussian distribution.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "gaussian_processes/mvnorm/index.html#r-implementation",
    "href": "gaussian_processes/mvnorm/index.html#r-implementation",
    "title": "The Multivariate Gaussian Distribution",
    "section": "R Implementation",
    "text": "R Implementation\n\nset.seed(123)\nmu &lt;- c(1.1, 3, 8)\nSigma &lt;- matrix(c(1, 0.1, 0.3, 0.1, 1, 0.1, 0.3, 0.1, 1), 3, 3)\n\nmvrnorm &lt;- function(n, mu, Sigma) {\n  L &lt;- chol(Sigma) # this returns L', where LL' = Sigma\n  d &lt;- length(mu)\n\n  out &lt;- matrix(0, nrow = n, ncol = d)\n  for (i in 1:n) {\n    u &lt;- rnorm(d)\n    out[i, ] &lt;- t(t(L) %*% u + mu)\n  }\n  return(out)\n}\n\nmvrnorm(10, mu, Sigma)\n\n           [,1]     [,2]     [,3]\n [1,] 0.5395244 2.714929 9.298527\n [2,] 1.1705084 3.135691 9.661861\n [3,] 1.5609162 1.787372 7.395843\n [4,] 0.6543380 4.173380 8.294725\n [5,] 1.5007715 3.150205 7.599224\n [6,] 2.8869131 3.674046 6.700175\n [7,] 1.8013559 2.599714 7.161280\n [8,] 0.8820251 1.957341 7.169001\n [9,] 0.4749607 1.259257 8.490846\n[10,] 1.2533731 1.882905 9.158747",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "The Multivariate Gaussian Distribution"
    ]
  },
  {
    "objectID": "reflections/reflection_1.html",
    "href": "reflections/reflection_1.html",
    "title": "Reflection 1",
    "section": "",
    "text": "A key aspect of my experience at Boise state is that I am a second-degree student. My first B.A. degree was in the field of psychology. After graduating in 2013, I worked as a research technician in various settings and as a data analyst. Coursework and independent research experience from my first degree prepared me for the work I would perform, but I was constantly drawn to the study of mathematics and statistical science. Modern software makes computing and visualizing statistical data quick and often easy to perform. To me, this convenience highlights the importance of understanding the fundamentals beneath a software implementation. I was computing summaries and building/visualizing small statistical models. I wanted to make sure that I wasn’t mishandling these “golems”, as Richard McElreath (McElreath 2018, chap. 1) describes them.\nWhen I began my program at Boise State, I had already completed Calculus I, Calculus II, Calculus III, and an introduction to Linear Algebra. I believe there are two themes visible in my coursework, and will elaborate on them in the following paragraphs. Below is a list of courses I completed during my second degree, with courses as part of the statistics emphasis in italics and courses I want to highlight in bold.\n\n\n\nTerm\nCourse\nTitle\n\n\n\n\n2021 Spring\nMATH 361\nProbability & Statistics I\n\n\n2021 Summer\nMATH 189\nDiscrete Mathematics\n\n\n2021 Fall\nMATH 287\nMathematical Proofs & Methods\n\n\n\nMATH 471\nData Analysis\n\n\n2022 Spring\nMATH 305\nAbstract Algebra/Number Theory\n\n\n\nMATH 314\nFoundations of Analysis\n\n\n2022 Fall\nMATH 333\nDifferential Equations w/ Matrix Theory\n\n\n\nMATH 462\nProbability & Statistics II\n\n\n2023 Spring\nMATH 472\nComputational Statistics\n\n\n2023 Fall\nMATH 365\nIntroduction to Computational Mathematics\n\n\n2024 Spring\nMATH 308\nIntroduction to Algebraic Cryptology\n\n\n2024 Fall\nMATH 403\nLinear Algebra\n\n\n\nMATH 465\nIntroduction to Numerical Methods\n\n\n\nThe first theme is the development of literacy for mathematical writing. I greatly appreciated the first few classes I took that emphasized the examination and writing of mathematical proofs. I remember MATH 287, MATH 305, and MATH 314 being very challenging, but deeply rewarding in terms of strengthening my ability to read and decompose theorems that I’d encounter in other classes. I credit MATH 287 in particular for helping me learn about the structure of proofs, and the kinds of logical arguments used to integrate and extend mathematical facts/topics.\nThe second theme is an appreciation for the “species” of mathematical objects and spaces that one might encounter. I think MATH-314 and MATH-403 are my favorite examples of this, but I should also mention MATH-305 for similar reasons. MATH-314 was interesting in that it cracks open several things we almost take for granted, such as calculus and the real number line. Calculus is incredibly far-reaching in its impact on technology and science, but we don’t really get a sense for the mathematical results that provide its foundation until studying analysis. Similarly, linear algebra is deeply influential for modern computing, but I’ve grown to appreciate how useful it is for translating mathematical concepts into multiple dimensions. Being aware of the space(s) or set(s) of numbers you’re reasoning about is crucial to framing the problems you’re working on.\nUnifying these two themes is an interest in computation and statistical inference. This stems from my motivations to pursue the degree, but my learning in MATH-462 and MATH-472 would have been much more shallow if I hadn’t taken MATH-287 and MATH-314. Most importantly, courses like MATH-365, MATH-465, and MATH-472 emphasized the implementation of mathematical methods via programming and simulation. I’ve found that defining and conducting a simulation and implementing algorithms to be the best way I learn when exploring a mathematical property or topics within a course. Looking back, I think the absence of an application area was a reason I struggled with MATH-189 and MATH-305.\n\n\n\n\nReferences\n\nMcElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC.",
    "crumbs": [
      "Home",
      "Reflections",
      "Reflection 1"
    ]
  },
  {
    "objectID": "reflections/cv.html",
    "href": "reflections/cv.html",
    "title": "cv",
    "section": "",
    "text": "Technical Skills\n\nPython (pandas, prefect, flask, fastapi)\nSQL (dbt, Oracle, Snowflake, MS SQL-Server)\nR (dplyr/tidyverse, devtools)\nJS (Svelte, SvelteKit)\nStatistical analysis (R, SAS, SPSS)\nData visualization (Tableau, ggplot2/shiny, Observable)\nGit/GitHub, Google Apps Suite, Qualtrics\n\n\n\nProfessional Experience\n\nSenior Data Engineer\nUniversity of Colorado-Boulder, Office of Information Technology\nApril 2024 to present\n\n\nData Engineer\nUniversity of Colorado-Boulder, Office of Information Technology\nJanuary 2023 to April 2024\n\n\nData Warehouse Developer\nBoise State University, Office of Information Technology\nJuly 2021 to December 2022\n\n\nInstitutional Research Analyst\nBoise State University, Office of Institutional Research\nJanuary 2020 to July 2021\n\n\nData Analyst\nUniversity of Michigan, School of Social Work\nNovember 2015 to December 2019\n\n\nLab Manager & Research Technician\nUniversity of Michigan, Department of Psychology\nJune 2014 to November 2015\n\n\nSurvey Technician\nUniversity of Michigan, Survey Research Center\nMay 2013 to June 2014\n\n\n\nEducation\n\n\nB.S. Applied Mathematics, Statistics Emphasis  Boise State University, 2021 - 2025 (anticipated)\n\n\n\nB.A. Psychology  The College of Idaho, 2009 - 2013, cum laude\n\n\n\n\nSelected Publications\n\n\nPerron, B.E., Victor, B.G., Bushman, G., Moore, A., Ryan, J.P., Lu, A.J., & Piellusch, E.K. (2019). Detecting substance-related problems in narrative investigation summaries of child abuse and neglect using text mining and machine learning. Child Abuse & Neglect, 98, 104180. doi:10.1016/j.chiabu.2019.104180\n\n\n\nRyan J.P., Jacob B.A., Gross M., Perron B.E., Moore A., & Ferguson S.M. (2018). Early exposure to child maltreatment and academic outcomes. Child Maltreatment. Advance online publication. doi:10.1177/1077559518786815\n\n\n\n\nCommunity Engagement\n\n\nCo-Organizer, Ann Arbor R-Users Group  2017-2019\n\n\n\nWorkshop Instructor, Ann Arbor R-Users Group  2016"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 401",
    "section": "",
    "text": "Reading Bonilla, Chai, & Williams (2007)\nWorking towards implementing a simulation of multi-task GPR\nLast week, I completed a basic simulation of bivariate GPR, which can be viewed here. It’s light on theory/notation, but I think it follows fairly straightforwardly from the univariate case.\n\nI need to actually start doing more formal parameter estimation for the kernel/covariance functions. Currently, I’ve just been using arbitrary values for \\(\\alpha\\), \\(\\rho\\) and \\(\\sigma\\) in the RBF kernel. These are fine for demonstration purposes, but aren’t representative of what a fully application would entail.\n\n\n\n\n\n\nReading and searching for more background information about multi-output Gaussian Processes\n\ne.g., vector-valued GPs (blog post)\nCompleted review of the multivariate Gaussian distribution (link)\nStill working on modeling bivariate data, but not (yet) fruitfully\n\n\nIdeally, we could model position and velocity together as a vector\n\\[\n\\begin{bmatrix} u \\\\ v \\end{bmatrix} = f(x, y) + \\epsilon\n\\]\nrather than something like \\[\n\\begin{align*}\nu &= f_1(x) + \\epsilon_1 \\\\\nv &= f_2(y) + \\epsilon_2\n\\end{align*}\n\\]\n\n\n\n\nWorking on an overview of Gaussian processes\n\nI have a more formal writeup (in the sense of notation) of GPR here\n\nWorking on an overview of KRR\n\ntodos:\n\nupdate citations\n\n\n\n\n\n\n\nReviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing.\n\n\n\n\n\nDiscussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "MATH 401",
    "section": "",
    "text": "Reading Bonilla, Chai, & Williams (2007)\nWorking towards implementing a simulation of multi-task GPR\nLast week, I completed a basic simulation of bivariate GPR, which can be viewed here. It’s light on theory/notation, but I think it follows fairly straightforwardly from the univariate case.\n\nI need to actually start doing more formal parameter estimation for the kernel/covariance functions. Currently, I’ve just been using arbitrary values for \\(\\alpha\\), \\(\\rho\\) and \\(\\sigma\\) in the RBF kernel. These are fine for demonstration purposes, but aren’t representative of what a fully application would entail."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "MATH 401",
    "section": "",
    "text": "Reading and searching for more background information about multi-output Gaussian Processes\n\ne.g., vector-valued GPs (blog post)\nCompleted review of the multivariate Gaussian distribution (link)\nStill working on modeling bivariate data, but not (yet) fruitfully\n\n\nIdeally, we could model position and velocity together as a vector\n\\[\n\\begin{bmatrix} u \\\\ v \\end{bmatrix} = f(x, y) + \\epsilon\n\\]\nrather than something like \\[\n\\begin{align*}\nu &= f_1(x) + \\epsilon_1 \\\\\nv &= f_2(y) + \\epsilon_2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "MATH 401",
    "section": "",
    "text": "Working on an overview of Gaussian processes\n\nI have a more formal writeup (in the sense of notation) of GPR here\n\nWorking on an overview of KRR\n\ntodos:\n\nupdate citations"
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "MATH 401",
    "section": "",
    "text": "Reviewed sections 1-3. Covered the algorithm that auths present on page N.\nGoal: try to have some workable code to demonstrate KRR and GPR on simulated data. Let’s understand what the methods are producing."
  },
  {
    "objectID": "index.html#section-4",
    "href": "index.html#section-4",
    "title": "MATH 401",
    "section": "",
    "text": "Discussion, admin work\nDecided to cover first 3 sections of __ paper\nMentioned __ textbook."
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#independent-gaussian-proccesses-for-each-component-of-the-velocity-field",
    "href": "gaussian_processes/multioutput/index.html#independent-gaussian-proccesses-for-each-component-of-the-velocity-field",
    "title": "Multioutput GPR",
    "section": "Independent Gaussian Proccesses for each component of the velocity field",
    "text": "Independent Gaussian Proccesses for each component of the velocity field\nHere we’ll\n\nB &lt;- diag(1, 2) # I_2\nK_XX &lt;- k_XX(X)\nSigma &lt;- kronecker(B, K_XX) + kronecker(diag(c(0.001, 0.001)), diag(1, nrow = nrow(X)))\nSi &lt;- solve(Sigma) %*% y\n\nf &lt;- data.frame()\nfor (i in 1:nrow(d)) {\n  x_star &lt;- as.matrix(d[i, 1:2])\n  K_star &lt;- apply(X[, 1:2], 1, function(x) k(x_star, x))\n  f_star &lt;- t(kronecker(B, K_star)) %*% Si |&gt; t()\n  f &lt;- rbind(f, f_star)\n}\n\n# z &lt;- cbind(Z, f)\n# sqrt(colMeans((z[, 3:4] - z[, 5:6])^2))\nf &lt;- f |&gt;\n  tibble() |&gt;\n  set_names(c(\"vx_h\", \"vy_h\")) |&gt;\n  bind_cols(d) |&gt;\n  mutate(col = factor(as.numeric(index %in% S$index), levels = 0:1, labels = c(\"train\", \"test\")))\n\np1 &lt;- p0 +\n  geom_segment(\n    data = f,\n    aes(x, y, xend = x + vx_h/50, yend = y + vy_h/50, color = col),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.25,\n    alpha = 0.8\n  ) +\n  scale_color_scico_d(palette = \"imola\", name = \"\") +\n  theme_minimal(base_size = 15) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np1",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.knit.html",
    "href": "gaussian_processes/multioutput/index.knit.html",
    "title": "Multioutput GPR",
    "section": "",
    "text": "We are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(X, Y \\in \\mathbb{R}^{N \\times 2}\\), and it may be convenient to refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N \\times 1}\\). Let \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be the (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n\\]\nwhere \\(\\alpha, \\rho &gt; 0\\); again, \\(x_i, x_j \\in \\mathbb{R}^2\\). This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let the covariance matrix between all points in \\(\\mathbf{X}\\), \\(K_{XX} \\in \\mathbb{R}^{N \\times N}\\), be defined as \\[\nK_{XX} = K(\\mathbf{X}, \\mathbf{X}) = (k(x_i, x_j))_{i,j} = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\]\nLet the matrix of similarities between the outputs \\(\\mathbf{Y}\\), \\(B \\in \\mathbb{R}^{2 \\times 2}\\) be defined as \\[\nB = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix}.\n\\]\nLet \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{\\Sigma} = B \\otimes K_{XX} + \\begin{bmatrix}\n  \\sigma^2_1 & 0 \\\\\n  0 & \\sigma^2_2\n\\end{bmatrix} \\otimes I_N,\n\\]\nwhere \\(\\otimes\\) is the Kronecker product, \\(I\\) is the identity matrix, and \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the variances of the noise for each component of \\(Y\\).\nFor a single new vector, \\(x_*\\)"
  },
  {
    "objectID": "gaussian_processes/multioutput/index.knit.html#multitask-gaussian-process-prediction",
    "href": "gaussian_processes/multioutput/index.knit.html#multitask-gaussian-process-prediction",
    "title": "Multioutput GPR",
    "section": "",
    "text": "We are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(X, Y \\in \\mathbb{R}^{N \\times 2}\\), and it may be convenient to refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N \\times 1}\\). Let \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be the (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n\\]\nwhere \\(\\alpha, \\rho &gt; 0\\); again, \\(x_i, x_j \\in \\mathbb{R}^2\\). This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let the covariance matrix between all points in \\(\\mathbf{X}\\), \\(K_{XX} \\in \\mathbb{R}^{N \\times N}\\), be defined as \\[\nK_{XX} = K(\\mathbf{X}, \\mathbf{X}) = (k(x_i, x_j))_{i,j} = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\]\nLet the matrix of similarities between the outputs \\(\\mathbf{Y}\\), \\(B \\in \\mathbb{R}^{2 \\times 2}\\) be defined as \\[\nB = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix}.\n\\]\nLet \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{\\Sigma} = B \\otimes K_{XX} + \\begin{bmatrix}\n  \\sigma^2_1 & 0 \\\\\n  0 & \\sigma^2_2\n\\end{bmatrix} \\otimes I_N,\n\\]\nwhere \\(\\otimes\\) is the Kronecker product, \\(I\\) is the identity matrix, and \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the variances of the noise for each component of \\(Y\\).\nFor a single new vector, \\(x_*\\)"
  },
  {
    "objectID": "gaussian_processes/multioutput/index.knit.html#visualizing-vector-fields",
    "href": "gaussian_processes/multioutput/index.knit.html#visualizing-vector-fields",
    "title": "Multioutput GPR",
    "section": "Visualizing vector fields",
    "text": "Visualizing vector fields\nThis example and dataset is drawn from The R Graphics Cookbook (Chang 2018), specifically chapter 13.12, on creating vector fields.\n\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\nglimpse(isabel)\n\nRows: 156,250\nColumns: 8\n$ x     &lt;dbl&gt; -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83, -83,…\n$ y     &lt;dbl&gt; 41.70000, 41.55571, 41.41142, 41.26713, 41.12285, 40.97856, 40.8…\n$ z     &lt;dbl&gt; 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0…\n$ vx    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ vy    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ vz    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ t     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ speed &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\n# Keep a subset in the middle of the the z-axis\nd_isabel &lt;- isabel |&gt;\n  filter(z == 10.035) |&gt;\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n &lt;- function(x, by = 4) {\n  x &lt;- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x &lt;- every_n(unique(isabel$x))\nkeep_y &lt;- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd &lt;- d_isabel |&gt;\n  filter(x %in% keep_x, y %in% keep_y) |&gt;\n  mutate(index = 1:n())\n\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Make the plot with the subset, and use an arrowhead 0.1 cm long\np0 &lt;- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.25\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np0"
  },
  {
    "objectID": "gaussian_processes/multioutput/index.knit.html#independent-gaussian-proccesses-for-each-component-of-the-velocity-field",
    "href": "gaussian_processes/multioutput/index.knit.html#independent-gaussian-proccesses-for-each-component-of-the-velocity-field",
    "title": "Multioutput GPR",
    "section": "Independent Gaussian Proccesses for each component of the velocity field",
    "text": "Independent Gaussian Proccesses for each component of the velocity field\n\nB &lt;- diag(1, 2) # I_2\nK_XX &lt;- k_XX(X)\nSigma &lt;- kronecker(B, K_XX) + kronecker(diag(c(0.001, 0.001)), diag(1, nrow = nrow(X)))\nSi &lt;- solve(Sigma) %*% y\n\nf &lt;- data.frame()\nfor (i in 1:nrow(d)) {\n  x_star &lt;- as.matrix(d[i, 1:2])\n  K_star &lt;- apply(X[, 1:2], 1, function(x) k(x_star, x))\n  f_star &lt;- t(kronecker(B, K_star)) %*% Si |&gt; t()\n  f &lt;- rbind(f, f_star)\n}\n\n# z &lt;- cbind(Z, f)\n# sqrt(colMeans((z[, 3:4] - z[, 5:6])^2))\nf &lt;- f |&gt;\n  tibble() |&gt;\n  set_names(c(\"vx_h\", \"vy_h\")) |&gt;\n  bind_cols(d) |&gt;\n  mutate(col = factor(as.numeric(index %in% S$index), levels = 0:1, labels = c(\"train\", \"test\")))\n\np1 &lt;- p0 +\n  geom_segment(\n    data = f,\n    aes(x, y, xend = x + vx_h/50, yend = y + vy_h/50, color = col),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.25,\n    alpha = 0.8\n  ) +\n  scale_color_scico_d(palette = \"imola\", name = \"\") +\n  theme_minimal(base_size = 15) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np1"
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#overview",
    "href": "gaussian_processes/multioutput/index.html#overview",
    "title": "Multioutput GPR",
    "section": "",
    "text": "We are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times 2}\\), and we may refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N}\\). We are engaged in a regression task, i.e., we’re attempting to learn the functional relationship between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), with an assumption that this relationship has been corrupted to some degree by noise.\nLet \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be the (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(-\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n\\tag{1}\\]\nwhere \\(\\alpha, \\rho &gt; 0\\); again, \\(x_i, x_j \\in \\mathbb{R}^2\\). This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let the covariance matrix between all points in \\(\\mathbf{X}\\), \\(\\mathbf{K}_{XX} \\in \\mathbb{R}^{N \\times N}\\), be defined as \\[\n\\mathbf{K}_{XX} = (k(x_i, x_j))_{i,j} = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\tag{2}\\]\nLet the matrix of similarities between the outputs \\(\\mathbf{Y}\\), \\(\\mathbf{B} \\in \\mathbb{R}^{2 \\times 2}\\) be defined as \\[\n\\mathbf{B} = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix} = \\frac{1}{N} \\mathbf{Y}^\\top \\mathbf{K}_{XX} \\mathbf{Y}.\n\\tag{3}\\]\nThe expression for \\(\\mathbf{B}\\) comes from Bonilla et al. (Bonilla, Chai, and Williams 2007), in section 2.3 (the authors use the notation \\(K^f\\) and \\(K^x\\) instead of \\(\\mathbf{B}\\) and \\(\\mathbf{K}_{XX}\\), respectively).\nLet \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{\\Sigma} = \\mathbf{B} \\otimes \\mathbf{K}_{XX} + \\begin{bmatrix}\n  \\sigma^2_1 & 0 \\\\\n  0 & \\sigma^2_2\n\\end{bmatrix} \\otimes \\mathbf{I}_N,\n\\tag{4}\\]\nwhere \\(\\otimes\\) is the Kronecker product, \\(\\mathbf{I}\\) is the identity matrix, and \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the variances of the noise for each component of \\(\\mathbf{Y}\\).\nFor a single new vector, \\(x' \\in \\mathbb{R}^2\\), the predicted value for \\(y' \\in \\mathbb{R}^2\\) is given by \\[\ny' = (\\mathbf{B} \\otimes \\mathbf{k}_{x'})^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{y},\n\\tag{5}\\]\nwhere \\(\\mathbf{k}_{x'} \\in \\mathbb{R}^{N}\\) is defined as \\[\n\\mathbf{k}_{x'} = \\begin{bmatrix}\n  k(x', x_1) \\\\\n  k(x', x_2) \\\\\n  \\vdots \\\\\n  k(x', x_N)\n\\end{bmatrix}.\n\\tag{6}\\]",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#motivation-visualizing-vector-fields",
    "href": "gaussian_processes/multioutput/index.html#motivation-visualizing-vector-fields",
    "title": "Multioutput GPR",
    "section": "",
    "text": "Elsewhere, I’ve discussed the multivariate normal distribution, and Gaussian processes. Introductory materials discussing Gaussian Processes (GPs) typically focus on univariate outcomes. In more formal notation, input data \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times P}\\) is used to model \\(\\mathbf{y} \\in \\mathbb{R}^N\\): \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{X}) + \\mathbf{\\epsilon} \\text{ where } \\\\\nf(\\mathbf{X}) &\\sim \\mathcal{N}_N(\\mathbf{0}, \\mathbf{K}), \\\\\n\\mathbf{\\epsilon} &\\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_y), \\text{ and } \\\\\n\\mathbf{K} &= [k(x_i, x_j)]_{ij}^N.\n\\end{align*}\n\\]\n\n\nThroughout this page, I’ll be using the subscript after \\(\\mathcal{N}\\) to indicate we’re looking at a multivariate normal distribution, and to show the dimension of the vectors that the distribution produces.\nWhile interesting univariate outcomes can be found in abundance, many physical systems and processes are better understood and represented as vectors. In this page, we’ll use the example of the velocity field of hurricane Isabel, which made landfall in 2003. Here is the description of the dataset, from the gcookbook package:\n\n\nThis example and dataset is drawn from The R Graphics Cookbook (Chang 2018), specifically chapter 13.12, on creating vector fields.\n\nThis data is from a simulation of hurricane Isabel in 2003. It includes temperature and wind data for a 2139km (east-west) x 2004km (north-south) x 19.8km (vertical) volume. The simluation data is from the National Center for Atmospheric Research, and it was used in the IEEE Visualization 2004 Contest.\n\nBelow I’ve plotted the x and y components of the storm’s velocity field, viewed at approximately 10km above sea-level. Each arrow shows the velocity of the storm’s winds at a given point in the x-y plane. In mathematical notation, we denote the velocity for a particular x-y point as a vector: \\[\n\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}.\n\\]\nVelocity captures both the direction of the wind, and its magnitude (or strength). Here, longer arrows indicate higher measured speeds. If we wanted, we could plot wind-speed by itself over the area. However, looking only at speed would cause us to miss key dynamics of the phenomena we’re looking at, such as the storm’s spiral shape.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gcookbook)\nlibrary(scico)\nset.seed(123)\n\neps &lt;- sqrt(.Machine$double.eps)\n\n# Keep a subset in the middle of the the z-axis\nd_isabel &lt;- isabel |&gt;\n  filter(z == 10.035) |&gt;\n  as_tibble()\n\n# Keep 1 out of every 'by' values in vector x\nevery_n &lt;- function(x, by = 4) {\n  x &lt;- sort(x)\n  x[seq(1, length(x), by = by)]\n}\n\n# Keep 1 of every 4 values in x and y\nkeep_x &lt;- every_n(unique(isabel$x))\nkeep_y &lt;- every_n(unique(isabel$y))\n\n# Keep only those rows where x value is in keepx and y value is in keepy\nd &lt;- d_isabel |&gt;\n  filter(x %in% keep_x, y %in% keep_y) |&gt;\n  mutate(index = 1:n())\n\n# Need to load grid for arrow() function\nlibrary(grid)\n\n# Set a consistent plotting theme\ntheme_set(theme_minimal(base_size = 15))\n\n# Make a plot with the subset, and use an arrowhead 0.1 cm long\np0 &lt;- ggplot(d, aes(x, y)) +\n  geom_segment(\n    aes(xend = x + vx/50, yend = y + vy/50),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    linewidth = 0.25\n  ) +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\np0\n\n\n\n\n\n\n\n\n\nA similar consideration applies to the task of predicting velocity using statistical models. A “naive” approach might be to build a model for each component of velocity. In our case, that would mean finding two functions, \\(f_1: \\mathbb{R}^2 \\to \\mathbb{R}\\) and \\(f_2: \\mathbb{R}^2 \\to \\mathbb{R}\\). This amounts to treating \\(v_x\\) and \\(v_y\\) as being independent of each other. Depending on context, this may be true according to scientific theory. However, when working with data provided via sensors and instruments, measurement error may move us away from the theoretical ideal.\nAn improved approach would allow us to incorporate natural information on how our observed target values (each \\(v_x\\) and \\(v_y\\)) might interrelate. The approach should also (ideally) account for the fact that we’re trying to predict a vector at a specific point. In other words, we’d like a single function, whose outputs are vector-valued:\n\\[\n\\underbrace{\\begin{aligned}[c]\nv_x = f_1\\Bigl((x, y)^\\top \\Bigr) + \\epsilon \\\\\nv_y = f_2\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Naive approach}}\n\\qquad\\longrightarrow\\qquad\n\\underbrace{\\begin{aligned}[c]\n\\begin{bmatrix}v_x \\\\ v_y\\end{bmatrix} = f\\Bigl((x, y)^\\top \\Bigr) + \\epsilon\n\\end{aligned}}_{\\text{Vector-valued approach}}\n\\]\nThese characteristics are possible within the framework of Gaussian Process Regression, with the goal of modeling multiple outputs simultaneously being referred to as “multioutput”, “multitask”, “cokriging”, or “vector-valued” GP regression. Alvarez, Rosasco, & Lawrence (2012) provides a technical introduction to some of these topics (Alvarez et al. 2012). In the following section, we’ll cover the theory behind the approach as it applies to our example data.",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  },
  {
    "objectID": "gaussian_processes/multioutput/index.html#theory-and-notation",
    "href": "gaussian_processes/multioutput/index.html#theory-and-notation",
    "title": "Multioutput GPR",
    "section": "",
    "text": "We are working in the space of the two-dimensional real numbers, \\(\\mathbb{R}^2\\). Let \\(S = (\\mathbf{X}, \\mathbf{Y}) = (x_1, y_1), \\dots, (x_N, y_N)\\) be our set of training data, where \\(x_i, y_i \\in \\mathbb{R}^2\\). It may be convenient to note \\(\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{N \\times 2}\\), and we may refer to \\(\\mathbf{y} = \\text{vec}\\ Y \\in \\mathbb{R}^{2N}\\) and \\(\\mathbf{x} = \\text{vec}\\ X \\in \\mathbb{R}^{2N}\\). We will use \\(\\mathbf{X}_* \\in \\mathbb{R}^{M \\times 2}\\) to denote a set of test points, for which we want to generate predictions. As with the conventions for the training data, we will use \\(\\mathbf{x}_* = \\text{vec}\\ \\mathbf{X}_* \\in \\mathbb{R}^{2M}\\).\nWe are engaged in a regression task, i.e., we’re attempting to learn the functional relationship \\(f\\) between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), with an assumption that this relationship has been corrupted to some degree by noise. For the following, I’ve adapted notation from Chapter 2.2 in the well-known Rasmussen and Williams text to the context of our multidimensional \\(\\mathbf{Y}\\) (Williams and Rasmussen 2006). Setting up the problem, we have \\[\n\\begin{align*}\n\\mathbf{y} &= f(\\mathbf{x}) + \\epsilon \\\\\nf &\\sim \\mathcal{GP}(m, k) \\\\\nf(\\mathbf{x}) &\\sim \\mathcal{N}_{2N}(\\mathbf{0}, \\mathbf{K}_{XX})\n\\end{align*}\n\\]\nTo parse these statements, we assume \\(f\\) is drawn from a Gaussian Process with mean function \\(m\\) and covariance (kernel) function \\(k\\). Thus, our observations \\(\\mathbf{y} = f(\\mathbf{x})\\) have a multivariate normal probability distribution, parameterized by a covariance matrix \\(\\mathbf{K}_{XX}\\). By convention, we assume the mean vector is \\(\\mathbf{0}\\) (implying that \\(m\\) is the zero function).\nThe joint distribution of \\(\\mathbf{y}\\) and our predictions for the test point(s) \\(\\mathbf{y}_*\\) is also described by a multivariate normal distribution, specified as: \\[\n\\begin{align*}\n\\begin{bmatrix}\n  \\mathbf{y} \\\\\n  \\mathbf{y_*}\n\\end{bmatrix} &\\sim \\mathcal{N}_{2N + 2M}\\Biggl( \\mathbf{0}, \\begin{bmatrix}\n  \\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N & \\mathbf{K}_{XX_*} \\\\\n  \\mathbf{K}_{X_*X} & \\mathbf{K}_{X_* X_*}\n\\end{bmatrix} \\Biggr) \\\\\n\\mathbf{V} &= \\begin{bmatrix} \\sigma^2_1 & 0 \\\\ 0 & \\sigma^2_2 \\end{bmatrix}\n\\end{align*}\n\\]\nHere, \\(\\sigma^2_1, \\sigma^2_2 &gt; 0\\) represent the independent and additive noise associated with each of our outcome’s components. The \\(\\otimes\\) symbol denotes the Kronecker product. From this joint distribution, we can derive the conditional distribution for \\(\\mathbf{y}_*\\):\n\\[\n\\begin{align*}\n\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*} &\\sim \\mathcal{N}_{2M}(\\bar{\\mathbf{f}}_*,\\ \\text{cov}(\\mathbf{f}_*)) \\\\\n\\bar{\\mathbf{f}}_* &= \\mathbb{E}[\\mathbf{y}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X_*}] = \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1} \\mathbf{y} \\\\\n\\text{cov}(\\mathbf{f}_*) &= \\mathbf{K}_{X_*X_*} - \\mathbf{K}_{X_*X}(\\mathbf{K}_{XX} + \\mathbf{V} \\otimes \\mathbf{I}_N)^{-1}\\mathbf{K}_{XX_*} \\\\\n\\end{align*}\n\\]\nWe will now work through how each piece of our distribution is defined. Let \\(k: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) be a (scalar) kernel function, defined as \\[\nk(x_i, x_j) = \\alpha^2 \\cdot \\exp\\Bigl(-\\frac{1}{2\\rho^2} \\| x_i - x_j \\|_2 \\Bigl),\n\\tag{1}\\]\nwhere \\(\\alpha, \\rho &gt; 0\\), and \\(\\| \\cdot \\|_2\\) is the Euclidean norm. This is also known as the squared exponential function, or the radial basis function. The parameter \\(\\alpha\\) controls …, while \\(\\rho\\) controls the length-scale. Let \\(\\mathbf{k}_{XX} \\in \\mathbb{R}^{N \\times N}\\), the covariance matrix between all points in \\(\\mathbf{X}\\), be defined as \\[\n\\mathbf{k}_{XX} = (k(x_i, x_j))_{i,j}^N = \\begin{bmatrix}\n  k(x_1, x_1) & \\cdots & k(x_1, x_N) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  k(x_N, x_1) & \\cdots & k(x_N, x_N)\n\\end{bmatrix}.\n\\tag{2}\\]\nLet \\(\\mathbf{B} \\in \\mathbb{R}^{2 \\times 2}\\), the matrix of similarities between the outputs \\(\\mathbf{Y}\\), be defined as \\[\n\\mathbf{B} = \\begin{bmatrix}\n  b_{11} & b_{12} \\\\\n  b_{21} & b_{22}\n\\end{bmatrix} = \\frac{1}{N} \\mathbf{Y}^\\top \\mathbf{k}_{XX} \\mathbf{Y}.\n\\tag{3}\\]\nThe expression for \\(\\mathbf{B}\\) comes from Bonilla et al. (Bonilla, Chai, and Williams 2007), in section 2.3 (the authors use the notation \\(K^f\\) and \\(K^x\\) instead of \\(\\mathbf{B}\\) and \\(\\mathbf{k}_{XX}\\), respectively).\nLet \\(\\mathbf{K}_{XX} \\in \\mathbb{R}^{2N \\times 2N}\\) be defined as \\[\n\\mathbf{K}_{XX} = \\mathbf{B} \\otimes \\mathbf{k}_{XX}.\n\\tag{4}\\]\nWe can now define the other pieces needed to establish the covariance matrix used for the joint distribution of \\(\\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{y}_* \\end{bmatrix}\\):\n\\[\n\\begin{align*}\n  \\mathbf{K}_{X_*X} &= \\mathbf{B} \\otimes (k(x_{*i}, x_j))_{i,j=1}^{M,N} \\in \\mathbb{R}^{2M \\times 2N} \\\\\n  \\mathbf{K}_{XX_*} &= \\mathbf{K}_{X_*X}^\\top \\in \\mathbb{R}^{2N \\times 2M} \\\\\n  \\mathbf{K}_{X_*X_*} &= \\mathbf{B} \\otimes (k(x_{*i}, x_{*j}))_{i,j=1}^{M} \\in \\mathbb{R}^{2M \\times 2M}.\n\\end{align*}\n\\tag{5}\\]",
    "crumbs": [
      "Home",
      "Gaussian Processes",
      "Multioutput GPR"
    ]
  }
]